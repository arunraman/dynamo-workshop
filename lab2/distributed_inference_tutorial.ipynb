{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Disaggregated Serving with AIConfigurator\n",
    "\n",
    "This lab guides you through deploying disaggregated inference (separate prefill and decode workers) using AIConfigurator to find optimal configurations.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- âœ… **Completed Lab 1** (namespace, platform, and aggregated serving)\n",
    "- âœ… Your namespace and Dynamo platform are still running from Lab 1\n",
    "- âœ… HuggingFace token configured\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Verify your environment from Lab 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597942ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Verify environment from Lab 1\n",
    "USER_FRONTEND2_PORT = os.environ.get('USER_FRONTEND2_PORT', '11000')\n",
    "NAMESPACE = os.environ.get('NAMESPACE', f\"dynamo-{os.environ.get('USER', 'unknown')}\")\n",
    "os.environ['RELEASE_VERSION'] = '0.6.0'\n",
    "os.environ['CACHE_PATH'] = '/data/huggingface-cache'  # Replace with your cache path\n",
    "\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(\"ğŸ“ Lab 2: Disaggregated Serving Configuration\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(f\"  Namespace:            {NAMESPACE}\")\n",
    "print(f\"  Disaggregated Port:   {USER_FRONTEND2_PORT}\")\n",
    "print(\"\")\n",
    "print(\"ğŸ’¡ Disaggregated endpoint: localhost:{USER_FRONTEND2_PORT}\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Deploy with AIConfigurator\n",
    "\n",
    "AIConfigurator helps find optimal configurations for disaggregated serving by analyzing your model and hardware.\n",
    "\n",
    "### Step 1: Install AIConfigurator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you were not able to install it earlier.\n",
    "%%python\n",
    "# pre-installed in the container\n",
    "# !pip3 install aiconfigurator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Configuration Analysis\n",
    "\n",
    "Example: Find optimal configuration for Llama 3.1-70B on 16 H200 GPUs with TensorRT-LLM engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aiconfigurator cli default --model LLAMA3.1_70B --total_gpus 16 --system h200_sxm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy Disaggregated Serving\n",
    "\n",
    "Use the AIConfigurator output as a reference for vLLM engine and update and deploy `disagg_router.yaml`.\n",
    "\n",
    "**Note:** In this workshop we use a smaller model and stick to TP=1 for one prefill worker and one decode worker. You can test with `meta-llama/Llama-3.1-70B-Instruct` with TP=2 using four GPUs in total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Update disagg_router.yaml\n",
    "sed -i \"s/my-tag/$RELEASE_VERSION/g\" disagg_router.yaml\n",
    "sed -i \"s|/YOUR/LOCAL/CACHE/FOLDER|$CACHE_PATH|g\" disagg_router.yaml\n",
    "\n",
    "echo \"âœ“ Configuration updated\"\n",
    "grep \"image:\" disagg_router.yaml\n",
    "\n",
    "# Deploy\n",
    "kubectl apply -f disagg_router.yaml --namespace $NAMESPACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Monitor Deployment\n",
    "\n",
    "Wait for all pods (frontend, prefill workers, and decode workers) to be ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "kubectl get dynamographdeployment -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pod status (wait for all pods to be 1/1 Ready):\"\n",
    "kubectl get pods -n $NAMESPACE | grep disagg\n",
    "\n",
    "# To watch in real-time:\n",
    "# kubectl get pods -n $NAMESPACE -w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a53ed3",
   "metadata": {},
   "source": [
    "### Step 5: Test Disaggregated Serving\n",
    "\n",
    "Forward the port and test the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Forward the service port\n",
    "kubectl port-forward deployment/vllm-v1-disagg-router-frontend $USER_FRONTEND2_PORT:8000 -n $NAMESPACE &\n",
    "\n",
    "echo \"âœ“ Port forward started on localhost:${USER_FRONTEND2_PORT}\"\n",
    "sleep 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the deployment: Non-streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:${USER_FRONTEND2_PORT}/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\\\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain the difference between prefill and decode in LLM inference\"}], \\\n",
    "    \"stream\": false,\\\n",
    "    \"max_tokens\": 100 \\\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the deployment: streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:${USER_FRONTEND2_PORT}/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\", \\\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}], \\\n",
    "    \"stream\": true, \\\n",
    "    \"max_tokens\": 100 \\\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Disaggregated Architecture\n",
    "\n",
    "### Key Differences from Aggregated Serving\n",
    "\n",
    "| Aspect | Aggregated (Lab 1) | Disaggregated (Lab 2) |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Workers** | Single worker type handles both prefill and decode | Separate prefill and decode workers |\n",
    "| **Routing** | Round-robin across workers | KV-aware routing between prefill and decode |\n",
    "| **Use Case** | Single-node models, simpler topology | Multi-node models, optimized resource usage |\n",
    "| **Efficiency** | Good for small-medium models | Better for large models with different compute profiles |\n",
    "\n",
    "### When to Use Disaggregated Serving\n",
    "\n",
    "- **Large models** that require multiple GPUs or tensor parallelism\n",
    "- **Workloads with different prefill/decode characteristics**\n",
    "- **Optimizing resource allocation** (different GPU types for prefill vs decode)\n",
    "- **Scaling prefill and decode independently**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Cleanup\n",
    "\n",
    "Delete the disaggregated deployment when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete disaggregated deployment\n",
    "kubectl delete dynamographdeployment vllm-v1-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Disaggregated deployment deleted\"\n",
    "echo \"\"\n",
    "echo \"Note: Your namespace and platform from Lab 1 are still running\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Check Disaggregated Deployment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check all disaggregated pods\n",
    "kubectl get pods -n $NAMESPACE | grep disagg\n",
    "\n",
    "# Check prefill worker logs\n",
    "PREFILL_POD=$(kubectl get pods -n $NAMESPACE | grep prefill | head -1 | awk '{print $1}')\n",
    "if [ -n \"$PREFILL_POD\" ]; then\n",
    "    echo \"Prefill worker logs:\"\n",
    "    kubectl logs $PREFILL_POD -n $NAMESPACE --tail=30\n",
    "fi\n",
    "\n",
    "# Check decode worker logs\n",
    "DECODE_POD=$(kubectl get pods -n $NAMESPACE | grep decode | head -1 | awk '{print $1}')\n",
    "if [ -n \"$DECODE_POD\" ]; then\n",
    "    echo \"Decode worker logs:\"\n",
    "    kubectl logs $DECODE_POD -n $NAMESPACE --tail=30\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "1. **Pods stuck in Pending**: Check if sufficient GPU resources are available. Disaggregated serving requires more GPUs than aggregated.\n",
    "   ```bash\n",
    "   kubectl describe pod <pod-name> -n $NAMESPACE | grep -A 5 Events\n",
    "   ```\n",
    "\n",
    "2. **Communication issues between prefill and decode**: Check NATS connectivity and ensure all workers registered correctly.\n",
    "   ```bash\n",
    "   kubectl logs -l component=Frontend -n $NAMESPACE --tail=50\n",
    "   ```\n",
    "\n",
    "3. **Model loading failures**: Check that the model is compatible with disaggregated serving and tensor parallelism settings are correct.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- âœ… How to use AIConfigurator to find optimal configurations\n",
    "- âœ… Deploying disaggregated serving (separate prefill/decode workers)\n",
    "- âœ… Understanding differences between aggregated and disaggregated architectures\n",
    "- âœ… When to use disaggregated serving for large models\n",
    "\n",
    "### Key Takeaways\n",
    "- **AIConfigurator** automates finding optimal tensor parallelism and resource allocation\n",
    "- **Disaggregated serving** separates prefill and decode for better resource utilization\n",
    "- **KV-aware routing** is essential for coordinating between prefill and decode workers\n",
    "- **Scaling** prefill and decode independently can optimize for different workload patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- ğŸ“– [Dynamo Documentation](https://docs.dynamo.nvidia.com)\n",
    "- ğŸ”§ [AIConfigurator Guide](../../dynamo/docs/tools/aiconfigurator.md)\n",
    "- ğŸ—ï¸ [Disaggregated Serving Architecture](../../dynamo/docs/architecture/disaggregated.md)\n",
    "- ğŸ“Š [AIPerf Benchmarking Tool](https://github.com/ai-dynamo/aiperf)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! ğŸ‰** You've successfully deployed disaggregated inference with Dynamo!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
