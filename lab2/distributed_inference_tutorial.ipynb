{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Inference with Dynamo\n",
    "\n",
    "This interactive notebook guides you through deploying distributed inference with Dynamo on Kubernetes.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- âœ… Kubernetes cluster with GPU support\n",
    "- âœ… `kubectl` and `helm` 3.x installed\n",
    "- âœ… HuggingFace token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Single-Node-Sized Models with Aggregated Serving\n",
    "\n",
    "Deploy multiple replicas of a model with KV cache-based routing for load balancing.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Set your configuration variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your configuration\n",
    "os.environ['RELEASE_VERSION'] = '0.5.0'\n",
    "os.environ['NAMESPACE'] = 'your_namespace_here'  # Replace with your namespace\n",
    "os.environ['HF_TOKEN'] = 'your_huggingface_token'  # Replace with your HuggingFace token\n",
    "os.environ['CACHE_PATH'] = '/data/huggingface-cache'  # Replace with your cache path\n",
    "\n",
    "print(\"âœ“ Configuration set:\")\n",
    "print(f\"  Release Version: {os.environ['RELEASE_VERSION']}\")\n",
    "print(f\"  Namespace: {os.environ['NAMESPACE']}\")\n",
    "print(f\"  Cache Path: {os.environ['CACHE_PATH']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Dynamo CRDs\n",
    "\n",
    "**Note:** CRDs are cluster-wide resources and only need to be installed **once per cluster**. If already installed, skip to Step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if CRDs already exist\n",
    "if kubectl get crd dynamographdeployments.nvidia.com &>/dev/null && \\\n",
    "   kubectl get crd dynamocomponentdeployments.nvidia.com &>/dev/null; then\n",
    "    echo \"âœ“ CRDs already installed, skipping to Step 2\"\n",
    "else\n",
    "    echo \"Installing Dynamo CRDs...\"\n",
    "    helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-crds-$RELEASE_VERSION.tgz\n",
    "    helm install dynamo-crds dynamo-crds-$RELEASE_VERSION.tgz --namespace default\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Verifying CRD installation:\"\n",
    "    kubectl get crd | grep nvidia.com\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Dynamo Platform\n",
    "\n",
    "This installs ETCD, NATS, and the Dynamo Operator Controller in your namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace\n",
    "kubectl create namespace $NAMESPACE 2>/dev/null || echo \"Namespace $NAMESPACE already exists\"\n",
    "\n",
    "# Download platform chart\n",
    "helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-platform-$RELEASE_VERSION.tgz\n",
    "\n",
    "# Install or upgrade\n",
    "if helm list -n $NAMESPACE | grep -q dynamo-platform; then\n",
    "    echo \"Upgrading Dynamo platform...\"\n",
    "    helm upgrade dynamo-platform dynamo-platform-$RELEASE_VERSION.tgz --namespace $NAMESPACE\n",
    "else\n",
    "    echo \"Installing Dynamo platform...\"\n",
    "    helm install dynamo-platform dynamo-platform-$RELEASE_VERSION.tgz --namespace $NAMESPACE\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"Platform installation initiated. Checking status...\"\n",
    "kubectl get pods -n $NAMESPACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure and Deploy Model\n",
    "\n",
    "**âš ï¸ IMPORTANT:** Before deploying, we need to update the YAML configuration files with your specific values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Update agg_router.yaml with your configuration\n",
    "\n",
    "# Replace my-tag with actual version\n",
    "sed -i \"s/my-tag/$RELEASE_VERSION/g\" agg_router.yaml\n",
    "\n",
    "# Replace cache path\n",
    "sed -i \"s|/YOUR/LOCAL/CACHE/FOLDER|$CACHE_PATH|g\" agg_router.yaml\n",
    "\n",
    "echo \"âœ“ Configuration updated in agg_router.yaml\"\n",
    "echo \"\"\n",
    "echo \"Verify image tags (should show version, not my-tag):\"\n",
    "grep \"image:\" agg_router.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create HuggingFace secret and deploy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "    --from-literal=HF_TOKEN=$HF_TOKEN \\\n",
    "    --namespace $NAMESPACE 2>/dev/null || echo \"Secret already exists\"\n",
    "\n",
    "# Deploy the model\n",
    "kubectl apply -f agg_router.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Deployment created. This will take 4-6 minutes for first run.\"\n",
    "echo \"  - Pulling container images\"\n",
    "echo \"  - Downloading model from HuggingFace\"\n",
    "echo \"  - Loading model and running torch.compile\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor deployment progress:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "kubectl get dynamographdeployment -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pod status (wait for all pods to be 1/1 Ready):\"\n",
    "kubectl get pods -n $NAMESPACE | grep vllm\n",
    "\n",
    "# To watch in real-time, uncomment the line below:\n",
    "# kubectl get pods -n $NAMESPACE -w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test the Deployment\n",
    "\n",
    "Once all pods are `1/1 Ready`, forward the service port (run this in a separate terminal or background):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Forward the service port (run in background with &)\n",
    "kubectl port-forward deployment/vllm-agg-router-frontend 8000:8000 -n $NAMESPACE &\n",
    "\n",
    "echo \"âœ“ Port forward started on localhost:8000\"\n",
    "echo \"  (To stop: use 'pkill -f port-forward' or press Ctrl+C in the terminal running it)\"\n",
    "sleep 5  # Give it time to start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Simple Non-Streaming Request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\\\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}], \\\n",
    "    \"stream\": false,\\\n",
    "    \"max_tokens\": 50 \\\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Streaming Request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\", \\\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}], \\\n",
    "    \"stream\": true, \\\n",
    "    \"max_tokens\": 100 \\\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Deploy with AIConfigurator\n",
    "\n",
    "AIConfigurator helps find optimal configurations for disaggregated serving by analyzing your model and hardware.\n",
    "\n",
    "### Step 1: Install AIConfigurator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install aiconfigurator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Configuration Analysis\n",
    "\n",
    "Example: Find optimal configuration for Llama 3.1-70B on 16 H200 GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!aiconfigurator cli default --model LLAMA3.1_70B --total_gpus 16 --system h200_sxm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy with Recommended Settings\n",
    "\n",
    "Based on AIConfigurator output, update and deploy `disagg_router.yaml`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Update disagg_router.yaml\n",
    "sed -i '' \"s/my-tag/$RELEASE_VERSION/g\" disagg_router.yaml\n",
    "sed -i '' \"s|/YOUR/LOCAL/CACHE/FOLDER|$CACHE_PATH|g\" disagg_router.yaml\n",
    "\n",
    "echo \"âœ“ Configuration updated\"\n",
    "grep \"image:\" disagg_router.yaml\n",
    "\n",
    "# Deploy\n",
    "kubectl apply -f disagg_router.yaml --namespace $NAMESPACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Check if pods are stuck in ImagePullBackOff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check for image pull errors\n",
    "POD=$(kubectl get pods -n $NAMESPACE | grep vllm | grep -v Running | head -1 | awk '{print $1}')\n",
    "\n",
    "if [ -n \"$POD\" ]; then\n",
    "    echo \"Checking pod: $POD\"\n",
    "    kubectl describe pod $POD -n $NAMESPACE | grep -A 5 \"Failed\"\n",
    "else\n",
    "    echo \"âœ“ All pods are running successfully\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View logs from a worker pod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get logs from first worker pod\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE | grep vllmdecodeworker | head -1 | awk '{print $1}')\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Viewing logs from: $WORKER_POD\"\n",
    "    echo \"Look for:\"\n",
    "    echo \"  - 'Loading model weights...' (downloading)\"\n",
    "    echo \"  - 'Model loading took X.XX GiB' (loaded)\"\n",
    "    echo \"  - 'torch.compile takes X.X s' (ready)\"\n",
    "    echo \"\"\n",
    "    kubectl logs $WORKER_POD -n $NAMESPACE --tail=50\n",
    "else\n",
    "    echo \"No worker pods found yet\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To remove the deployment when done:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete deployment\n",
    "kubectl delete dynamographdeployment vllm-agg-router -n $NAMESPACE\n",
    "kubectl delete secret hf-token-secret -n $NAMESPACE\n",
    "\n",
    "# (Optional) Uninstall platform\n",
    "# helm uninstall dynamo-platform -n $NAMESPACE\n",
    "\n",
    "# (Optional) Delete namespace\n",
    "# kubectl delete namespace $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Cleanup complete\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- ðŸ“– [Dynamo Documentation](https://docs.dynamo.nvidia.com)\n",
    "- ðŸ”§ [AIPerf Benchmarking Tool](https://github.com/ai-dynamo/aiperf)\n",
    "- ðŸ“¦ [NGC Container Catalog](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-dynamo/containers/vllm-runtime)\n",
    "- ðŸŽ¯ [vLLM Backend Guide](../../../components/backends/vllm/deploy/README.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! ðŸŽ‰** You've successfully deployed Dynamo distributed inference on Kubernetes!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
