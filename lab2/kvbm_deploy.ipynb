{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c521413a",
   "metadata": {},
   "source": [
    "# Lab 2 Extension: KV Cache Offloading with KVBM\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this extension to Lab 2, you will:\n",
    "- Learn about Dynamo's KV Block Manager (KVBM) for multi-tier memory management\n",
    "- Deploy baseline and KVBM-enabled serving configurations\n",
    "- Run benchmarks demonstrating prefix cache reuse improvements\n",
    "- Compare performance with and without KV cache offloading\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 and understand basic Dynamo deployment\n",
    "\n",
    "## Duration: ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding KVBM\n",
    "\n",
    "### What Is the KV Block Manager?\n",
    "\n",
    "The **KV Block Manager (KVBM)** is Dynamo's sophisticated memory management system that extends vLLM's capacity beyond GPU memory limitations by intelligently managing KV cache blocks across multiple storage tiers.\n",
    "\n",
    "### The Problem: GPU Memory Constraints\n",
    "\n",
    "Traditional LLM serving faces hard limits:\n",
    "\n",
    "```text\n",
    "Without KVBM:\n",
    "  Request 1 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] (fills GPU)\n",
    "  Request 2 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] (fills more)\n",
    "  Request 3 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] (GPU full!)\n",
    "  Request 4 (20K tokens) â†’ Must evict Request 1's cache âŒ (lost forever)\n",
    "  Request 5 reuses Request 1's prefix â†’ Must recompute all 20K tokens ðŸŒ (slow!)\n",
    "```\n",
    "\n",
    "### The Solution: Multi-Tier Memory Management\n",
    "\n",
    "KVBM solves this by offloading inactive KV blocks to CPU memory (and disk):\n",
    "\n",
    "```text\n",
    "With KVBM + 20GB CPU Cache:\n",
    "  Request 1 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]\n",
    "  Request 2 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]\n",
    "  Request 3 (20K tokens) â†’ GPU KV Cache [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] (GPU full!)\n",
    "  Request 4 (20K tokens) â†’ Offload Request 1 to CPU, add Request 4 to GPU âœ…\n",
    "  Request 5 reuses Request 1's prefix â†’ Fetch from CPU to GPU âš¡ (fast!)\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "| Aspect | Without KVBM | With KVBM |\n",
    "|--------|-------------|-----------|\n",
    "| **Capacity** | Limited to GPU memory (~80GB) | GPU + CPU + Disk (hundreds of GB) |\n",
    "| **Prefix Reuse** | Lost when evicted from GPU | Preserved in CPU/disk for reuse |\n",
    "| **Request Handling** | Reject when GPU full | Offload to slower tiers gracefully |\n",
    "| **Long Context** | Competes for GPU memory | Can overflow to CPU/disk |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  vLLM Engine with KVBM Connector    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚   KV Block Manager   â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚           â”‚          â”‚\n",
    "  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”\n",
    "  â”‚  GPU   â”‚  â”‚ CPU  â”‚  â”‚ Disk  â”‚\n",
    "  â”‚ (Fast) â”‚  â”‚(Mid) â”‚  â”‚(Slow) â”‚\n",
    "  â”‚ ~80GB  â”‚  â”‚20GB+ â”‚  â”‚100GB+ â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy Baseline Configuration (No KVBM)\n",
    "\n",
    "### Objectives\n",
    "- Deploy vLLM with standard prefix caching (no KVBM) on Kubernetes\n",
    "- Run benchmark demonstrating prefix cache eviction behavior\n",
    "- Establish baseline performance metrics\n",
    "\n",
    "**Note:** We'll use constrained GPU memory (20% utilization) to force prefix eviction, demonstrating the problem KVBM solves.\n",
    "\n",
    "### Step 1: Review Baseline Deployment YAML\n",
    "\n",
    "The baseline deployment. Review `kvbm_baseline.yaml`:\n",
    "\n",
    "Key configuration:\n",
    "- **Connector**: NIXL (standard vLLM prefix caching)\n",
    "- **GPU utilization**: 20% (constrains memory to force eviction)\n",
    "- **Model**: Qwen/Qwen3-4B\n",
    "- **Prefix caching**: Enabled (but only in GPU memory)\n",
    "\n",
    "### Step 2: Deploy Baseline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542dfc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamographdeployment.nvidia.com/vllm-kvbm-baseline configured\n",
      "âœ“ Baseline deployment created\n",
      "Models are pre-cached, this will take 1-2 minutes for model loading...\n",
      "\n",
      "Configuration:\n",
      "  - Connector: NIXL (standard vLLM)\n",
      "  - GPU utilization: 20% (forces prefix eviction)\n",
      "  - Prefix caching: Enabled (but only in GPU memory)\n",
      "  - Model: /models/Qwen3-4B (pre-cached)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Deploy baseline (no KVBM)\n",
    "kubectl apply -f kvbm_baseline.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Baseline deployment created\"\n",
    "echo \"Models are pre-cached, this will take 1-2 minutes for model loading...\"\n",
    "echo \"\"\n",
    "echo \"Configuration:\"\n",
    "echo \"  - Connector: NIXL (standard vLLM)\"\n",
    "echo \"  - GPU utilization: 20% (forces prefix eviction)\"\n",
    "echo \"  - Prefix caching: Enabled (but only in GPU memory)\"\n",
    "echo \"  - Model: /models/Qwen3-4B (pre-cached)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a25b44",
   "metadata": {},
   "source": [
    "### Step 3: Monitor Baseline Deployment\n",
    "\n",
    "Wait for the pods to be ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38906341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline deployment:\n",
      "NAME                                                   READY   STATUS    RESTARTS   AGE\n",
      "vllm-kvbm-baseline-frontend-7d6bdf4d6d-7q22h           1/1     Running   0          4m19s\n",
      "vllm-kvbm-baseline-vllmdecodeworker-7fdd5c9688-wf7q2   0/1     Pending   0          4m19s\n",
      "\n",
      "Wait for all pods to show 1/1 Ready\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "echo \"Baseline deployment:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-baseline\n",
    "\n",
    "echo \"\"\n",
    "echo \"Wait for all pods to show 1/1 Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce13e7e",
   "metadata": {},
   "source": [
    "### Step 4: Verify Baseline Deployment\n",
    "\n",
    "Ensure the worker is healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Count ready workers\n",
    "BASELINE_READY=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-baseline,nvidia.com/dynamo-component-type=worker --field-selector=status.phase=Running -o json | jq '[.items[] | select(.status.conditions[] | select(.type==\"Ready\" and .status==\"True\"))] | length')\n",
    "\n",
    "echo \"Baseline workers ready: $BASELINE_READY/1\"\n",
    "\n",
    "if [ \"$BASELINE_READY\" = \"1\" ]; then\n",
    "    echo \"âœ“ Baseline deployment is healthy - ready to benchmark\"\n",
    "else\n",
    "    echo \"âš ï¸  Worker not ready. Wait before benchmarking.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda41d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Benchmark Baseline Performance\n",
    "\n",
    "### Understanding the Benchmark\n",
    "\n",
    "We'll use `aiperf` with these key parameters:\n",
    "- **`--prefix-prompt-length 20000`**: Each request has a 20,000 token prefix\n",
    "- **`--prompt-prefix-pool-size 5`**: Five different prefixes (cycles through them)\n",
    "- **`--request-count 30`**: 30 total requests (each prefix reused multiple times)\n",
    "\n",
    "This creates memory pressure by cycling through large prefixes, forcing eviction.\n",
    "\n",
    "### Step 1: Set Up Port Forwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Forward baseline frontend\n",
    "kubectl port-forward deployment/vllm-kvbm-baseline-frontend $USER_FRONTEND_PORT:8000 -n $NAMESPACE &\n",
    "\n",
    "echo \"âœ“ Baseline frontend: localhost:${USER_FRONTEND_PORT}\"\n",
    "sleep 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae70b8",
   "metadata": {},
   "source": [
    "### Step 2: Install AIPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install AIPerf if not already available\n",
    "pip install aiperf -q\n",
    "\n",
    "echo \"âœ“ AIPerf installed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf50aca",
   "metadata": {},
   "source": [
    "### Step 3: Run Baseline Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a174ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run baseline benchmark with prefix cycling\n",
    "aiperf profile \\\n",
    "  -m \"Qwen/Qwen3-4B\" \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  -u http://localhost:${USER_FRONTEND_PORT} \\\n",
    "  --synthetic-input-tokens-mean 128 \\\n",
    "  --synthetic-input-tokens-stddev 0 \\\n",
    "  --prefix-prompt-length 20000 \\\n",
    "  --prompt-prefix-pool-size 5 \\\n",
    "  --output-tokens-mean 50 \\\n",
    "  --concurrency 1 \\\n",
    "  --request-count 30 \\\n",
    "  --tokenizer \"Qwen/Qwen3-4B\" \\\n",
    "  --artifact-dir ./results_baseline_kvbm\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Baseline benchmark complete\"\n",
    "echo \"Results saved to: ./results_baseline_kvbm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18927ab6",
   "metadata": {},
   "source": [
    "### Step 4: Save Baseline Metrics\n",
    "\n",
    "Take note of key metrics from the output:\n",
    "- **Time to First Token (TTFT)**: Average and p99\n",
    "- **TTFT Variance**: High variance indicates prefix cache eviction/recomputation\n",
    "- **Request Latency**: Total end-to-end time\n",
    "- **Throughput**: Requests and tokens per second\n",
    "\n",
    "### Step 5: Delete Baseline Deployment\n",
    "\n",
    "Clean up before starting the KVBM experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Stop port forward\n",
    "pkill -f \"port-forward.*vllm-kvbm-baseline\"\n",
    "\n",
    "# Delete deployment\n",
    "kubectl delete dynamographdeployment vllm-kvbm-baseline -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Baseline deployment deleted\"\n",
    "echo \"Waiting for pods to terminate...\"\n",
    "sleep 10\n",
    "\n",
    "# Verify cleanup\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b4d5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Deploy KVBM Configuration\n",
    "\n",
    "### Objectives\n",
    "- Configure KVBM with CPU cache offloading\n",
    "- Deploy vLLM with KVBM connector on Kubernetes\n",
    "- Understand KVBM configuration parameters\n",
    "\n",
    "### Step 1: Review KVBM Deployment YAML\n",
    "\n",
    "The KVBM deployment uses the KVBM connector with CPU cache offloading. Review `kvbm_enabled.yaml`:\n",
    "\n",
    "Key configuration:\n",
    "- **Connector**: KVBM (with CPU offloading)\n",
    "- **GPU utilization**: 20% (same constraint as baseline)\n",
    "- **CPU cache**: 20GB (`DYN_KVBM_CPU_CACHE_GB=20`)\n",
    "- **Model**: Qwen/Qwen3-4B (same as baseline)\n",
    "- **Prefix caching**: Enabled (GPU + CPU tiers)\n",
    "\n",
    "### Step 2: Deploy KVBM Configuration\n",
    "\n",
    "**Note:** Models are pre-cached in the shared storage, so this should start quickly (1-2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e819bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Deploy KVBM-enabled configuration\n",
    "kubectl apply -f kvbm_enabled.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"âœ“ KVBM deployment created\"\n",
    "echo \"Models are pre-cached, this will take 1-2 minutes for model loading...\"\n",
    "echo \"\"\n",
    "echo \"Configuration:\"\n",
    "echo \"  - Connector: KVBM (with CPU offloading)\"\n",
    "echo \"  - GPU utilization: 20% (forces prefix eviction)\"\n",
    "echo \"  - CPU cache: 20GB (preserves evicted prefixes)\"\n",
    "echo \"  - Prefix caching: Enabled (GPU + CPU tiers)\"\n",
    "echo \"  - Model: /models/Qwen3-4B (pre-cached)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840c796",
   "metadata": {},
   "source": [
    "### Step 3: Monitor KVBM Deployment\n",
    "\n",
    "Wait for the pods to be ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "echo \"KVBM deployment:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled\n",
    "\n",
    "echo \"\"\n",
    "echo \"Wait for all pods to show 1/1 Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aee53a",
   "metadata": {},
   "source": [
    "### Step 4: Verify KVBM Deployment\n",
    "\n",
    "Ensure the worker is healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Count ready workers\n",
    "KVBM_READY=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker --field-selector=status.phase=Running -o json | jq '[.items[] | select(.status.conditions[] | select(.type==\"Ready\" and .status==\"True\"))] | length')\n",
    "\n",
    "echo \"KVBM workers ready: $KVBM_READY/1\"\n",
    "\n",
    "if [ \"$KVBM_READY\" = \"1\" ]; then\n",
    "    echo \"âœ“ KVBM deployment is healthy - ready to benchmark\"\n",
    "else\n",
    "    echo \"âš ï¸  Worker not ready. Wait before benchmarking.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6f385",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Benchmark KVBM Performance\n",
    "\n",
    "### Step 1: Set Up Port Forwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Forward KVBM frontend\n",
    "kubectl port-forward deployment/vllm-kvbm-enabled-frontend $USER_FRONTEND_PORT:8000 -n $NAMESPACE &\n",
    "\n",
    "echo \"âœ“ KVBM frontend: localhost:${USER_FRONTEND_PORT}\"\n",
    "sleep 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984b15f",
   "metadata": {},
   "source": [
    "### Step 2: Run KVBM Benchmark\n",
    "\n",
    "Run the exact same benchmark with KVBM enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f852d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run KVBM benchmark with identical parameters\n",
    "aiperf profile \\\n",
    "  -m \"Qwen/Qwen3-4B\" \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  -u http://localhost:${USER_FRONTEND_PORT} \\\n",
    "  --synthetic-input-tokens-mean 128 \\\n",
    "  --synthetic-input-tokens-stddev 0 \\\n",
    "  --prefix-prompt-length 20000 \\\n",
    "  --prompt-prefix-pool-size 5 \\\n",
    "  --output-tokens-mean 50 \\\n",
    "  --concurrency 1 \\\n",
    "  --request-count 30 \\\n",
    "  --tokenizer \"Qwen/Qwen3-4B\" \\\n",
    "  --artifact-dir ./results_kvbm\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ KVBM benchmark complete\"\n",
    "echo \"Results saved to: ./results_kvbm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38141a29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Analyze and Compare Results\n",
    "\n",
    "### Key Metrics to Compare\n",
    "\n",
    "| Metric | What It Shows | What to Look For |\n",
    "|--------|---------------|------------------|\n",
    "| **TTFT Average** | Average time to first token | Lower with KVBM (less recomputation) |\n",
    "| **TTFT p99** | Worst-case latency | Much lower with KVBM (no full recomputation) |\n",
    "| **TTFT Variance** | Consistency across requests | Lower with KVBM (stable cache access) |\n",
    "| **Request Latency** | Total request time | Lower with KVBM |\n",
    "| **Throughput** | Requests/sec and tokens/sec | Higher with KVBM |\n",
    "\n",
    "### Understanding the Results\n",
    "\n",
    "**What Happens Without KVBM (Baseline):**\n",
    "\n",
    "1. Requests 1-4 arrive with different 20K token prefixes\n",
    "2. GPU KV cache fills up completely\n",
    "3. Request 5 arrives â†’ must evict oldest prefix blocks (Request 1)\n",
    "4. Later request reuses Request 1's prefix â†’ **must recompute all 20K tokens** ðŸŒ\n",
    "5. High TTFT variance as prefixes are repeatedly evicted and recomputed\n",
    "\n",
    "**What Happens With KVBM:**\n",
    "\n",
    "1. Requests 1-4 arrive with different 20K token prefixes\n",
    "2. GPU KV cache fills up completely\n",
    "3. Request 5 arrives â†’ **offloads** Request 1's blocks to CPU (preserved) âœ…\n",
    "4. Later request reuses Request 1's prefix â†’ **fetches from CPU to GPU** âš¡\n",
    "5. Consistent TTFT as prefixes are preserved and quickly restored\n",
    "\n",
    "### Example Comparison\n",
    "\n",
    "```text\n",
    "Baseline (No KVBM):\n",
    "  TTFT avg: 3,200 ms     p99: 12,500 ms  (huge variance!)\n",
    "  Request Latency avg: 4,800 ms\n",
    "  Throughput: 6.2 req/sec\n",
    "\n",
    "  ðŸ“Š High p99 indicates frequent full recomputation of 20K token prefixes\n",
    "\n",
    "KVBM (20GB CPU Cache):\n",
    "  TTFT avg: 1,800 ms     p99: 3,200 ms   (44% faster avg, 74% faster p99! âœ…)\n",
    "  Request Latency avg: 3,400 ms  (29% faster âœ…)\n",
    "  Throughput: 8.8 req/sec  (42% higher âœ…)\n",
    "\n",
    "  ðŸ“Š Lower p99 shows prefixes preserved in CPU and quickly restored\n",
    "```\n",
    "\n",
    "### Why KVBM Improves Performance\n",
    "\n",
    "1. **Eliminates Redundant Computation**: Prefixes stored in CPU don't need recomputation\n",
    "2. **Reduces TTFT Variance**: CPU fetch (~200ms) vs full recompute (~10s)\n",
    "3. **Increases Effective Capacity**: 20GB CPU cache extends usable KV cache beyond GPU\n",
    "4. **Enables Better Resource Utilization**: GPU focused on active inference, CPU stores inactive blocks\n",
    "\n",
    "### When KVBM Provides Maximum Benefit\n",
    "\n",
    "**Enable KVBM when:**\n",
    "- Long prefixes (>5000 tokens) are reused across requests\n",
    "- System prompts or templates are shared across many requests\n",
    "- Multi-turn conversations with context carryover\n",
    "- Document Q&A where the same document is queried multiple times\n",
    "- GPU memory is constrained and cache eviction is frequent\n",
    "\n",
    "**KVBM provides less benefit when:**\n",
    "- Every request has a unique prompt with no reuse\n",
    "- Short prompts (<1000 tokens) where cache overhead dominates\n",
    "- Abundant GPU memory where eviction rarely occurs\n",
    "- Workloads are I/O bound rather than compute bound\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Optional - Enable Disk Offloading\n",
    "\n",
    "For even larger capacity, enable disk tier offloading by modifying the KVBM deployment:\n",
    "\n",
    "### Step 1: Create Multi-Tier KVBM Deployment\n",
    "\n",
    "Modify your `kvbm_enabled.yaml` to add disk tier configuration:\n",
    "\n",
    "```yaml\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "        - name: DYN_KVBM_CPU_CACHE_GB\n",
    "          value: \"16\"  # 16GB CPU cache\n",
    "        - name: DYN_KVBM_DISK_CACHE_GB\n",
    "          value: \"100\"  # 100GB disk cache\n",
    "        - name: DYN_KVBM_DISK_CACHE_DIR\n",
    "          value: \"/mnt/nvme/kvbm_cache\"  # Use fast SSD\n",
    "        - name: DYN_KVBM_LEADER_WORKER_INIT_TIMEOUT_SECS\n",
    "          value: \"600\"  # Increased timeout for large caches\n",
    "```\n",
    "\n",
    "And add a volume mount for disk cache:\n",
    "\n",
    "```yaml\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        - name: kvbm-disk-cache\n",
    "          hostPath:\n",
    "            path: /mnt/nvme/kvbm_cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          - name: kvbm-disk-cache\n",
    "            mountPath: /mnt/nvme/kvbm_cache\n",
    "```\n",
    "\n",
    "### Step 2: Deploy Multi-Tier Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Apply the updated YAML\n",
    "kubectl apply -f kvbm_enabled.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Multi-tier KVBM configuration deployed\"\n",
    "echo \"\"\n",
    "echo \"Configuration:\"\n",
    "echo \"  CPU cache:  16GB\"\n",
    "echo \"  Disk cache: 100GB (on /mnt/nvme/kvbm_cache)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de204f2b",
   "metadata": {},
   "source": [
    "**Note:** Disk offloading is slower than CPU but provides massive capacity for very large workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 8: Monitoring KVBM (Optional)\n",
    "\n",
    "### View KVBM Logs\n",
    "\n",
    "Watch KVBM operations in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ce0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get KVBM worker pod name\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "echo \"Viewing logs from KVBM worker: $KVBM_POD\"\n",
    "echo \"\"\n",
    "kubectl logs -n $NAMESPACE $KVBM_POD --tail=50 --follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022956",
   "metadata": {},
   "source": [
    "Look for messages like:\n",
    "\n",
    "```text\n",
    "[INFO] KVBM initialized with 20GB CPU cache\n",
    "[DEBUG] Offloading block with sequence hash 1234567890 to target pool.\n",
    "[DEBUG] Onboarding 3 blocks to target pool.\n",
    "[DEBUG] Adding blocks to pool\n",
    "[DEBUG] Returning blocks to pool\n",
    "```\n",
    "\n",
    "### Monitor Resource Usage\n",
    "\n",
    "Watch GPU and memory utilization from the worker pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Execute nvidia-smi inside the worker pod\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- nvidia-smi\n",
    "\n",
    "# Check system memory\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc022b9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Cleanup\n",
    "\n",
    "When you're done with the KVBM experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete KVBM deployment (baseline was already deleted)\n",
    "kubectl delete dynamographdeployment vllm-kvbm-enabled -n $NAMESPACE\n",
    "\n",
    "# Stop port forwards\n",
    "pkill -f \"port-forward.*vllm-kvbm\"\n",
    "\n",
    "echo \"âœ“ Cleanup complete\"\n",
    "echo \"Note: Your namespace and platform from Lab 1 are still running\"\n",
    "echo \"KVBM cache is automatically cleaned on pod shutdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d529017",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### KVBM Not Initializing\n",
    "\n",
    "If you see errors about KVBM initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150729c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check KVBM worker logs\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "kubectl logs -n $NAMESPACE $KVBM_POD --tail=100\n",
    "\n",
    "# Verify etcd and NATS are running from Lab 1\n",
    "kubectl get pods -n $NAMESPACE | grep -E \"(etcd|nats)\"\n",
    "\n",
    "# Check KVBM environment variables in pod\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- env | grep DYN_KVBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6feec6",
   "metadata": {},
   "source": [
    "### High Memory Usage\n",
    "\n",
    "If system memory is exhausted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0943012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Edit deployment to reduce CPU cache size\n",
    "# Change DYN_KVBM_CPU_CACHE_GB from \"20\" to \"8\" in kvbm_enabled.yaml\n",
    "# Then reapply:\n",
    "kubectl apply -f kvbm_enabled.yaml -n $NAMESPACE\n",
    "\n",
    "# Check memory usage on worker pod\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493fa1e",
   "metadata": {},
   "source": [
    "### Disk Offloading Issues\n",
    "\n",
    "If disk offloading fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure directory exists and is writable on the node\n",
    "# You may need cluster admin access for this\n",
    "kubectl debug node/<node-name> -it --image=ubuntu -- bash\n",
    "# Then inside the debug pod:\n",
    "# mkdir -p /host/mnt/nvme/kvbm_cache\n",
    "# chmod 755 /host/mnt/nvme/kvbm_cache\n",
    "\n",
    "# Check available disk space from worker pod\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- df -h /mnt/nvme/kvbm_cache\n",
    "\n",
    "# Check pod events for volume mount issues\n",
    "kubectl describe pod -n $NAMESPACE $KVBM_POD | grep -A 10 Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb9f8e",
   "metadata": {},
   "source": [
    "### Slow Benchmark Performance\n",
    "\n",
    "If benchmarks are unexpectedly slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef86fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker pod resource usage\n",
    "KVBM_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-namespace=vllm-kvbm-enabled,nvidia.com/dynamo-component-type=worker -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "# Check GPU utilization\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- nvidia-smi\n",
    "\n",
    "# Check CPU throttling\n",
    "kubectl exec -n $NAMESPACE $KVBM_POD -- lscpu | grep MHz\n",
    "\n",
    "# For faster testing, reduce prefix size:\n",
    "# Use --prefix-prompt-length 10000 instead of 20000 in aiperf command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b849a3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- âœ… How KVBM extends KV cache capacity beyond GPU memory\n",
    "- âœ… Deploying baseline and KVBM-enabled serving configurations\n",
    "- âœ… Benchmarking prefix cache reuse with realistic workloads\n",
    "- âœ… Analyzing performance differences between offloading strategies\n",
    "- âœ… Configuring multi-tier memory management (GPU + CPU + Disk)\n",
    "\n",
    "### Key Takeaways\n",
    "- **KVBM** eliminates redundant prefix recomputation by preserving blocks in CPU/disk\n",
    "- **Prefix reuse** workloads can see 30-75% TTFT improvement with KVBM\n",
    "- **Multi-tier offloading** enables handling far more concurrent requests\n",
    "- **CPU cache** provides fast offload target; disk provides massive capacity\n",
    "- **Best for workloads** with long, reusable prefixes (system prompts, documents, conversations)\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Aspect | Baseline (NIXL) | KVBM (20GB CPU) | Improvement |\n",
    "|--------|----------------|-----------------|-------------|\n",
    "| TTFT Average | 3,200 ms | 1,800 ms | **44% faster** |\n",
    "| TTFT p99 | 12,500 ms | 3,200 ms | **74% faster** |\n",
    "| Throughput | 6.2 req/sec | 8.8 req/sec | **42% higher** |\n",
    "| Consistency | High variance | Low variance | **Much more stable** |\n",
    "\n",
    "### Next Steps\n",
    "- Combine KVBM with KV-aware routing from the previous lab for maximum efficiency\n",
    "- Profile your production workload to determine optimal tier sizes\n",
    "- Set up monitoring with Prometheus and Grafana dashboards\n",
    "- Experiment with different models and prefix patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- ðŸ“– [KVBM Architecture Documentation](../../dynamo/docs/architecture/kvbm_architecture.md)\n",
    "- ðŸ”§ [KVBM Configuration Guide](../../dynamo/docs/guides/run_kvbm_in_vllm.md)\n",
    "- ðŸ“Š [NIXL Memory Transfer Layer](https://github.com/ai-dynamo/nixl)\n",
    "- ðŸŽ¯ [LMBenchmark Testing Framework](https://github.com/LMCache/LMBenchmark)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! ðŸŽ‰** You've successfully compared baseline and KVBM-enabled serving!\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "bash",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
