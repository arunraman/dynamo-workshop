{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: Introduction and Docker-Based Deployment\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this lab, you will:\n",
        "- Set up the Dynamo environment\n",
        "- Deploy Dynamo using Docker-based aggregated deployment\n",
        "- Configure a backend engine (to be determined during the lab)\n",
        "- Benchmark the deployment using AI-Perf\n",
        "\n",
        "## Duration: ~90 minutes\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup\n",
        "\n",
        "### Objectives\n",
        "- Install prerequisites (Docker, Python, uv)\n",
        "- Verify system requirements (GPU availability)\n",
        "- Install Dynamo dependencies\n",
        "\n",
        "### Tasks\n",
        "- [ ] Check Python version\n",
        "- [ ] Install Docker and Docker Compose\n",
        "- [ ] Install uv package manager\n",
        "- [ ] Verify GPU access\n",
        "- [ ] Install Python development headers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Docker-Based Aggregated Deployment\n",
        "\n",
        "### Objectives\n",
        "- Understand aggregated serving architecture\n",
        "- Deploy etcd and NATS using Docker Compose\n",
        "- Deploy Dynamo frontend and router\n",
        "- Deploy inference workers with selected backend\n",
        "\n",
        "### Architecture\n",
        "```\n",
        "Client → Frontend → Router → Worker(s) with Backend Engine\n",
        "                        ↓\n",
        "                   etcd + NATS\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "- [ ] Review Docker Compose configuration\n",
        "- [ ] Start etcd and NATS services\n",
        "- [ ] Deploy Dynamo frontend\n",
        "- [ ] Select and deploy backend engine (vLLM/SGLang/TensorRT-LLM)\n",
        "- [ ] Verify deployment health\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Backend Engine Selection\n",
        "\n",
        "### Available Backends\n",
        "- **vLLM**: High-throughput serving with PagedAttention\n",
        "- **SGLang**: Optimized for complex prompting and structured generation\n",
        "- **TensorRT-LLM**: Maximum performance on NVIDIA GPUs\n",
        "\n",
        "### Tasks\n",
        "- [ ] Review backend engine options\n",
        "- [ ] Select appropriate backend for use case\n",
        "- [ ] Configure backend-specific parameters\n",
        "- [ ] Deploy selected backend\n",
        "- [ ] Test basic inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Testing and Validation\n",
        "\n",
        "### Objectives\n",
        "- Send test requests to the deployment\n",
        "- Verify OpenAI API compatibility\n",
        "- Test streaming and non-streaming responses\n",
        "\n",
        "### Tasks\n",
        "- [ ] Send sample chat completion request\n",
        "- [ ] Test streaming responses\n",
        "- [ ] Verify response format\n",
        "- [ ] Test different parameters (temperature, max_tokens, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Benchmarking with AI-Perf\n",
        "\n",
        "### Objectives\n",
        "- Install and configure AI-Perf benchmarking tool\n",
        "- Run performance benchmarks\n",
        "- Analyze throughput, latency, and token metrics\n",
        "- Compare performance across different configurations\n",
        "\n",
        "### Metrics to Measure\n",
        "- Throughput (requests/second, tokens/second)\n",
        "- Latency (TTFT, TPOT, end-to-end)\n",
        "- GPU utilization\n",
        "- KV cache efficiency\n",
        "\n",
        "### Tasks\n",
        "- [ ] Install AI-Perf\n",
        "- [ ] Configure benchmark parameters\n",
        "- [ ] Run baseline benchmark\n",
        "- [ ] Analyze results\n",
        "- [ ] Experiment with different load patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Exercises and Exploration\n",
        "\n",
        "### Exercise 1: Parameter Tuning\n",
        "- Adjust backend engine parameters\n",
        "- Measure impact on performance\n",
        "\n",
        "### Exercise 2: Load Testing\n",
        "- Test with increasing concurrent requests\n",
        "- Identify bottlenecks\n",
        "\n",
        "### Exercise 3: Comparison\n",
        "- Try different backend engines\n",
        "- Compare performance characteristics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What You Learned\n",
        "- ✅ How to set up Dynamo environment\n",
        "- ✅ Docker-based aggregated deployment architecture\n",
        "- ✅ Backend engine selection and configuration\n",
        "- ✅ Performance benchmarking with AI-Perf\n",
        "\n",
        "### Key Takeaways\n",
        "- Aggregated serving is simpler to deploy and manage\n",
        "- Different backends have different performance characteristics\n",
        "- AI-Perf provides comprehensive performance insights\n",
        "\n",
        "### Next Steps\n",
        "In **Lab 2**, you'll deploy Dynamo on Kubernetes with both aggregated and disaggregated serving topologies.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
