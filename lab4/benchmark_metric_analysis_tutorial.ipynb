{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bae3b6a",
   "metadata": {},
   "source": [
    "# Benchmark Metric Analysis Tutorial\n",
    "This interactive notebook guides you through analysising the benchmark metrics.\n",
    "\n",
    "- In this guide, we will introduce some benchmarking concepts, including metrics such as TTFT (Time to First Token) and ITL (Inter-Token Latency).\n",
    "- Understanding these metrics helps correctly interpret aiperf results.\n",
    "- This understanding lets you\n",
    "    - Identify performance bottlenecks\n",
    "    - Tune parameters to improve inference performance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eaeb5d",
   "metadata": {},
   "source": [
    "## Part 1: Time to First Token(TTFT) Analysis\n",
    "TTFT shows how long a user needs to wait before seeing the model’s output. This is the time it takes from submitting the query to receiving the first token (if the response is not empty). Time to first token generally includes both request queuing time, prefill time and network latency. \n",
    "\n",
    "![TTFT Def](images/ttft_def.png)\n",
    "\n",
    "In practice, there are usually several ways to reduce TTFT. Here, we briefly list one example that controls TTFT through the `max_num_tokens` parameter. The definition of `max_num_tokens` is: it specifies the maximum number of batched input tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb140dc",
   "metadata": {},
   "source": [
    "![TTFT](images/ttft_illustration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a02f7",
   "metadata": {},
   "source": [
    "### Now let's test it\n",
    "The AIPerf test command is as follows, with an input sequence length of 4096 and an output sequence length of 200.\n",
    "\n",
    "```bash\n",
    "aiperf profile \\\n",
    "  --model Qwen/Qwen3-32B-FP8 \\\n",
    "  --url http://localhost:8000 \\\n",
    "  --endpoint-type chat \\\n",
    "  --num-requests 100 \\\n",
    "  --concurrency 8 \\\n",
    "  --isl 4096 \\\n",
    "  --osl 200 \\\n",
    "  --streaming\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c106f",
   "metadata": {},
   "source": [
    "The first test scenario sets a smaller `max_num_tokens` to ensure that only one prefill is processed at a iteration.\n",
    "\n",
    "```yaml\n",
    "build_config:\n",
    "  max_num_tokens: 5000\n",
    "  max_batch_size: 8\n",
    "\n",
    "kv_cache_config:\n",
    "  dtype: fp8\n",
    "  enable_block_reuse: false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49cb60",
   "metadata": {},
   "source": [
    "![TTFT with large max_num_tokens](images/ttft_seq_prefill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44db63",
   "metadata": {},
   "source": [
    "The second test scenario sets a larger `max_num_tokens` to ensure that all the prefill are processed at a iteration.\n",
    "\n",
    "```yaml\n",
    "build_config:\n",
    "  max_num_tokens: 65536\n",
    "  max_batch_size: 8\n",
    "\n",
    "kv_cache_config:\n",
    "  dtype: fp8\n",
    "  enable_block_reuse: false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386abeb8",
   "metadata": {},
   "source": [
    "![TTFT with small max_num_tokens](images/ttft_batch_prefill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac858202",
   "metadata": {},
   "source": [
    "We can clearly see that compared to a limited `max_num_tokens`, an excessively large value causes a sharp increase in TTFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e4678",
   "metadata": {},
   "source": [
    "For disaggregated serving, another important factor to consider is at which stage the first token is returned. Typically, there are two implementations in the industry: one returns immediately after the prefill stage, while the other returns only after the KV cache has been transferred to the decode worker.\n",
    "\n",
    "![TTFT disagg](images/ttft_disagg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4247f80",
   "metadata": {},
   "source": [
    "In general, the return stage can be roughly identified by checking the time to second token — a higher value typically indicates the first approach, while a lower one suggests the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5af826",
   "metadata": {},
   "source": [
    "## Part 2: Inter-token Latency Analysis (ITL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697bfc0",
   "metadata": {},
   "source": [
    "ITL is defined as the average time between consecutive tokens. (Different benchmark tools may calculate ITL differently; here take AIPerf as an example)\n",
    "$$ITL = \\frac{Request \\ latency - \\ TTFT}{Total \\ output \\ tokens - 1}$$\n",
    "![ITL def](images/itl_def.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e784de4",
   "metadata": {},
   "source": [
    "Output tokens per user correspond to $\\frac{1000}{ITL}$ (1000 for millisecond), which reflects the per-user output token throughput (here noted as TPS per user).\n",
    "![ITL p50](images/itl_p50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14097a3f",
   "metadata": {},
   "source": [
    "Even though the p50 ITL × p50 TPS per user is roughly 1000, let’s move on to analyze the average ITL and average TPS per user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b2089",
   "metadata": {},
   "source": [
    "![ITL avg](images/itl_avg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb382fa",
   "metadata": {},
   "source": [
    "Obviously, the average ITL multiplied by the average TPS per user does not equal 1000 — apart from statistical factors, this partially reveals the characteristics of the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15521f3f",
   "metadata": {},
   "source": [
    "## Part 3: Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79f720",
   "metadata": {},
   "source": [
    "This section briefly lists some configurations that may affect performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb68de",
   "metadata": {},
   "source": [
    "### Cuda Graph Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22681913",
   "metadata": {},
   "source": [
    "Currently, three frameworks support CUDA Graph. When configuring it, note that enabling padding may reduce performance. The following are two different configurations.\n",
    "The first one is:\n",
    "```yaml\n",
    "cuda_graph_config:\n",
    "  batch_sizes: [1,4,8,16,32,64]\n",
    "  enable_padding: true\n",
    "```\n",
    "And its performance is:\n",
    "![ITL avg](images/cuda_graph_skip.png)\n",
    "The second one is:\n",
    "```yaml\n",
    "cuda_graph_config:\n",
    "  batch_sizes: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,\n",
    "        26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,\n",
    "        51,52,53,54,55,56,57,58,59,60,61,62,63,64]\n",
    "```\n",
    "And its performance is:\n",
    "![ITL avg](images/cuda_graph_seq.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c75858",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
