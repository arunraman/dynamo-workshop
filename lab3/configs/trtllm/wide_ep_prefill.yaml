# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# TensorRT-LLM Wide EP Prefill Worker Configuration
# For DeepSeek-R1 deployment with disaggregated serving

backend: pytorch

# WideEP and MoE configuration
moe_config:
  backend: WIDEEP
  # Path to EPLB configuration file
  load_balancer: ./configs/trtllm/eplb.yaml
  
  # Maximum tokens processed by MoE layers
  # Recommendation: moe_max_num_tokens = max_batch_size * moe_expert_parallel_size
  # Example: 4096 = 256 * 16
  # If unspecified, defaults to max_num_tokens
  # moe_max_num_tokens: 4096

# Parallelism configuration
tensor_parallel_size: 16          # TP across 16 GPUs
moe_expert_parallel_size: 16      # EP across 16 GPUs (for 256 experts)
pipeline_parallel_size: 1         # No pipeline parallelism
enable_attention_dp: true         # Enable data parallel attention

# Batch and sequence settings
max_batch_size: 256               # Maximum batch size
max_num_tokens: 256               # Maximum tokens per iteration
max_seq_len: 8448                 # 8192 input + 256 output

# KV cache configuration with FP8 quantization
kv_cache_config:
  # With DP attention enabled, use lower fraction to ensure enough memory
  # Without DP attention, can use higher values like 0.85
  free_gpu_memory_fraction: 0.30
  
  # FP8 quantization for KV cache (saves 50% memory)
  # Options: fp8, fp16, bf16
  dtype: fp8

# CUDA graph configuration for optimization
cuda_graph_config:
  enable_padding: true
  # Explicitly specify batch sizes for CUDA graph compilation
  # These should cover your common batch sizes for best performance
  batch_sizes:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256

# Scheduler configuration
# NOTE: overlap_scheduler enabled by default
disable_overlap_scheduler: false

# Logging
print_iter_log: true

# KV cache transfer backend for disaggregation
cache_transceiver_config:
  backend: DEFAULT  # or NIXL for RDMA-based transfer

