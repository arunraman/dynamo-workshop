# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# TensorRT-LLM Wide EP Aggregated Mode Configuration
# For DeepSeek-R1 deployment without disaggregation (single worker handles both prefill and decode)

backend: pytorch

# WideEP and MoE configuration
moe_config:
  backend: WIDEEP
  # Path to EPLB configuration file
  load_balancer: ./configs/trtllm/eplb.yaml
  
  # moe_max_num_tokens will default to max_num_tokens if left unspecified
  # If you want to set this value explicitly, recommendation:
  #   moe_max_num_tokens = max_batch_size * moe_expert_parallel_size
  #   Example: 4096 = 256 * 16
  # moe_max_num_tokens: 4096

# Parallelism configuration
tensor_parallel_size: 16          # TP across 16 GPUs
moe_expert_parallel_size: 16      # EP across 16 GPUs

# Enable data parallel attention
enable_attention_dp: true

# Batch and sequence settings
max_batch_size: 256
max_num_tokens: 256
max_seq_len: 8448

# KV cache configuration
kv_cache_config:
  # With DP attention, use conservative memory fraction
  free_gpu_memory_fraction: 0.30
  
  # FP8 quantization for KV cache
  dtype: fp8

# CUDA graph configuration
cuda_graph_config:
  enable_padding: true
  batch_sizes:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256

# Logging
print_iter_log: true

# Notes:
# - Aggregated mode is simpler to deploy but less flexible
# - No disaggregation means prefill and decode share the same resources
# - Good for smaller deployments or testing
# - For production, disaggregated mode (separate prefill/decode) is recommended

