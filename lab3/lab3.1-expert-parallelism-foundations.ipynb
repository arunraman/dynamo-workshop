{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: Expert Parallelism Foundations\n",
    "\n",
    "Welcome! This notebook starts from first principles and builds up to Expert Parallelism (EP) for Mixture-of-Experts (MoE) models in Dynamo. You'll learn the main parallelism strategies, why EP exists, and how Dynamo supports WideEP, DeepEP, and dynamic load balancing (EPLB). You'll also run small, illustrative Python snippets to visualize concepts.\n",
    "\n",
    "**Duration**: 45-60 minutes\n",
    "\n",
    "**Next**: After completing this lab, continue to **Lab 3.2: Wide EP Production Deployment** to learn how to deploy these concepts in production with Kubernetes, SGLang, and TensorRT-LLM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Explain why parallelism is needed for LLM inference (prefill vs decode)\n",
    "- Differentiate DP, TP, PP, SP, and EP\n",
    "- Describe what MoE is and why EP applies only to MoE models\n",
    "- Understand Expert Parallelism in Dynamo (Standard, Wide, Deep, Dynamic/EPLB)\n",
    "- Identify when to use Wide-EP and how EPLB balances load\n",
    "- Interpret high-level NVL72 Wide-EP insights and their implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "**Foundations (Sections 1-5)**\n",
    "1. [Why Parallelism Matters for LLMs](#1.-Why-Parallelism-Matters-for-LLMs)\n",
    "2. [Parallelism Strategies at a Glance](#2.-Parallelism-Strategies-at-a-Glance)\n",
    "3. [From Dense to MoE: Why Experts?](#3.-From-Dense-to-MoE:-Why-Experts?)\n",
    "   - [3.1 How MoE Expert Selection Works](#3.1-How-MoE-Expert-Selection-Works-(Flat-Expert-Pool))\n",
    "4. [Expert Parallelism (EP): Core Idea](#4.-Expert-Parallelism-(EP):-Core-Idea)\n",
    "5. [Expert Parallelism in Dynamo](#5.-Expert-Parallelism-in-Dynamo)\n",
    "\n",
    "**Deep Dives (Section 6)**\n",
    "- [6. Deep Dives: EP Variants](#6.-Deep-Dives:-EP-Variants)\n",
    "  - 6.1 [Standard EP](#6.1-Deep-Dive:-Standard-EP)\n",
    "  - 6.2 [Wide EP](#6.2-Deep-Dive:-Wide-EP)\n",
    "  - 6.3 [Deep EP](#6.3-Deep-Dive:-Deep-EP)\n",
    "  - 6.4 [Dynamic EP (EPLB)](#6.4-Deep-Dive:-Dynamic-EP-(EPLB))\n",
    "\n",
    "**Hands-On (Section 7)**\n",
    "- [7. Hands-on Exercises](#7.-Hands-on-Exercises)\n",
    "  - [Exercise 1: WideEP Routing Simulation](#Exercise-1-‚Äî-WideEP-Routing-Simulation)\n",
    "  - [Exercise 2: Throughput Comparison Chart](#Exercise-2-‚Äî-Throughput-Comparison-Chart)\n",
    "\n",
    "**Advanced Topics (Section 8)**\n",
    "- [8. Advanced: Large-Scale Wide-EP on GB200 NVL72](#8.-Advanced:-Large-Scale-Wide-EP-on-GB200-NVL72)\n",
    "  - [GroupGEMM and Weight-Loading Intuition](#GroupGEMM-and-Weight-Loading-Intuition)\n",
    "  - [When to use Wide-EP (NVL72 perspective)](#When-to-use-Wide-EP-(NVL72-perspective))\n",
    "\n",
    "**Resources (Section 9)**\n",
    "- [9. Further Reading](#9.-Further-Reading)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "- **Beginners**: Start with Sections 1-5 for core concepts\n",
    "- **Intermediate**: Explore Section 6 (Deep Dives) for EP variants\n",
    "- **Advanced**: Read Section 8 for NVL72 insights and state-of-the-art optimizations\n",
    "- **Hands-on learners**: Run the exercises in Section 7 to build intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Parallelism Matters for LLMs\n",
    "\n",
    "Large Language Models are big and compute-intensive. Parallelism helps distribute memory and compute across multiple GPUs/nodes.\n",
    "\n",
    "### LLM Inference Phases\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  LLM Inference                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ                                 ‚îÇ\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Phase 1 ‚îÇ                       ‚îÇ Phase 2 ‚îÇ\n",
    "   ‚îÇ PREFILL ‚îÇ                       ‚îÇ DECODE  ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "- **Prefill**: processes prompt tokens in parallel (compute-bound)\n",
    "- **Decode**: generates one token at a time using KV cache (memory-bound)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallelism Strategies at a Glance\n",
    "\n",
    "| Type | Description | Key Benefit | Applies To |\n",
    "|------|-------------|-------------|------------|\n",
    "| Data Parallelism (DP) | Same model copy on each GPU, different data shards | Simple throughput scaling | Dense + MoE |\n",
    "| Tensor Parallelism (TP) | Split individual layers across GPUs | Fit larger models than one GPU | Dense + MoE |\n",
    "| Pipeline Parallelism (PP) | Split model layers into stages across GPUs | Memory-efficient for deep models | Dense + MoE |\n",
    "| Sequence Parallelism (SP) | Distribute sequence tokens across GPUs for attention | Efficient long sequences | Dense + MoE |\n",
    "| Expert Parallelism (EP) | Distribute MoE experts across GPUs, route tokens | Scale capacity without linear compute | MoE only |\n",
    "\n",
    "**Notes:**\n",
    "- DP/TP/PP/SP apply to all models.\n",
    "- EP applies only to MoE models (because only MoE has experts).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Dense to MoE: Why Experts?\n",
    "\n",
    "Dense models activate all parameters for every token. MoE models activate only a small subset of experts (e.g., top-1 or top-2) per token.\n",
    "\n",
    "**Example:**\n",
    "- Dense: 200B parameters ‚Üí all active per token\n",
    "- MoE: 1T parameters ‚Üí ~200B active per token (top-k experts)\n",
    "\n",
    "This gives huge capacity without proportional compute cost. The challenge becomes routing tokens to the right experts efficiently.\n",
    "\n",
    "### 3.1 How MoE Expert Selection Works (Flat Expert Pool)\n",
    "\n",
    "Most production MoE models (DeepSeek-V3, Qwen3, Mixtral) use a **flat expert pool** architecture. Let's understand how it works:\n",
    "\n",
    "#### The Router Network\n",
    "\n",
    "```\n",
    "Input Token Embedding\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Router  ‚îÇ ‚Üê Small neural network (learned during training)\n",
    "    ‚îÇ Network ‚îÇ    Outputs: score for EACH expert\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "   [s‚ÇÄ, s‚ÇÅ, s‚ÇÇ, ..., s‚ÇÇ‚ÇÖ‚ÇÖ]  ‚Üê Scores for all 256 experts\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    Top-K Selection (e.g., k=8)\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "   [E‚ÇÅ‚ÇÇ, E‚ÇÑ‚ÇÖ, E‚Çá‚Çà, E‚ÇÅ‚ÇÇ‚ÇÉ, E‚ÇÅ‚ÇÖ‚ÇÜ, E‚ÇÅ‚Çà‚Çâ, E‚ÇÇ‚ÇÄ‚ÇÅ, E‚ÇÇ‚ÇÉ‚ÇÑ]\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "   Weighted Sum ‚Üí Output\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Router Network**: A small learned neural network that:\n",
    "   - Takes token embedding as input\n",
    "   - Outputs a score for **every expert** in the pool\n",
    "   - Trained end-to-end with the model\n",
    "\n",
    "2. **Top-K Selection**: \n",
    "   - Selects the k experts with highest scores\n",
    "   - Only these experts process the token\n",
    "   - Others are completely skipped (sparse activation)\n",
    "\n",
    "3. **Weighted Combination**:\n",
    "   - Each selected expert's output is weighted by its score\n",
    "   - Final output = weighted sum of expert outputs\n",
    "\n",
    "#### Example: DeepSeek-V3 Configuration\n",
    "\n",
    "- **Total experts**: 256 (flat pool)\n",
    "- **Active per token**: 8 (top-8 selection)\n",
    "- **Activation ratio**: 8/256 = 3.1% of experts per token\n",
    "- **Effective compute**: ~37B parameters active (out of 671B total)\n",
    "\n",
    "#### Why \"Flat\" Pool?\n",
    "\n",
    "All 256 experts are:\n",
    "- ‚úÖ Scored simultaneously by the router\n",
    "- ‚úÖ Treated equally (no hierarchy or grouping)\n",
    "- ‚úÖ Can be selected for any token (maximum flexibility)\n",
    "\n",
    "**Contrast with hierarchical** (rare in practice):\n",
    "- ‚ùå Would have multiple routing stages\n",
    "- ‚ùå Experts grouped by domain (Language/Reasoning/etc.)\n",
    "- ‚ùå First choose group, then choose expert within group\n",
    "\n",
    "#### Hands-On: Simulating MoE Routing\n",
    "\n",
    "Let's simulate how a router selects experts for different tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate MoE expert routing with flat expert pool\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration (similar to DeepSeek-V3)\n",
    "NUM_EXPERTS = 256\n",
    "TOP_K = 8\n",
    "NUM_TOKENS = 5\n",
    "\n",
    "# Simulate router scores for different tokens\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MoE Expert Routing Simulation (Flat Expert Pool)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total experts: {NUM_EXPERTS}\")\n",
    "print(f\"Active experts per token: {TOP_K}\")\n",
    "print(f\"Activation ratio: {TOP_K/NUM_EXPERTS*100:.1f}%\\n\")\n",
    "\n",
    "# Simulate routing for different tokens\n",
    "tokens = [\"The\", \"quantum\", \"entanglement\", \"phenomenon\", \"is\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(tokens), figsize=(15, 3))\n",
    "fig.suptitle(\"Expert Selection for Different Tokens (Top-8 from 256 experts)\", fontsize=14)\n",
    "\n",
    "for idx, token in enumerate(tokens):\n",
    "    # Simulate router scores (in practice, these come from a learned neural network)\n",
    "    router_scores = np.random.randn(NUM_EXPERTS)\n",
    "    \n",
    "    # Top-K selection\n",
    "    top_k_indices = np.argsort(router_scores)[-TOP_K:][::-1]\n",
    "    top_k_scores = router_scores[top_k_indices]\n",
    "    \n",
    "    # Normalize scores to weights (softmax-like)\n",
    "    weights = np.exp(top_k_scores) / np.sum(np.exp(top_k_scores))\n",
    "    \n",
    "    print(f\"Token: '{token}'\")\n",
    "    print(f\"  Selected experts: {top_k_indices.tolist()}\")\n",
    "    print(f\"  Weights: {[f'{w:.3f}' for w in weights]}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize\n",
    "    ax = axes[idx]\n",
    "    colors = ['#ff6b6b' if i in top_k_indices else '#e0e0e0' for i in range(NUM_EXPERTS)]\n",
    "    ax.bar(range(NUM_EXPERTS), [1 if i in top_k_indices else 0.1 for i in range(NUM_EXPERTS)], \n",
    "           color=colors, width=1.0, edgecolor='none')\n",
    "    ax.set_title(f'\"{token}\"', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Expert ID', fontsize=9)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    ax.set_xlim(0, NUM_EXPERTS)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Selected', fontsize=9)\n",
    "    ax.set_xticks([0, 64, 128, 192, 255])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Key Observations:\")\n",
    "print(\"  ‚Ä¢ Different tokens route to different experts\")\n",
    "print(\"  ‚Ä¢ Only 3.1% of experts active per token (8/256)\")\n",
    "print(\"  ‚Ä¢ This is why MoE scales capacity without proportional compute!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Expert Parallelism Deployment\n",
    "\n",
    "Now you understand how MoE models work internally. But here's the deployment challenge:\n",
    "\n",
    "**Problem**: DeepSeek-V3 has 256 experts. How do we distribute them across GPUs?\n",
    "\n",
    "**Solutions** (covered in Section 4):\n",
    "- **Standard EP**: Distribute experts across 8-16 GPUs\n",
    "- **Wide EP**: Distribute across 32-64+ GPUs for maximum throughput\n",
    "- **EPLB**: Dynamically balance load when some experts are more popular\n",
    "\n",
    "The router mechanism you just learned operates **within** whatever EP deployment strategy you choose!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expert Parallelism (EP): Core Idea\n",
    "\n",
    "In Expert Parallelism, each GPU or node hosts a subset of experts. Tokens are routed to the selected experts via efficient all-to-all communication.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[GPU0: Experts 0,1]  <-->  [GPU1: Experts 2,3]  <-->  [GPU2: Experts 4,5]\n",
    "              ‚Üò  Tokens routed dynamically  ‚Üô\n",
    "```\n",
    "\n",
    "**Performance depends on:**\n",
    "- Expert placement (static vs dynamic)\n",
    "- Load balancing (even token distribution)\n",
    "- Communication optimization (all-to-all)\n",
    "\n",
    "**Key consideration:** How many experts should each GPU store? We'll explore this question in detail in Section 8.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expert Parallelism in Dynamo\n",
    "\n",
    "Dynamo provides pluggable backends with different EP capabilities.\n",
    "\n",
    "| EP Type | Description | Example Backend / Model |\n",
    "|---------|-------------|-------------------------|\n",
    "| Standard EP | Static expert distribution | Mixtral via SGLang |\n",
    "| Wide EP | Disaggregated experts across clusters | DeepSeek-R1 WideEP |\n",
    "| Deep EP | Hierarchical/nested experts | DeepSeek-V2/V3 |\n",
    "| Dynamic EP (EPLB) | Adaptive routing/load balancing | EPLB in SGLang |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Dives: EP Variants\n",
    "\n",
    "This section goes deeper into each Expert Parallelism variant. Read these after the basics.\n",
    "\n",
    "- **Standard EP**: static placement of experts across GPUs\n",
    "- **Wide EP**: distributed experts across nodes/clusters; used with horizontal replicas\n",
    "- **Deep EP**: hierarchical/nested experts for specialization\n",
    "- **Dynamic EP (EPLB)**: runtime (or static) load balancing of expert placements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Deep Dive: Standard EP\n",
    "\n",
    "- Experts are statically sharded across GPUs\n",
    "- Router selects top-k experts per token, then all-to-all routes tokens\n",
    "- Good starting point for small-to-medium MoE deployments\n",
    "- Combine with TP/PP for dense layers if needed\n",
    "\n",
    "**Tip:** Monitor expert usage distribution; if imbalance appears, consider enabling EPLB.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deep Dive: Wide EP\n",
    "\n",
    "- Distributes experts across many GPUs/nodes; often used with multi-replica (horizontal) serving\n",
    "- Targets high throughput at scale; pairs well with disaggregated prefill/decode\n",
    "- Benefits from high-bandwidth interconnect (e.g., NVL72) to hide all-to-all costs\n",
    "- Works best with batching and GroupGEMM-friendly routing\n",
    "\n",
    "#### WideEP vs DeepEP Comparison\n",
    "\n",
    "| Dimension | Wide EP | Deep EP |\n",
    "|-----------|---------|---------|\n",
    "| Goal | Maximize throughput & utilization | Enhance specialization |\n",
    "| Structure | Experts spread across GPUs/nodes | Experts grouped hierarchically |\n",
    "| Routing | Token ‚Üí top-k experts globally | Token ‚Üí coarse ‚Üí fine experts |\n",
    "| Use Case | Distributed inference at scale | Hierarchical reasoning |\n",
    "\n",
    "**Visuals:**\n",
    "```\n",
    "WideEP:  GPU clusters -> flat expert pool\n",
    "         [E0,E1,E2,...E64]\n",
    "\n",
    "DeepEP:  Hierarchical expert tree\n",
    "         Root\n",
    "          ‚îú‚îÄ‚îÄ Language Experts\n",
    "          ‚îÇ    ‚îú‚îÄ‚îÄ Code Expert\n",
    "          ‚îÇ    ‚îú‚îÄ‚îÄ Math Expert\n",
    "          ‚îú‚îÄ‚îÄ Reasoning Experts\n",
    "               ‚îú‚îÄ‚îÄ Symbolic\n",
    "               ‚îú‚îÄ‚îÄ Commonsense\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Deep Dive: Deep EP\n",
    "\n",
    "- Organizes experts hierarchically (coarse ‚Üí fine routing)\n",
    "- Improves specialization; useful for mixed domains and complex reasoning tasks\n",
    "- May reduce routing search space at each level; pairs with expert grouping/placement policies\n",
    "\n",
    "**Diagram (conceptual):**\n",
    "```\n",
    "Root\n",
    " ‚îú‚îÄ‚îÄ Language Experts\n",
    " ‚îÇ    ‚îú‚îÄ‚îÄ Code Expert\n",
    " ‚îÇ    ‚îú‚îÄ‚îÄ Math Expert\n",
    " ‚îú‚îÄ‚îÄ Reasoning Experts\n",
    "      ‚îú‚îÄ‚îÄ Symbolic\n",
    "      ‚îú‚îÄ‚îÄ Commonsense\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Deep Dive: Dynamic EP (EPLB)\n",
    "\n",
    "**Expert Parallelism Load Balancer (EPLB)** dynamically optimizes expert placement across GPUs to balance workload.\n",
    "\n",
    "#### Critical: EPLB Changes WHERE Experts Live, NOT Which Expert Processes a Token\n",
    "\n",
    "**Important distinction to avoid confusion:**\n",
    "\n",
    "**What the Router Does (Never Changes):**\n",
    "```\n",
    "Token \"quantum\" ‚Üí Router ‚Üí Selects Expert 0, Expert 45, Expert 123\n",
    "```\n",
    "- The router's decision is **fixed by the model's learned weights**\n",
    "- If the router says \"Expert 0 should process this token,\" then **Expert 0 MUST process it**\n",
    "- This is critical for correctness and **never changes during inference**\n",
    "\n",
    "**What EPLB Does (Physical Placement Only):**\n",
    "\n",
    "EPLB only changes **WHERE** Expert 0 physically lives, not which tokens it processes.\n",
    "\n",
    "**Without EPLB (naive placement):**\n",
    "```\n",
    "GPU 0: Expert 0, Expert 1  ‚Üê Expert 0 gets 1000 tokens, Expert 1 gets 800 tokens\n",
    "GPU 1: Expert 2, Expert 3  ‚Üê Expert 2 gets 600 tokens, Expert 3 gets 500 tokens\n",
    "GPU 2: Expert 4, Expert 5  ‚Üê Expert 4 gets 300 tokens, Expert 5 gets 200 tokens\n",
    "GPU 3: Expert 6, Expert 7  ‚Üê Expert 6 gets 100 tokens, Expert 7 gets 50 tokens\n",
    "\n",
    "Problem: GPU 0 is overloaded (1800 tokens), GPU 3 is mostly idle (150 tokens)\n",
    "Throughput: Limited by slowest GPU!\n",
    "```\n",
    "\n",
    "**With EPLB (smart rebalancing):**\n",
    "```\n",
    "GPU 0: Expert 0           ‚Üê Still processes the same 1000 tokens\n",
    "GPU 1: Expert 1, Expert 4 ‚Üê 800 + 300 = 1100 tokens\n",
    "GPU 2: Expert 2, Expert 5, Expert 6 ‚Üê 600 + 200 + 100 = 900 tokens\n",
    "GPU 3: Expert 3, Expert 7 ‚Üê 500 + 50 = 550 tokens\n",
    "\n",
    "Result: All GPUs balanced (~900 tokens average)\n",
    "Throughput: 1.8x improvement!\n",
    "```\n",
    "\n",
    "**Key Point:** The token still goes to Expert 0! It just travels to a different GPU now.\n",
    "\n",
    "#### EPLB Modes\n",
    "\n",
    "- **Static EPLB**: precomputed expert‚ÜíGPU mappings from historical data\n",
    "- **Online EPLB**: runtime redistribution to match live workload patterns\n",
    "- Can replicate hot experts and relocate cold ones\n",
    "- Performed between forward passes without breaking CUDA graphs\n",
    "- **Goal**: minimize GPU utilization variance and improve tokens/sec\n",
    "\n",
    "#### Expert Replication\n",
    "\n",
    "EPLB can also **replicate** popular experts:\n",
    "```\n",
    "GPU 0: Expert 0 (replica A)\n",
    "GPU 1: Expert 0 (replica B), Expert 1\n",
    "```\n",
    "- Both replicas have **identical weights**\n",
    "- A token needing Expert 0 can go to **either replica** (whichever GPU is less busy)\n",
    "- Output is **exactly the same** regardless of which replica processes it\n",
    "\n",
    "### Figure 4: EPLB Load Balancing\n",
    "\n",
    "![Figure 4: EPLB](images/figure4_eplb_load_balancing.gif)\n",
    "\n",
    "*Diagram showing Expert Parallel Load Balancer (EPLB) redistributes experts to ensure balanced GPU workload, preventing over- and under-utilization.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPLB Load Balancing Demonstration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scenario: 8 experts, 4 GPUs\n",
    "# Each expert has different popularity (tokens to process)\n",
    "expert_popularity = np.array([1000, 800, 600, 500, 300, 200, 100, 50])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EPLB Load Balancing Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìä Expert Popularity (tokens each expert needs to process):\")\n",
    "for i, tokens in enumerate(expert_popularity):\n",
    "    bar = '‚ñà' * (tokens // 100)\n",
    "    print(f\"  Expert {i}: {bar} {tokens} tokens\")\n",
    "\n",
    "# Without EPLB: naive assignment (2 experts per GPU)\n",
    "print(\"\\n\\n‚ùå WITHOUT EPLB (Naive Assignment: 2 experts per GPU)\")\n",
    "print(\"-\" * 70)\n",
    "naive_assignment = {\n",
    "    0: [0, 1],  # GPU 0 gets Expert 0, 1\n",
    "    1: [2, 3],  # GPU 1 gets Expert 2, 3\n",
    "    2: [4, 5],  # GPU 2 gets Expert 4, 5\n",
    "    3: [6, 7]   # GPU 3 gets Expert 6, 7\n",
    "}\n",
    "\n",
    "naive_loads = []\n",
    "for gpu_id, expert_ids in naive_assignment.items():\n",
    "    load = sum(expert_popularity[e] for e in expert_ids)\n",
    "    naive_loads.append(load)\n",
    "    expert_str = ', '.join([f\"E{e}\" for e in expert_ids])\n",
    "    print(f\"GPU {gpu_id}: [{expert_str}] ‚Üí {load} tokens\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Problem:\")\n",
    "print(f\"    ‚Ä¢ GPU 0 is overloaded: {max(naive_loads)} tokens\")\n",
    "print(f\"    ‚Ä¢ GPU 3 is mostly idle: {min(naive_loads)} tokens\")\n",
    "print(f\"    ‚Ä¢ Throughput limited by slowest GPU!\")\n",
    "\n",
    "# With EPLB: balanced assignment\n",
    "print(\"\\n\\n‚úÖ WITH EPLB (Smart Rebalancing)\")\n",
    "print(\"-\" * 70)\n",
    "eplb_assignment = {\n",
    "    0: [0],        # Most popular expert gets its own GPU\n",
    "    1: [1, 4],     # Balance: 800 + 300 = 1100\n",
    "    2: [2, 5, 6],  # Balance: 600 + 200 + 100 = 900\n",
    "    3: [3, 7]      # Balance: 500 + 50 = 550\n",
    "}\n",
    "\n",
    "eplb_loads = []\n",
    "for gpu_id, expert_ids in eplb_assignment.items():\n",
    "    load = sum(expert_popularity[e] for e in expert_ids)\n",
    "    eplb_loads.append(load)\n",
    "    expert_str = ', '.join([f\"E{e}\" for e in expert_ids])\n",
    "    print(f\"GPU {gpu_id}: [{expert_str}] ‚Üí {load} tokens\")\n",
    "\n",
    "print(f\"\\n‚úÖ Result:\")\n",
    "print(f\"    ‚Ä¢ All GPUs balanced (~{np.mean(eplb_loads):.0f} tokens average)\")\n",
    "print(f\"    ‚Ä¢ Max load reduced from {max(naive_loads)} to {max(eplb_loads)}\")\n",
    "print(f\"    ‚Ä¢ Throughput improvement: ~{max(naive_loads)/max(eplb_loads):.1f}x faster!\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without EPLB\n",
    "colors_naive = ['#ff6b6b' if load > 1500 else '#ffa500' if load > 1000 else '#90EE90' for load in naive_loads]\n",
    "bars1 = ax1.bar(range(4), naive_loads, color=colors_naive, edgecolor='black', linewidth=1.5)\n",
    "ax1.axhline(y=np.mean(naive_loads), color='blue', linestyle='--', linewidth=2, label=f'Average: {np.mean(naive_loads):.0f}')\n",
    "ax1.set_title('Without EPLB: Imbalanced', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "ax1.set_xlabel('GPU', fontsize=12)\n",
    "ax1.set_ylabel('Tokens to Process', fontsize=12)\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_xticklabels([f'GPU {i}' for i in range(4)])\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 2000)\n",
    "\n",
    "# Add load labels on bars\n",
    "for i, (bar, load) in enumerate(zip(bars1, naive_loads)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, load + 50, f'{load}', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# With EPLB\n",
    "bars2 = ax2.bar(range(4), eplb_loads, color='#66cc99', edgecolor='black', linewidth=1.5)\n",
    "ax2.axhline(y=np.mean(eplb_loads), color='blue', linestyle='--', linewidth=2, label=f'Average: {np.mean(eplb_loads):.0f}')\n",
    "ax2.set_title('With EPLB: Balanced', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "ax2.set_xlabel('GPU', fontsize=12)\n",
    "ax2.set_ylabel('Tokens to Process', fontsize=12)\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_xticklabels([f'GPU {i}' for i in range(4)])\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 2000)\n",
    "\n",
    "# Add load labels on bars\n",
    "for i, (bar, load) in enumerate(zip(bars2, eplb_loads)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, load + 50, f'{load}', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ Key Takeaway:\")\n",
    "print(\"   EPLB monitors expert popularity and redistributes them across GPUs\")\n",
    "print(\"   to prevent bottlenecks and maximize throughput!\")\n",
    "print(\"   Remember: Experts still process the SAME tokens, just on different GPUs!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Transition: From Theory to Practice\n",
    "\n",
    "Great work! You now understand:\n",
    "- ‚úÖ Why parallelism matters for LLMs\n",
    "- ‚úÖ Different parallelism strategies (DP, TP, PP, SP, EP)\n",
    "- ‚úÖ How MoE models work with expert routing\n",
    "- ‚úÖ Expert Parallelism variants (Standard, Wide, Deep, Dynamic/EPLB)\n",
    "\n",
    "**Now let's reinforce these concepts with hands-on exercises!** The following interactive examples will help you visualize how EP and EPLB work in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Hands-on Exercises\n",
    "\n",
    "### Exercise 1 ‚Äî WideEP Routing Simulation\n",
    "\n",
    "Simulate how tokens get assigned to top-k experts.\n",
    "\n",
    "Run the cell below and observe randomized token‚Üíexperts assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: token ‚Üí experts (top-k) routing simulation\n",
    "import random\n",
    "\n",
    "experts = [f\"E{i}\" for i in range(8)]\n",
    "tokens = [f\"T{i}\" for i in range(16)]\n",
    "\n",
    "def route(tokens, experts, topk=2):\n",
    "    routing = {}\n",
    "    for t in tokens:\n",
    "        routing[t] = random.sample(experts, topk)\n",
    "    return routing\n",
    "\n",
    "routing = route(tokens, experts)\n",
    "for t, e in routing.items():\n",
    "    print(f\"{t} ‚Üí {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 ‚Äî Throughput Comparison Chart\n",
    "\n",
    "Visualize relative throughput improvements from different strategies.\n",
    "\n",
    "**‚ö†Ô∏è Note**: The values below are **illustrative examples only** to demonstrate the concept. Actual performance will vary significantly based on your hardware, model size, workload characteristics, and configuration. Always run your own benchmarks to measure real performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: simple bar chart of relative throughput (ILLUSTRATIVE VALUES)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [\"Dense\", \"Standard EP\", \"WideEP\", \"WideEP + EPLB\"]\n",
    "y = [1.0, 1.8, 2.5, 2.9]  # Illustrative values - NOT actual benchmarks!\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x, y, color=[\"#8888ff\", \"#66cc99\", \"#ffcc66\", \"#ff8888\"])\n",
    "plt.title(\"Relative Throughput by Strategy (Illustrative)\")\n",
    "plt.ylabel(\"Speedup (√ó Dense)\")\n",
    "plt.ylim(0, 3.2)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.text(0.5, 3.0, 'Values are illustrative only!', \n",
    "         ha='center', fontsize=10, style='italic', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced: Large-Scale Wide-EP on GB200 NVL72\n",
    "\n",
    "In this section, we explore how **Wide Expert Parallelism (Wide-EP)** on **GB200 NVL72** rack-scale systems enables efficient inference for massive MoE models like DeepSeek-R1.\n",
    "\n",
    "### 8.1 The Scaling Challenge\n",
    "\n",
    "As MoE models like DeepSeek-R1 grow to 671B parameters with 256 experts, deploying them efficiently becomes critical. We've seen that Expert Parallelism (EP) distributes experts across GPUs, but **how many GPUs should we use?**\n",
    "\n",
    "Consider deploying DeepSeek-R1 (256 experts):\n",
    "- **With 8 GPUs:** Each GPU stores 256 √∑ 8 = **32 experts per GPU**\n",
    "- **With 16 GPUs:** Each GPU stores 256 √∑ 16 = **16 experts per GPU**\n",
    "- **With 64 GPUs:** Each GPU stores 256 √∑ 64 = **4 experts per GPU**\n",
    "\n",
    "**The Question:**\n",
    "Does it matter how many GPUs we use? Is there a \"sweet spot\" for the number of experts per GPU?\n",
    "\n",
    "To answer this, we need to understand what happens during MoE inference at high throughput...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 The Problem: Weight-Loading Bottleneck\n",
    "\n",
    "MoE models dynamically load expert weights on a per-token, per-layer basis. In high-throughput inference:\n",
    "\n",
    "**The Challenge:**\n",
    "1. Each GPU stores many experts (e.g., 32 experts/GPU in small EP)\n",
    "2. Every token activates 8 experts, potentially from different experts on the same GPU\n",
    "3. Weights must be loaded into on-chip memory/registers before computation\n",
    "4. **GroupGEMM kernels** batch tokens per expert for efficiency, but are bottlenecked by weight-loading overhead\n",
    "\n",
    "**GroupGEMM**: A fused kernel that packs all tokens routed to the same expert into a single matrix multiplication. This is efficient _if_ we can keep weights in fast memory and reuse them across many tokens.\n",
    "\n",
    "### Figure 2: GroupGEMM Token Routing\n",
    "\n",
    "![Figure 2: GroupGEM](images/figure2_groupgemm_token_routing.webp)\n",
    "\n",
    "*Tokens routed to the same expert are packed together and processed with a single fused GroupGEMM kernel for efficient MoE inference.*\n",
    "\n",
    "**The Bottleneck:**\n",
    "- More experts per GPU ‚Üí more weight-loading pressure\n",
    "- Less weight reuse ‚Üí lower arithmetic intensity (FLOPs/byte)\n",
    "- GroupGEMM efficiency degrades as experts/GPU increases\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î Think About It: How Can We Solve This?\n",
    "\n",
    "Now that we've identified the weight-loading bottleneck, think about this:\n",
    "\n",
    "**Question:** How can we reduce the number of experts stored on each GPU? What's the key insight?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**The Key Insight:**\n",
    "If we **distribute the same 256 experts across MORE GPUs**, each GPU will store fewer experts!\n",
    "\n",
    "**Example:**\n",
    "- **Small EP (8 GPUs):** 256 experts √∑ 8 = 32 experts/GPU ‚ùå (too many!)\n",
    "- **Wide EP (64 GPUs):** 256 experts √∑ 64 = 4 experts/GPU ‚úÖ (much better!)\n",
    "\n",
    "**The Solution:**\n",
    "Instead of using 8 GPUs, use 32-64 GPUs (or more) to spread experts thinner. This reduces weight-loading pressure per GPU while maintaining the same total model capacity.\n",
    "\n",
    "**Trade-off:** We need more GPUs, but each GPU becomes more efficient!\n",
    "\n",
    "**Next step:** Let's see how this works in practice! üöÄ\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 The Solution: Wide-EP ‚Äî Distribute Experts Across More GPUs\n",
    "\n",
    "The solution is elegant: **spread experts across many more GPUs**, reducing the experts-per-GPU ratio.\n",
    "\n",
    "**This is Wide Expert Parallelism (Wide-EP):**\n",
    "- Instead of 8 GPUs, use 32‚Äì64 GPUs (or more)\n",
    "- Each GPU stores fewer experts (e.g., 4 instead of 32)\n",
    "- **Result**: Less weight-loading pressure per GPU, better GroupGEMM efficiency\n",
    "\n",
    "### Figure 1: Small-Scale vs Large-Scale EP\n",
    "\n",
    "![Figure 1: Small vs Large Scale EP](images/figure1_small_vs_large_scale_ep.gif)\n",
    "\n",
    "*Animation showing how small-scale EP deploys many experts per GPU, while large-scale EP (Wide-EP) spreads fewer experts per GPU across a much larger cluster, enabling efficient scaling of MoE layers.*\n",
    "\n",
    "Let's visualize how distributing DeepSeek-R1's 256 experts across different EP configurations reduces the burden per GPU:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Think About It: How Does This Solve the Problem?\n",
    "\n",
    "Now that we've seen the solution (distributing experts across more GPUs), think about this:\n",
    "\n",
    "**Question:** How does Wide-EP (fewer experts per GPU) actually solve the weight-loading bottleneck we identified?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**The Solution:**\n",
    "By distributing experts across more GPUs:\n",
    "- **Each GPU stores fewer experts** (e.g., 4 instead of 32)\n",
    "- **Less weight-loading overhead** per GPU (fewer experts to load from memory)\n",
    "- **Better weight reuse** within GroupGEMM kernels (same expert weights reused for more tokens)\n",
    "- **Higher arithmetic intensity** (more FLOPs per byte loaded)\n",
    "\n",
    "**Trade-off:** We need more GPUs, but the per-GPU efficiency improves significantly.\n",
    "\n",
    "**Key insight:** Wide-EP trades GPU count for per-GPU efficiency. But there's a catch‚Äîwe need high-bandwidth communication to make this practical!\n",
    "\n",
    "**Next step:** What hardware makes this practical? ü§î\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide-EP: Reducing Experts per GPU\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simple visualization: How Wide-EP distributes 256 experts\n",
    "TOTAL_EXPERTS = 256\n",
    "ep_configs = [\n",
    "    (\"Small EP\\n(8 GPUs)\", 8),\n",
    "    (\"Medium EP\\n(16 GPUs)\", 16),\n",
    "    (\"Wide EP\\n(32 GPUs)\", 32),\n",
    "    (\"Very Wide EP\\n(64 GPUs)\", 64)\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Wide-EP: Distributing 256 Experts Across GPUs\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDeepSeek-V3 has 256 experts. How many experts per GPU?\\n\")\n",
    "\n",
    "labels = []\n",
    "experts_per_gpu = []\n",
    "colors = []\n",
    "\n",
    "for label, num_gpus in ep_configs:\n",
    "    epg = TOTAL_EXPERTS // num_gpus\n",
    "    experts_per_gpu.append(epg)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Color code: red for many experts/GPU, green for few\n",
    "    if epg >= 20:\n",
    "        colors.append('#ff6b6b')  # Red - too many!\n",
    "    elif epg >= 10:\n",
    "        colors.append('#ffa500')  # Orange - moderate\n",
    "    else:\n",
    "        colors.append('#66cc99')  # Green - good!\n",
    "    \n",
    "    print(f\"{label:20s}: {epg:2d} experts/GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Key Insight:\")\n",
    "print(\"  ‚Ä¢ Fewer experts per GPU = Less weight-loading pressure\")\n",
    "print(\"  ‚Ä¢ Wide-EP (32-64 GPUs) enables efficient GroupGEMM operations\")\n",
    "print(\"  ‚Ä¢ Trade-off: More GPUs needed, but higher throughput per GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(range(len(labels)), experts_per_gpu, color=colors, \n",
    "              edgecolor='black', linewidth=1.5, width=0.6)\n",
    "\n",
    "ax.set_title('Wide-EP: Experts per GPU (256 total experts)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('EP Configuration', fontsize=12)\n",
    "ax.set_ylabel('Experts per GPU', fontsize=12)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, experts_per_gpu):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "            f'{value}', ha='center', va='bottom', \n",
    "            fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('More efficient!\\nLess weight-loading', \n",
    "            xy=(3, experts_per_gpu[3]), \n",
    "            xytext=(2.5, 15),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=11, color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 System Architecture: GB200 NVL72 Rack-Scale System\n",
    "\n",
    "Now we understand _why_ Wide-EP helps (fewer experts per GPU), but how do we make it practical? The answer is **GB200 NVL72**.\n",
    "\n",
    "**GB200 NVL72 Key Features:**\n",
    "- **72 GPUs** in a single coherent NVLink domain\n",
    "- **130 TB/s aggregate bandwidth** for GPU-to-GPU communication\n",
    "- Enables efficient all-to-all token routing during decode phase\n",
    "- Custom NCCL kernels handle non-static communication sizes with CUDA graph compatibility\n",
    "\n",
    "**Why EP=64 (not 72)?**\n",
    "- Clean division: 256 experts √∑ 64 = 4 experts/GPU (integer)\n",
    "- Power-of-two efficiency for collectives (64-way vs 72-way)\n",
    "- Reserve 8 GPUs for prefill/MTP/headroom in disaggregated serving\n",
    "\n",
    "### Figure 3: MoE Deployment on NVL72\n",
    "\n",
    "![Figure 3: MoE Deployment NVL72](images/figure3_moe_deployment_nvl72.gif)\n",
    "\n",
    "*Schematic diagram showing an MoE deployment with 232 experts per GPU (4 experts √ó 58 MoE layers) and only 8 experts activated per layer, coordinated across 72 GPUs in a GB200 NVL72 NVLink domain.*\n",
    "\n",
    "**EPLB (Expert Parallel Load Balancer):**\n",
    "- **Static mode**: Pre-computed expert‚ÜíGPU mappings from historical patterns\n",
    "- **Online mode**: Runtime redistribution with non-blocking weight updates between forward passes\n",
    "- Prevents \"hot experts\" from concentrating on the same GPU\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î Think About It: How Does NVL72 Make This Practical?\n",
    "\n",
    "Now that we understand the NVL72 architecture, think about this:\n",
    "\n",
    "**Question:** Why is NVL72's 130 TB/s NVLink bandwidth critical for making Wide-EP practical? What would happen without it?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Why NVL72 Matters:**\n",
    "- **Communication overhead:** With Wide-EP, tokens must be routed across many GPUs via all-to-all collectives\n",
    "- **130 TB/s bandwidth** hides this communication cost, making the all-to-all operations efficient\n",
    "- **Without high bandwidth:** Communication would become the bottleneck, negating the benefits of Wide-EP\n",
    "- **Coherent NVLink domain:** Enables efficient token routing without going through slow network interfaces\n",
    "\n",
    "**The Key Insight:**\n",
    "Wide-EP solves the weight-loading problem, but creates a communication problem. NVL72's massive bandwidth solves the communication problem, making Wide-EP practical!\n",
    "\n",
    "**Without NVL72:** You'd need to use fewer GPUs (smaller EP), which brings back the weight-loading bottleneck.\n",
    "\n",
    "**Next step:** Does this actually work in practice? Let's see the results! üìä\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Performance Results: Up to 1.8√ó Throughput Gain\n",
    "\n",
    "So does Wide-EP actually work? NVIDIA's benchmarks show significant performance improvements:\n",
    "\n",
    "### Figure 5: EP Throughput Comparison\n",
    "\n",
    "![Figure 5: Throughput](images/figure5_ep_throughput_comparison.webp)\n",
    "\n",
    "*Large-scale Expert Parallelism (EP=32) delivers up to **1.8√ó higher output token throughput per GPU** compared to small EP (EP=8) at 100 tokens/sec per user. Both configurations leverage disaggregated serving and multi-token prediction (MTP).*\n",
    "\n",
    "**Key Findings:**\n",
    "- **Super-linear scaling**: 4√ó more GPUs ‚Üí 7.2√ó total throughput (not just 4√ó)\n",
    "- **Per-GPU efficiency improves** with larger EP configurations\n",
    "- Enabled by NVL72's 130 TB/s NVLink hiding communication overhead\n",
    "- Works best with disaggregated serving (separate prefill/decode pools)\n",
    "\n",
    "**Why super-linear?**\n",
    "1. Fewer experts per GPU ‚Üí better GroupGEMM efficiency\n",
    "2. Higher weight reuse ‚Üí better arithmetic intensity\n",
    "3. NVL72 bandwidth offsets communication costs\n",
    "4. EPLB prevents load imbalance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 When to Use Wide-EP: Practical Guidance\n",
    "\n",
    "Wide-EP is powerful but not always the right choice. Here's when it makes sense:\n",
    "\n",
    "**‚úÖ Good candidates for Wide-EP:**\n",
    "- **Large MoE models** with many experts (e.g., DeepSeek-R1 with 256 experts)\n",
    "- **Latency-constrained throughput** scenarios (need high tokens/sec at fixed latency)\n",
    "- **High-bandwidth interconnect** available (NVL72, multi-node NVLink clusters)\n",
    "- **Variable workloads** where EPLB can balance load dynamically\n",
    "\n",
    "**‚ùå When to avoid Wide-EP:**\n",
    "- Small models with few experts (communication overhead dominates)\n",
    "- Low-latency, low-throughput scenarios (overhead not justified)\n",
    "- Limited interconnect bandwidth (communication becomes bottleneck)\n",
    "- Single-node deployments with < 8 GPUs\n",
    "\n",
    "**Integration with NVIDIA Dynamo:**\n",
    "\n",
    "Wide-EP works best when orchestrated by **NVIDIA Dynamo** for disaggregated serving:\n",
    "\n",
    "| Component | Role |\n",
    "|-----------|------|\n",
    "| **NVIDIA Dynamo** | Orchestration: disaggregates prefill/decode, SLA-aware autoscaling, dynamic routing |\n",
    "| **TensorRT-LLM Wide-EP** | Execution: expert-parallel MoE with optimized kernels, FP8, CUDA graphs, EPLB |\n",
    "\n",
    "**Together they enable:**\n",
    "- Prefill pool scales independently from decode pool\n",
    "- Decode pool uses Wide-EP (e.g., EP=64) for throughput\n",
    "- Dynamo Planner adapts to ISL/OSL fluctuations in real-time\n",
    "- EPLB balances expert load within the decode pool\n",
    "\n",
    "**Reference:** [NVIDIA Blog: Scaling Large MoE Models with Wide Expert Parallelism on NVL72](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Knowledge Check: Pop Quiz\n",
    "\n",
    "Test your understanding! Try to answer each question before revealing the answer.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1: Active Parameters\n",
    "\n",
    "**Q:** DeepSeek-R1 has 671B total parameters but only ~37B active per token. Why is this efficient?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer:**\n",
    "- Only 8 out of 256 experts are activated per token (sparse activation)\n",
    "- Reduces compute from 671B to ~37B FLOPs per token\n",
    "- Enables \"dense model quality at sparse model cost\"\n",
    "- Active parameters = dense layers + (8 active experts √ó expert size)\n",
    "\n",
    "**Key insight:** Sparse activation gives you the benefits of a huge model without paying the full computational cost!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: EP vs TP\n",
    "\n",
    "**Q:** What's the key difference between Expert Parallelism (EP) and Tensor Parallelism (TP)?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**TP (Tensor Parallelism):**\n",
    "- Splits individual tensor computations across GPUs\n",
    "- Applies to ALL layers (dense and MoE)\n",
    "- Each GPU computes a slice of the same operation\n",
    "- Example: 16K-dim matrix ‚Üí 8 GPUs √ó 2K slices\n",
    "\n",
    "**EP (Expert Parallelism):**\n",
    "- Distributes entire experts across GPUs\n",
    "- Applies ONLY to MoE layers\n",
    "- Each GPU holds complete expert weights for a subset of experts\n",
    "- Example: 256 experts ‚Üí 64 GPUs √ó 4 experts/GPU\n",
    "\n",
    "**Key difference:** TP splits operations, EP distributes whole experts!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: NVL72 Mystery\n",
    "\n",
    "**Q:** NVL72 has 72 GPUs. Why do we use EP=64 instead of EP=72 for DeepSeek-R1?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Multiple reasons:**\n",
    "1. **Clean division**: 256 experts √∑ 64 = 4 experts/GPU (integer division)\n",
    "2. **Power-of-two efficiency**: 64-way collectives optimize better than 72-way\n",
    "3. **Topology alignment**: Maps cleanly to 8√ó8 groupings; reduces cross-island traffic\n",
    "4. **Disaggregated serving**: Reserve 8 GPUs for prefill/MTP/headroom while 64 handle decode\n",
    "5. **Load balancing**: Equal experts per GPU simplifies EPLB algorithms\n",
    "\n",
    "**Key insight:** EP size is a design choice for performance, not dictated by raw GPU count!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4: EPLB Mechanics\n",
    "\n",
    "**Q:** Does EPLB (Expert Parallel Load Balancer) change which expert processes a token?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer: NO!** \n",
    "\n",
    "EPLB changes **WHERE** experts live, NOT which expert processes a token.\n",
    "\n",
    "**What stays the same:**\n",
    "- Router decision is fixed by learned model weights\n",
    "- Token \"quantum\" ‚Üí Router ‚Üí Expert 0, Expert 45, Expert 123\n",
    "- These expert IDs never change\n",
    "\n",
    "**What EPLB changes:**\n",
    "- Physical GPU placement of experts\n",
    "- Example: Move Expert 0 from GPU 0 to GPU 5 to balance load\n",
    "- Token still goes to Expert 0, just travels to a different GPU\n",
    "\n",
    "**Analogy:** Like reassigning doctors to different hospital wings‚Äîpatients still see the same specialists, just in different locations!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: GroupGEMM Magic\n",
    "\n",
    "**Q:** Why does Wide-EP (fewer experts per GPU) improve GroupGEMM efficiency?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Weight-loading bottleneck:**\n",
    "- MoE dynamically loads expert weights per token per layer\n",
    "- More experts per GPU = more weight-loading overhead\n",
    "- GroupGEMM needs weights in registers/on-chip memory before multiplication\n",
    "\n",
    "**Wide-EP solution:**\n",
    "- Fewer experts per GPU = less weight-loading pressure\n",
    "- Higher weight reuse within the GroupGEMM kernel\n",
    "- Better arithmetic intensity (more FLOPs per byte loaded)\n",
    "- Example: 32 experts/GPU ‚Üí 4 experts/GPU (8√ó reduction in weight pressure)\n",
    "\n",
    "**Trade-off:**\n",
    "- Need more GPUs\n",
    "- BUT: 130 TB/s NVLink on NVL72 hides communication overhead\n",
    "- Result: Up to 1.8√ó higher per-GPU throughput\n",
    "\n",
    "**Key insight:** Distribute experts to reduce memory pressure and improve compute efficiency!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus Question: Throughput Math üìä\n",
    "\n",
    "**Q:** If EP=8 achieves 100 tokens/sec/GPU, and EP=32 achieves 1.8√ó improvement, what's the total cluster throughput gain when scaling from 8 to 32 GPUs?\n",
    "\n",
    "<details>\n",
    "<summary>üëâ Click to reveal answer</summary>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**EP=8 baseline:**\n",
    "- 8 GPUs √ó 100 tokens/sec/GPU = 800 tokens/sec total\n",
    "\n",
    "**EP=32 with 1.8√ó per-GPU improvement:**\n",
    "- 32 GPUs √ó (100 √ó 1.8) tokens/sec/GPU = 32 √ó 180 = **5,760 tokens/sec total**\n",
    "\n",
    "**Gain:**\n",
    "- 5,760 √∑ 800 = **7.2√ó total throughput increase**\n",
    "- This is **super-linear scaling**! (4√ó more GPUs ‚Üí 7.2√ó throughput)\n",
    "- Enabled by NVL72's high-bandwidth interconnect hiding communication costs\n",
    "\n",
    "**Key insight:** Wide-EP doesn't just scale linearly‚Äîit improves per-GPU efficiency, giving you super-linear gains! üöÄ\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Lab 3.1. You now understand:\n",
    "\n",
    "- **DP/TP/PP/SP**: apply to all models; solve memory/compute distribution differently.\n",
    "- **EP**: MoE-only; route tokens to a small set of experts per token.\n",
    "- **Dynamo**: supports Standard EP, WideEP, DeepEP, and dynamic EPLB via backends.\n",
    "- **WideEP + EPLB**: production-proven for scaling throughput and balancing load.\n",
    "- **NVL72 insights**: Large-scale EP with high-bandwidth interconnects delivers up to 1.8√ó per-GPU throughput gains.\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Ready to deploy these concepts in production? Continue to:\n",
    "\n",
    "**Lab 3.2: Wide EP Production Deployment** (`lab3.2-wide-ep-deployment.ipynb`)\n",
    "\n",
    "In Lab 3.2, you'll learn:\n",
    "- Kubernetes deployment with Dynamo Operator\n",
    "- Multi-node SGLang deployment with EPLB\n",
    "- TensorRT-LLM Wide EP configuration\n",
    "- Monitoring, troubleshooting, and performance tuning\n",
    "- Production best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Further Reading\n",
    "\n",
    "### MoE Architecture and Comparisons\n",
    "- [The Big LLM Architecture Comparison (Sebastian Raschka)](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) - Detailed comparison of DeepSeek-V3, Qwen3, and other MoE architectures\n",
    "\n",
    "### Expert Parallelism Deployment\n",
    "- [Scaling Large MoE Models with Wide Expert Parallelism on NVL72](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com) - NVIDIA's technical deep-dive on Wide-EP deployment strategies\n",
    "- [TensorRT-LLM Wide-EP Examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/wide_ep)\n",
    "\n",
    "### Dynamo and Backends\n",
    "- [NVIDIA Dynamo Documentation](https://github.com/NVIDIA/Dynamo)\n",
    "- [SGLang Documentation](https://github.com/sgl-project/sglang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
