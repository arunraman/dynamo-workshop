{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: Expert Parallelism Foundations\n",
    "\n",
    "Welcome! This notebook starts from first principles and builds up to Expert Parallelism (EP) for Mixture-of-Experts (MoE) models in Dynamo. You'll learn the main parallelism strategies, why EP exists, and how Dynamo supports WideEP, DeepEP, and dynamic load balancing (EPLB). You'll also run small, illustrative Python snippets to visualize concepts.\n",
    "\n",
    "**Duration**: 45-60 minutes\n",
    "\n",
    "**Next**: After completing this lab, continue to **Lab 3.2: Wide EP Production Deployment** to learn how to deploy these concepts in production with Kubernetes, SGLang, and TensorRT-LLM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Explain why parallelism is needed for LLM inference (prefill vs decode)\n",
    "- Differentiate DP, TP, PP, SP, and EP\n",
    "- Describe what MoE is and why EP applies only to MoE models\n",
    "- Understand Expert Parallelism in Dynamo (Standard, Wide, Deep, Dynamic/EPLB)\n",
    "- Identify when to use Wide-EP and how EPLB balances load\n",
    "- Interpret high-level NVL72 Wide-EP insights and their implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "**Foundations (Sections 1-5)**\n",
    "1. [Why Parallelism Matters for LLMs](#1.-Why-Parallelism-Matters-for-LLMs)\n",
    "2. [Parallelism Strategies at a Glance](#2.-Parallelism-Strategies-at-a-Glance)\n",
    "3. [From Dense to MoE: Why Experts?](#3.-From-Dense-to-MoE:-Why-Experts?)\n",
    "   - [3.1 How MoE Expert Selection Works](#3.1-How-MoE-Expert-Selection-Works-(Flat-Expert-Pool))\n",
    "4. [Expert Parallelism (EP): Core Idea](#4.-Expert-Parallelism-(EP):-Core-Idea)\n",
    "5. [Expert Parallelism in Dynamo](#5.-Expert-Parallelism-in-Dynamo)\n",
    "\n",
    "**Deep Dives (Section 6)**\n",
    "- [6. Deep Dives: EP Variants](#6.-Deep-Dives:-EP-Variants)\n",
    "  - 6.1 [Standard EP](#6.1-Deep-Dive:-Standard-EP)\n",
    "  - 6.2 [Wide EP](#6.2-Deep-Dive:-Wide-EP)\n",
    "  - 6.3 [Deep EP](#6.3-Deep-Dive:-Deep-EP)\n",
    "  - 6.4 [Dynamic EP (EPLB)](#6.4-Deep-Dive:-Dynamic-EP-(EPLB))\n",
    "\n",
    "**Hands-On (Section 7)**\n",
    "- [7. Hands-on Exercises](#7.-Hands-on-Exercises)\n",
    "  - [Exercise 1: WideEP Routing Simulation](#Exercise-1-—-WideEP-Routing-Simulation)\n",
    "  - [Exercise 2: Throughput Comparison Chart](#Exercise-2-—-Throughput-Comparison-Chart)\n",
    "\n",
    "**Advanced Topics (Section 8)**\n",
    "- [8. Advanced: Large-Scale Wide-EP on GB200 NVL72](#8.-Advanced:-Large-Scale-Wide-EP-on-GB200-NVL72)\n",
    "  - [GroupGEMM and Weight-Loading Intuition](#GroupGEMM-and-Weight-Loading-Intuition)\n",
    "  - [When to use Wide-EP (NVL72 perspective)](#When-to-use-Wide-EP-(NVL72-perspective))\n",
    "\n",
    "**Resources (Section 9)**\n",
    "- [9. Further Reading](#9.-Further-Reading)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "- **Beginners**: Start with Sections 1-5 for core concepts\n",
    "- **Intermediate**: Explore Section 6 (Deep Dives) for EP variants\n",
    "- **Advanced**: Read Section 8 for NVL72 insights and state-of-the-art optimizations\n",
    "- **Hands-on learners**: Run the exercises in Section 7 to build intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Parallelism Matters for LLMs\n",
    "\n",
    "Large Language Models are big and compute-intensive. Parallelism helps distribute memory and compute across multiple GPUs/nodes.\n",
    "\n",
    "### LLM Inference Phases\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│                  LLM Inference                      │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "                         │\n",
    "        ┌────────────────┴────────────────┐\n",
    "        │                                 │\n",
    "   ┌────▼────┐                       ┌────▼────┐\n",
    "   │ Phase 1 │                       │ Phase 2 │\n",
    "   │ PREFILL │                       │ DECODE  │\n",
    "   └─────────┘                       └─────────┘\n",
    "```\n",
    "- **Prefill**: processes prompt tokens in parallel (compute-bound)\n",
    "- **Decode**: generates one token at a time using KV cache (memory-bound)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallelism Strategies at a Glance\n",
    "\n",
    "| Type | Description | Key Benefit | Applies To |\n",
    "|------|-------------|-------------|------------|\n",
    "| Data Parallelism (DP) | Same model copy on each GPU, different data shards | Simple throughput scaling | Dense + MoE |\n",
    "| Tensor Parallelism (TP) | Split individual layers across GPUs | Fit larger models than one GPU | Dense + MoE |\n",
    "| Pipeline Parallelism (PP) | Split model layers into stages across GPUs | Memory-efficient for deep models | Dense + MoE |\n",
    "| Sequence Parallelism (SP) | Distribute sequence tokens across GPUs for attention | Efficient long sequences | Dense + MoE |\n",
    "| Expert Parallelism (EP) | Distribute MoE experts across GPUs, route tokens | Scale capacity without linear compute | MoE only |\n",
    "\n",
    "**Notes:**\n",
    "- DP/TP/PP/SP apply to all models.\n",
    "- EP applies only to MoE models (because only MoE has experts).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Dense to MoE: Why Experts?\n",
    "\n",
    "Dense models activate all parameters for every token. MoE models activate only a small subset of experts (e.g., top-1 or top-2) per token.\n",
    "\n",
    "**Example:**\n",
    "- Dense: 200B parameters → all active per token\n",
    "- MoE: 1T parameters → ~200B active per token (top-k experts)\n",
    "\n",
    "This gives huge capacity without proportional compute cost. The challenge becomes routing tokens to the right experts efficiently.\n",
    "\n",
    "### 3.1 How MoE Expert Selection Works (Flat Expert Pool)\n",
    "\n",
    "Most production MoE models (DeepSeek-V3, Qwen3, Mixtral) use a **flat expert pool** architecture. Let's understand how it works:\n",
    "\n",
    "#### The Router Network\n",
    "\n",
    "```\n",
    "Input Token Embedding\n",
    "         │\n",
    "         ▼\n",
    "    ┌─────────┐\n",
    "    │ Router  │ ← Small neural network (learned during training)\n",
    "    │ Network │    Outputs: score for EACH expert\n",
    "    └────┬────┘\n",
    "         │\n",
    "         ▼\n",
    "   [s₀, s₁, s₂, ..., s₂₅₅]  ← Scores for all 256 experts\n",
    "         │\n",
    "         ▼\n",
    "    Top-K Selection (e.g., k=8)\n",
    "         │\n",
    "         ▼\n",
    "   [E₁₂, E₄₅, E₇₈, E₁₂₃, E₁₅₆, E₁₈₉, E₂₀₁, E₂₃₄]\n",
    "         │\n",
    "         ▼\n",
    "   Weighted Sum → Output\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Router Network**: A small learned neural network that:\n",
    "   - Takes token embedding as input\n",
    "   - Outputs a score for **every expert** in the pool\n",
    "   - Trained end-to-end with the model\n",
    "\n",
    "2. **Top-K Selection**: \n",
    "   - Selects the k experts with highest scores\n",
    "   - Only these experts process the token\n",
    "   - Others are completely skipped (sparse activation)\n",
    "\n",
    "3. **Weighted Combination**:\n",
    "   - Each selected expert's output is weighted by its score\n",
    "   - Final output = weighted sum of expert outputs\n",
    "\n",
    "#### Example: DeepSeek-V3 Configuration\n",
    "\n",
    "- **Total experts**: 256 (flat pool)\n",
    "- **Active per token**: 8 (top-8 selection)\n",
    "- **Activation ratio**: 8/256 = 3.1% of experts per token\n",
    "- **Effective compute**: ~37B parameters active (out of 671B total)\n",
    "\n",
    "#### Why \"Flat\" Pool?\n",
    "\n",
    "All 256 experts are:\n",
    "- ✅ Scored simultaneously by the router\n",
    "- ✅ Treated equally (no hierarchy or grouping)\n",
    "- ✅ Can be selected for any token (maximum flexibility)\n",
    "\n",
    "**Contrast with hierarchical** (rare in practice):\n",
    "- ❌ Would have multiple routing stages\n",
    "- ❌ Experts grouped by domain (Language/Reasoning/etc.)\n",
    "- ❌ First choose group, then choose expert within group\n",
    "\n",
    "#### Hands-On: Simulating MoE Routing\n",
    "\n",
    "Let's simulate how a router selects experts for different tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate MoE expert routing with flat expert pool\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration (similar to DeepSeek-V3)\n",
    "NUM_EXPERTS = 256\n",
    "TOP_K = 8\n",
    "NUM_TOKENS = 5\n",
    "\n",
    "# Simulate router scores for different tokens\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MoE Expert Routing Simulation (Flat Expert Pool)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total experts: {NUM_EXPERTS}\")\n",
    "print(f\"Active experts per token: {TOP_K}\")\n",
    "print(f\"Activation ratio: {TOP_K/NUM_EXPERTS*100:.1f}%\\n\")\n",
    "\n",
    "# Simulate routing for different tokens\n",
    "tokens = [\"The\", \"quantum\", \"entanglement\", \"phenomenon\", \"is\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(tokens), figsize=(15, 3))\n",
    "fig.suptitle(\"Expert Selection for Different Tokens (Top-8 from 256 experts)\", fontsize=14)\n",
    "\n",
    "for idx, token in enumerate(tokens):\n",
    "    # Simulate router scores (in practice, these come from a learned neural network)\n",
    "    router_scores = np.random.randn(NUM_EXPERTS)\n",
    "    \n",
    "    # Top-K selection\n",
    "    top_k_indices = np.argsort(router_scores)[-TOP_K:][::-1]\n",
    "    top_k_scores = router_scores[top_k_indices]\n",
    "    \n",
    "    # Normalize scores to weights (softmax-like)\n",
    "    weights = np.exp(top_k_scores) / np.sum(np.exp(top_k_scores))\n",
    "    \n",
    "    print(f\"Token: '{token}'\")\n",
    "    print(f\"  Selected experts: {top_k_indices.tolist()}\")\n",
    "    print(f\"  Weights: {[f'{w:.3f}' for w in weights]}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize\n",
    "    ax = axes[idx]\n",
    "    colors = ['#ff6b6b' if i in top_k_indices else '#e0e0e0' for i in range(NUM_EXPERTS)]\n",
    "    ax.bar(range(NUM_EXPERTS), [1 if i in top_k_indices else 0.1 for i in range(NUM_EXPERTS)], \n",
    "           color=colors, width=1.0, edgecolor='none')\n",
    "    ax.set_title(f'\"{token}\"', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Expert ID', fontsize=9)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    ax.set_xlim(0, NUM_EXPERTS)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Selected', fontsize=9)\n",
    "    ax.set_xticks([0, 64, 128, 192, 255])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Key Observations:\")\n",
    "print(\"  • Different tokens route to different experts\")\n",
    "print(\"  • Only 3.1% of experts active per token (8/256)\")\n",
    "print(\"  • This is why MoE scales capacity without proportional compute!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Expert Parallelism Deployment\n",
    "\n",
    "Now you understand how MoE models work internally. But here's the deployment challenge:\n",
    "\n",
    "**Problem**: DeepSeek-V3 has 256 experts. How do we distribute them across GPUs?\n",
    "\n",
    "**Solutions** (covered in Section 4):\n",
    "- **Standard EP**: Distribute experts across 8-16 GPUs\n",
    "- **Wide EP**: Distribute across 32-64+ GPUs for maximum throughput\n",
    "- **EPLB**: Dynamically balance load when some experts are more popular\n",
    "\n",
    "The router mechanism you just learned operates **within** whatever EP deployment strategy you choose!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expert Parallelism (EP): Core Idea\n",
    "\n",
    "In Expert Parallelism, each GPU or node hosts a subset of experts. Tokens are routed to the selected experts via efficient all-to-all communication.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[GPU0: Experts 0,1]  <-->  [GPU1: Experts 2,3]  <-->  [GPU2: Experts 4,5]\n",
    "              ↘  Tokens routed dynamically  ↙\n",
    "```\n",
    "\n",
    "**Performance depends on:**\n",
    "- Expert placement (static vs dynamic)\n",
    "- Load balancing (even token distribution)\n",
    "- Communication optimization (all-to-all)\n",
    "\n",
    "### Figure 1: Small-Scale vs Large-Scale EP\n",
    "\n",
    "![Figure 1: Small vs Large Scale EP](images/figure1_small_vs_large_scale_ep.gif)\n",
    "\n",
    "*Animation showing how small-scale EP deploys many experts per GPU, while large-scale EP spreads fewer experts per GPU across a much larger cluster, enabling efficient scaling of MoE layers.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expert Parallelism in Dynamo\n",
    "\n",
    "Dynamo provides pluggable backends with different EP capabilities.\n",
    "\n",
    "| EP Type | Description | Example Backend / Model |\n",
    "|---------|-------------|-------------------------|\n",
    "| Standard EP | Static expert distribution | Mixtral via SGLang |\n",
    "| Wide EP | Disaggregated experts across clusters | DeepSeek-R1 WideEP |\n",
    "| Deep EP | Hierarchical/nested experts | DeepSeek-V2/V3 |\n",
    "| Dynamic EP (EPLB) | Adaptive routing/load balancing | EPLB in SGLang |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Dives: EP Variants\n",
    "\n",
    "This section goes deeper into each Expert Parallelism variant. Read these after the basics.\n",
    "\n",
    "- **Standard EP**: static placement of experts across GPUs\n",
    "- **Wide EP**: distributed experts across nodes/clusters; used with horizontal replicas\n",
    "- **Deep EP**: hierarchical/nested experts for specialization\n",
    "- **Dynamic EP (EPLB)**: runtime (or static) load balancing of expert placements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Deep Dive: Standard EP\n",
    "\n",
    "- Experts are statically sharded across GPUs\n",
    "- Router selects top-k experts per token, then all-to-all routes tokens\n",
    "- Good starting point for small-to-medium MoE deployments\n",
    "- Combine with TP/PP for dense layers if needed\n",
    "\n",
    "**Tip:** Monitor expert usage distribution; if imbalance appears, consider enabling EPLB.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deep Dive: Wide EP\n",
    "\n",
    "- Distributes experts across many GPUs/nodes; often used with multi-replica (horizontal) serving\n",
    "- Targets high throughput at scale; pairs well with disaggregated prefill/decode\n",
    "- Benefits from high-bandwidth interconnect (e.g., NVL72) to hide all-to-all costs\n",
    "- Works best with batching and GroupGEMM-friendly routing\n",
    "\n",
    "#### WideEP vs DeepEP Comparison\n",
    "\n",
    "| Dimension | Wide EP | Deep EP |\n",
    "|-----------|---------|---------|\n",
    "| Goal | Maximize throughput & utilization | Enhance specialization |\n",
    "| Structure | Experts spread across GPUs/nodes | Experts grouped hierarchically |\n",
    "| Routing | Token → top-k experts globally | Token → coarse → fine experts |\n",
    "| Use Case | Distributed inference at scale | Hierarchical reasoning |\n",
    "\n",
    "**Visuals:**\n",
    "```\n",
    "WideEP:  GPU clusters -> flat expert pool\n",
    "         [E0,E1,E2,...E64]\n",
    "\n",
    "DeepEP:  Hierarchical expert tree\n",
    "         Root\n",
    "          ├── Language Experts\n",
    "          │    ├── Code Expert\n",
    "          │    ├── Math Expert\n",
    "          ├── Reasoning Experts\n",
    "               ├── Symbolic\n",
    "               ├── Commonsense\n",
    "```\n",
    "\n",
    "### Figure 2: MoE Deployment on NVL72\n",
    "\n",
    "![Figure 2: MoE Deployment NVL72](images/figure3_moe_deployment_nvl72.gif)\n",
    "\n",
    "*Schematic diagram showing an MoE deployment with 232 experts per GPU and only four activated per layer, coordinated across 72 GPUs in a GB200 NVL72 NVLink domain.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Deep Dive: Deep EP\n",
    "\n",
    "- Organizes experts hierarchically (coarse → fine routing)\n",
    "- Improves specialization; useful for mixed domains and complex reasoning tasks\n",
    "- May reduce routing search space at each level; pairs with expert grouping/placement policies\n",
    "\n",
    "**Diagram (conceptual):**\n",
    "```\n",
    "Root\n",
    " ├── Language Experts\n",
    " │    ├── Code Expert\n",
    " │    ├── Math Expert\n",
    " ├── Reasoning Experts\n",
    "      ├── Symbolic\n",
    "      ├── Commonsense\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Deep Dive: Deep EP\n",
    "\n",
    "- Organizes experts hierarchically (coarse → fine routing)\n",
    "- Improves specialization; useful for mixed domains and complex reasoning tasks\n",
    "- May reduce routing search space at each level; pairs with expert grouping/placement policies\n",
    "\n",
    "**Diagram (conceptual):**\n",
    "```\n",
    "Root\n",
    " ├── Language Experts\n",
    " │    ├── Code Expert\n",
    " │    ├── Math Expert\n",
    " ├── Reasoning Experts\n",
    "      ├── Symbolic\n",
    "      ├── Commonsense\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Deep Dive: Dynamic EP (EPLB)\n",
    "\n",
    "**Expert Parallelism Load Balancer (EPLB)** dynamically optimizes expert placement across GPUs to balance workload.\n",
    "\n",
    "#### Critical: EPLB Changes WHERE Experts Live, NOT Which Expert Processes a Token\n",
    "\n",
    "**Important distinction to avoid confusion:**\n",
    "\n",
    "**What the Router Does (Never Changes):**\n",
    "```\n",
    "Token \"quantum\" → Router → Selects Expert 0, Expert 45, Expert 123\n",
    "```\n",
    "- The router's decision is **fixed by the model's learned weights**\n",
    "- If the router says \"Expert 0 should process this token,\" then **Expert 0 MUST process it**\n",
    "- This is critical for correctness and **never changes during inference**\n",
    "\n",
    "**What EPLB Does (Physical Placement Only):**\n",
    "\n",
    "EPLB only changes **WHERE** Expert 0 physically lives, not which tokens it processes.\n",
    "\n",
    "**Without EPLB (naive placement):**\n",
    "```\n",
    "GPU 0: Expert 0, Expert 1  ← Expert 0 gets 1000 tokens, Expert 1 gets 800 tokens\n",
    "GPU 1: Expert 2, Expert 3  ← Expert 2 gets 600 tokens, Expert 3 gets 500 tokens\n",
    "GPU 2: Expert 4, Expert 5  ← Expert 4 gets 300 tokens, Expert 5 gets 200 tokens\n",
    "GPU 3: Expert 6, Expert 7  ← Expert 6 gets 100 tokens, Expert 7 gets 50 tokens\n",
    "\n",
    "Problem: GPU 0 is overloaded (1800 tokens), GPU 3 is mostly idle (150 tokens)\n",
    "Throughput: Limited by slowest GPU!\n",
    "```\n",
    "\n",
    "**With EPLB (smart rebalancing):**\n",
    "```\n",
    "GPU 0: Expert 0           ← Still processes the same 1000 tokens\n",
    "GPU 1: Expert 1, Expert 4 ← 800 + 300 = 1100 tokens\n",
    "GPU 2: Expert 2, Expert 5, Expert 6 ← 600 + 200 + 100 = 900 tokens\n",
    "GPU 3: Expert 3, Expert 7 ← 500 + 50 = 550 tokens\n",
    "\n",
    "Result: All GPUs balanced (~900 tokens average)\n",
    "Throughput: 1.8x improvement!\n",
    "```\n",
    "\n",
    "**Key Point:** The token still goes to Expert 0! It just travels to a different GPU now.\n",
    "\n",
    "#### EPLB Modes\n",
    "\n",
    "- **Static EPLB**: precomputed expert→GPU mappings from historical data\n",
    "- **Online EPLB**: runtime redistribution to match live workload patterns\n",
    "- Can replicate hot experts and relocate cold ones\n",
    "- Performed between forward passes without breaking CUDA graphs\n",
    "- **Goal**: minimize GPU utilization variance and improve tokens/sec\n",
    "\n",
    "#### Expert Replication\n",
    "\n",
    "EPLB can also **replicate** popular experts:\n",
    "```\n",
    "GPU 0: Expert 0 (replica A)\n",
    "GPU 1: Expert 0 (replica B), Expert 1\n",
    "```\n",
    "- Both replicas have **identical weights**\n",
    "- A token needing Expert 0 can go to **either replica** (whichever GPU is less busy)\n",
    "- Output is **exactly the same** regardless of which replica processes it\n",
    "\n",
    "### Figure 3: EPLB Load Balancing\n",
    "\n",
    "![Figure 3: EPLB](images/figure4_eplb_load_balancing.gif)\n",
    "\n",
    "*Diagram showing Expert Parallel Load Balancer (EPLB) redistributes experts to ensure balanced GPU workload, preventing over- and under-utilization.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPLB Load Balancing Demonstration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scenario: 8 experts, 4 GPUs\n",
    "# Each expert has different popularity (tokens to process)\n",
    "expert_popularity = np.array([1000, 800, 600, 500, 300, 200, 100, 50])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EPLB Load Balancing Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n📊 Expert Popularity (tokens each expert needs to process):\")\n",
    "for i, tokens in enumerate(expert_popularity):\n",
    "    bar = '█' * (tokens // 100)\n",
    "    print(f\"  Expert {i}: {bar} {tokens} tokens\")\n",
    "\n",
    "# Without EPLB: naive assignment (2 experts per GPU)\n",
    "print(\"\\n\\n❌ WITHOUT EPLB (Naive Assignment: 2 experts per GPU)\")\n",
    "print(\"-\" * 70)\n",
    "naive_assignment = {\n",
    "    0: [0, 1],  # GPU 0 gets Expert 0, 1\n",
    "    1: [2, 3],  # GPU 1 gets Expert 2, 3\n",
    "    2: [4, 5],  # GPU 2 gets Expert 4, 5\n",
    "    3: [6, 7]   # GPU 3 gets Expert 6, 7\n",
    "}\n",
    "\n",
    "naive_loads = []\n",
    "for gpu_id, expert_ids in naive_assignment.items():\n",
    "    load = sum(expert_popularity[e] for e in expert_ids)\n",
    "    naive_loads.append(load)\n",
    "    expert_str = ', '.join([f\"E{e}\" for e in expert_ids])\n",
    "    print(f\"GPU {gpu_id}: [{expert_str}] → {load} tokens\")\n",
    "\n",
    "print(f\"\\n⚠️  Problem:\")\n",
    "print(f\"    • GPU 0 is overloaded: {max(naive_loads)} tokens\")\n",
    "print(f\"    • GPU 3 is mostly idle: {min(naive_loads)} tokens\")\n",
    "print(f\"    • Throughput limited by slowest GPU!\")\n",
    "\n",
    "# With EPLB: balanced assignment\n",
    "print(\"\\n\\n✅ WITH EPLB (Smart Rebalancing)\")\n",
    "print(\"-\" * 70)\n",
    "eplb_assignment = {\n",
    "    0: [0],        # Most popular expert gets its own GPU\n",
    "    1: [1, 4],     # Balance: 800 + 300 = 1100\n",
    "    2: [2, 5, 6],  # Balance: 600 + 200 + 100 = 900\n",
    "    3: [3, 7]      # Balance: 500 + 50 = 550\n",
    "}\n",
    "\n",
    "eplb_loads = []\n",
    "for gpu_id, expert_ids in eplb_assignment.items():\n",
    "    load = sum(expert_popularity[e] for e in expert_ids)\n",
    "    eplb_loads.append(load)\n",
    "    expert_str = ', '.join([f\"E{e}\" for e in expert_ids])\n",
    "    print(f\"GPU {gpu_id}: [{expert_str}] → {load} tokens\")\n",
    "\n",
    "print(f\"\\n✅ Result:\")\n",
    "print(f\"    • All GPUs balanced (~{np.mean(eplb_loads):.0f} tokens average)\")\n",
    "print(f\"    • Max load reduced from {max(naive_loads)} to {max(eplb_loads)}\")\n",
    "print(f\"    • Throughput improvement: ~{max(naive_loads)/max(eplb_loads):.1f}x faster!\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without EPLB\n",
    "colors_naive = ['#ff6b6b' if load > 1500 else '#ffa500' if load > 1000 else '#90EE90' for load in naive_loads]\n",
    "bars1 = ax1.bar(range(4), naive_loads, color=colors_naive, edgecolor='black', linewidth=1.5)\n",
    "ax1.axhline(y=np.mean(naive_loads), color='blue', linestyle='--', linewidth=2, label=f'Average: {np.mean(naive_loads):.0f}')\n",
    "ax1.set_title('Without EPLB: Imbalanced', fontsize=14, fontweight='bold', color='#d32f2f')\n",
    "ax1.set_xlabel('GPU', fontsize=12)\n",
    "ax1.set_ylabel('Tokens to Process', fontsize=12)\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_xticklabels([f'GPU {i}' for i in range(4)])\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 2000)\n",
    "\n",
    "# Add load labels on bars\n",
    "for i, (bar, load) in enumerate(zip(bars1, naive_loads)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, load + 50, f'{load}', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# With EPLB\n",
    "bars2 = ax2.bar(range(4), eplb_loads, color='#66cc99', edgecolor='black', linewidth=1.5)\n",
    "ax2.axhline(y=np.mean(eplb_loads), color='blue', linestyle='--', linewidth=2, label=f'Average: {np.mean(eplb_loads):.0f}')\n",
    "ax2.set_title('With EPLB: Balanced', fontsize=14, fontweight='bold', color='#388e3c')\n",
    "ax2.set_xlabel('GPU', fontsize=12)\n",
    "ax2.set_ylabel('Tokens to Process', fontsize=12)\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_xticklabels([f'GPU {i}' for i in range(4)])\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 2000)\n",
    "\n",
    "# Add load labels on bars\n",
    "for i, (bar, load) in enumerate(zip(bars2, eplb_loads)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, load + 50, f'{load}', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎯 Key Takeaway:\")\n",
    "print(\"   EPLB monitors expert popularity and redistributes them across GPUs\")\n",
    "print(\"   to prevent bottlenecks and maximize throughput!\")\n",
    "print(\"   Remember: Experts still process the SAME tokens, just on different GPUs!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How WideEP Works in Dynamo\n",
    "\n",
    "Example Dynamo configuration (SGLang backend) for WideEP:\n",
    "\n",
    "```yaml\n",
    "backend: sglang\n",
    "model: deepseek-r1\n",
    "parallelism:\n",
    "  type: wide-ep\n",
    "  num_experts: 64\n",
    "  active_experts: 2\n",
    "  routing: topk\n",
    "  comms: alltoall\n",
    "hardware:\n",
    "  cluster: GB200\n",
    "  gpus_per_node: 8\n",
    "  interconnect: NVLINK\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- Tokens are routed across nodes via high-speed interconnects.\n",
    "- Each expert processes its share to keep memory and compute balanced.\n",
    "- Dynamic padding ensures even per-batch load.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend Support in Dynamo\n",
    "\n",
    "| Backend | EP Support | Notes |\n",
    "|---------|------------|-------|\n",
    "| SGLang | ✅ WideEP, EPLB | Full support for DeepSeek models |\n",
    "| TensorRT-LLM | ✅ via `moe_expert_parallel_size` | Works with Dynamo runtime; strong FP8/KV optimizations |\n",
    "| vLLM | ⚠️ Basic MoE only | EP integration planned (router layer) |\n",
    "\n",
    "Example TensorRT-LLM (conceptual) config:\n",
    "```json\n",
    "{\n",
    "  \"backend\": \"tensorrt-llm\",\n",
    "  \"parallelism\": {\n",
    "    \"type\": \"wide-ep\",\n",
    "    \"moe_ep_size\": 8,\n",
    "    \"enable_dynamic_routing\": true\n",
    "  },\n",
    "  \"hardware\": {\n",
    "    \"gpus\": 64,\n",
    "    \"interconnect\": \"NVLINK\",\n",
    "    \"cluster\": \"GB200\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hands-on Exercises\n",
    "\n",
    "### Exercise 1 — WideEP Routing Simulation\n",
    "\n",
    "Simulate how tokens get assigned to top-k experts.\n",
    "\n",
    "Run the cell below and observe randomized token→experts assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: token → experts (top-k) routing simulation\n",
    "import random\n",
    "\n",
    "experts = [f\"E{i}\" for i in range(8)]\n",
    "tokens = [f\"T{i}\" for i in range(16)]\n",
    "\n",
    "def route(tokens, experts, topk=2):\n",
    "    routing = {}\n",
    "    for t in tokens:\n",
    "        routing[t] = random.sample(experts, topk)\n",
    "    return routing\n",
    "\n",
    "routing = route(tokens, experts)\n",
    "for t, e in routing.items():\n",
    "    print(f\"{t} → {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — Throughput Comparison Chart\n",
    "\n",
    "Visualize relative throughput improvements from different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: simple bar chart of relative throughput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [\"Dense\", \"Standard EP\", \"WideEP\", \"WideEP + EPLB\"]\n",
    "y = [1.0, 1.8, 2.5, 2.9]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x, y, color=[\"#8888ff\", \"#66cc99\", \"#ffcc66\", \"#ff8888\"])\n",
    "plt.title(\"Relative Throughput by Strategy\")\n",
    "plt.ylabel(\"Speedup (× Dense)\")\n",
    "plt.ylim(0, 3.2)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced: Large-Scale Wide-EP on GB200 NVL72\n",
    "\n",
    "Drawing from NVIDIA's recent technical deep-dive, Wide Expert Parallelism (Wide-EP) on GB200 NVL72 emphasizes scaling MoE inference efficiently across many GPUs:\n",
    "\n",
    "- **Large-scale EP**: distributing experts across ≥8 GPUs to increase aggregate bandwidth and utilization.\n",
    "- **Reduce weight-loading pressure**: fewer experts per GPU → less frequent weight swaps → better GroupGEMM efficiency.\n",
    "- **Improve arithmetic intensity**: tokens routed to the same expert are batched into GroupGEMMs for higher FLOPs/byte.\n",
    "- **NVL72 interconnect**: leverages a coherent NVLink domain (~130 TB/s aggregate) to offset all-to-all overhead during decode.\n",
    "- **Custom NCCL kernels**: handle non-static communication sizes and integrate with CUDA graphs at rack scale.\n",
    "- **EPLB modes**: static (precomputed mappings) and online (runtime redistribution), with containerized weight updates between forward passes.\n",
    "- **Observed outcome**: up to ~1.8× higher per-GPU throughput vs smaller EP configurations on NVL72.\n",
    "\n",
    "Reference: [Scaling Large MoE Models with Wide Expert Parallelism on NVL72](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupGEMM and Weight-Loading Intuition\n",
    "\n",
    "MoE layers pack tokens per expert into a single GroupGEMM for efficiency. Wide-EP reduces the number of experts per GPU, easing weight-loading pressure and improving reuse inside the GroupGEMM kernel.\n",
    "\n",
    "### Figure 4: GroupGEMM Token Routing\n",
    "\n",
    "![Figure 4: GroupGEMM](images/figure2_groupgemm_token_routing.webp)\n",
    "\n",
    "*Tokens routed to the same expert are packed together and processed with a single fused GroupGEMM kernel for efficient MoE inference.*\n",
    "\n",
    "The toy simulation below shows:\n",
    "- How expert popularity drives packed group sizes\n",
    "- How increasing EP rank reduces experts-per-GPU (proxy for fewer weights to juggle)\n",
    "\n",
    "Reference: [NVIDIA NVL72 Wide-EP blog](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide-EP: Reducing Experts per GPU\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simple visualization: How Wide-EP distributes 256 experts\n",
    "TOTAL_EXPERTS = 256\n",
    "ep_configs = [\n",
    "    (\"Small EP\\n(8 GPUs)\", 8),\n",
    "    (\"Medium EP\\n(16 GPUs)\", 16),\n",
    "    (\"Wide EP\\n(32 GPUs)\", 32),\n",
    "    (\"Very Wide EP\\n(64 GPUs)\", 64)\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Wide-EP: Distributing 256 Experts Across GPUs\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDeepSeek-V3 has 256 experts. How many experts per GPU?\\n\")\n",
    "\n",
    "labels = []\n",
    "experts_per_gpu = []\n",
    "colors = []\n",
    "\n",
    "for label, num_gpus in ep_configs:\n",
    "    epg = TOTAL_EXPERTS // num_gpus\n",
    "    experts_per_gpu.append(epg)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Color code: red for many experts/GPU, green for few\n",
    "    if epg >= 20:\n",
    "        colors.append('#ff6b6b')  # Red - too many!\n",
    "    elif epg >= 10:\n",
    "        colors.append('#ffa500')  # Orange - moderate\n",
    "    else:\n",
    "        colors.append('#66cc99')  # Green - good!\n",
    "    \n",
    "    print(f\"{label:20s}: {epg:2d} experts/GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Key Insight:\")\n",
    "print(\"  • Fewer experts per GPU = Less weight-loading pressure\")\n",
    "print(\"  • Wide-EP (32-64 GPUs) enables efficient GroupGEMM operations\")\n",
    "print(\"  • Trade-off: More GPUs needed, but higher throughput per GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(range(len(labels)), experts_per_gpu, color=colors, \n",
    "              edgecolor='black', linewidth=1.5, width=0.6)\n",
    "\n",
    "ax.set_title('Wide-EP: Experts per GPU (256 total experts)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('EP Configuration', fontsize=12)\n",
    "ax.set_ylabel('Experts per GPU', fontsize=12)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, experts_per_gpu):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "            f'{value}', ha='center', va='bottom', \n",
    "            fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('More efficient!\\nLess weight-loading', \n",
    "            xy=(3, experts_per_gpu[3]), \n",
    "            xytext=(2.5, 15),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=11, color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Wide-EP (NVL72 perspective)\n",
    "\n",
    "- **Model scale**: Larger MoE (e.g., DeepSeek-R1 with many experts) benefits most.\n",
    "- **Latency-constrained throughput**: Improves tokens/sec/GPU at similar latency targets.\n",
    "- **Hardware**: High-bandwidth interconnect (e.g., NVL72 coherent NVLink) is key to hide all-to-all.\n",
    "- **Scheduling**: Combine disaggregated serving + Wide-EP for best results.\n",
    "\n",
    "**Dynamo + TensorRT-LLM synergy:**\n",
    "- **Dynamo**: orchestrates disaggregated prefill/decode, SLA-aware scaling, routing.\n",
    "- **TensorRT-LLM Wide-EP**: executes expert-parallel MoE with optimized kernels, FP8/graphs, and EPLB.\n",
    "\n",
    "### Figure 5: EP Throughput Comparison\n",
    "\n",
    "![Figure 5: Throughput](images/figure5_ep_throughput_comparison.webp)\n",
    "\n",
    "*Large-scale Expert Parallelism (EP) rank 32 delivers up to 1.8x higher output token throughput per GPU compared to small EP rank 8 at 100 tokens/sec per user. Both configurations leverage disaggregated serving and multi-token prediction (MTP).*\n",
    "\n",
    "Reference: [NVIDIA NVL72 Wide-EP blog](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Lab 3.1. You now understand:\n",
    "\n",
    "- **DP/TP/PP/SP**: apply to all models; solve memory/compute distribution differently.\n",
    "- **EP**: MoE-only; route tokens to a small set of experts per token.\n",
    "- **Dynamo**: supports Standard EP, WideEP, DeepEP, and dynamic EPLB via backends.\n",
    "- **WideEP + EPLB**: production-proven for scaling throughput and balancing load.\n",
    "- **NVL72 insights**: Large-scale EP with high-bandwidth interconnects delivers up to 1.8× per-GPU throughput gains.\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "Ready to deploy these concepts in production? Continue to:\n",
    "\n",
    "**Lab 3.2: Wide EP Production Deployment** (`lab3.2-wide-ep-deployment.ipynb`)\n",
    "\n",
    "In Lab 3.2, you'll learn:\n",
    "- Kubernetes deployment with Dynamo Operator\n",
    "- Multi-node SGLang deployment with EPLB\n",
    "- TensorRT-LLM Wide EP configuration\n",
    "- Monitoring, troubleshooting, and performance tuning\n",
    "- Production best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Further Reading\n",
    "\n",
    "### MoE Architecture and Comparisons\n",
    "- [The Big LLM Architecture Comparison (Sebastian Raschka)](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) - Detailed comparison of DeepSeek-V3, Qwen3, and other MoE architectures\n",
    "\n",
    "### Expert Parallelism Deployment\n",
    "- [Scaling Large MoE Models with Wide Expert Parallelism on NVL72](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/?utm_source=chatgpt.com) - NVIDIA's technical deep-dive on Wide-EP deployment strategies\n",
    "- [TensorRT-LLM Wide-EP Examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/wide_ep)\n",
    "\n",
    "### Dynamo and Backends\n",
    "- [NVIDIA Dynamo Documentation](https://github.com/NVIDIA/Dynamo)\n",
    "- [SGLang Documentation](https://github.com/sgl-project/sglang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
