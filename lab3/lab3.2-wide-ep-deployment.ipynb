{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Wide EP Production Deployment\n",
    "\n",
    "## ğŸ¯ Overview\n",
    "\n",
    "Welcome to Lab 3.2! Now that you understand Expert Parallelism concepts from Lab 3.1, you'll **deploy a real MoE model** (DeepSeek-R1) using Wide EP in a production Kubernetes environment.\n",
    "\n",
    "**What you'll do in this lab:**\n",
    "1. âœ… Verify your environment is ready\n",
    "2. ğŸ”§ Build a custom Docker image with DeepEP support\n",
    "3. ğŸš€ Deploy DeepSeek-R1 with Wide Expert Parallelism\n",
    "4. ğŸ“Š Monitor expert load balancing (EPLB) in action\n",
    "5. ğŸ¯ Benchmark and optimize performance\n",
    "\n",
    "**Time Required**: 90-120 minutes (includes Docker image build)\n",
    "\n",
    "**Hardware Requirements**: \n",
    "- 16 GPUs minimum (2 nodes Ã— 8 GPUs each)\n",
    "- High-bandwidth interconnect (NVLink or InfiniBand recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Before You Start\n",
    "\n",
    "**Prerequisites (must be completed first):**\n",
    "- âœ… Lab 3.1: Expert Parallelism Foundations\n",
    "- âœ… Kubernetes cluster with GPU nodes\n",
    "- âœ… **Dynamo Operator already installed** (from Lab 1 or Lab 2)\n",
    "- âœ… kubectl configured and working\n",
    "- âœ… HuggingFace account with access token\n",
    "\n",
    "ğŸ’¡ **Note**: This lab assumes you've completed Lab 1 or Lab 2, where you installed the Dynamo Operator. We'll skip the operator installation and focus on deploying the MoE model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "**Part 1: Setup & Prerequisites**\n",
    "- [Quick Recap: Lab 3.1 Concepts](#Quick-Recap:-Lab-3.1-Concepts)\n",
    "- [Prerequisites Check](#Prerequisites-Check)\n",
    "\n",
    "**Part 2: Deployment**\n",
    "- [Understanding Your Deployment Options](#Understanding-Your-Deployment-Options)\n",
    "- [Step-by-Step Deployment Guide](#Step-by-Step-Deployment-Guide)\n",
    "\n",
    "**Part 3: Configuration Deep Dive**\n",
    "- [SGLang Configuration Details](#Section-3:-Deploying-MoE-Models-with-SGLang-and-Expert-Parallelism)\n",
    "- [Monitoring Expert Parallelism and EPLB](#Monitoring-Expert-Parallelism-and-EPLB)\n",
    "\n",
    "**Part 4: Performance**\n",
    "- [Benchmarking Your Deployment](#Section-4:-Performance-Benchmarking-for-EP-Deployments)\n",
    "\n",
    "**Wrap-Up**\n",
    "- [Summary](#Summary)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Quick Recap: Lab 3.1 Concepts\n",
    "\n",
    "In Lab 3.1, you learned the foundations of Expert Parallelism. Here's a quick refresher:\n",
    "\n",
    "**Key Concepts**:\n",
    "- **MoE Models**: Only activate a subset of experts per token (e.g., 8 out of 256 experts)\n",
    "- **Expert Parallelism (EP)**: Distribute experts across GPUs to scale capacity\n",
    "- **Wide EP**: Spread experts across many nodes for maximum throughput\n",
    "- **EPLB**: Dynamic load balancing to prevent GPU hotspots\n",
    "\n",
    "**What you're deploying today:**\n",
    "```\n",
    "DeepSeek-R1 Model:\n",
    "  - 671B total parameters\n",
    "  - 256 experts (distributed via EP)\n",
    "  - Only 8 experts active per token (~37B active params)\n",
    "  - Disaggregated: Separate prefill & decode workers\n",
    "```\n",
    "\n",
    "**Deployment Architecture** (what we're building):\n",
    "```\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚  Frontend   â”‚\n",
    "                â”‚  (CPU)      â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚                       â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Prefill Workerâ”‚       â”‚ Decode Worker â”‚\n",
    "    â”‚   Node 1      â”‚       â”‚   Node 2      â”‚\n",
    "    â”‚   8 GPUs      â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   8 GPUs      â”‚\n",
    "    â”‚               â”‚ NIXL  â”‚               â”‚\n",
    "    â”‚  TP=8, EP=8   â”‚  KV   â”‚ TP=8, DP=8    â”‚\n",
    "    â”‚               â”‚       â”‚ EP=8          â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Now let's verify your environment and deploy!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Prerequisites Check\n",
    "\n",
    "Before we start deploying, let's verify your environment is ready. Run the following checks:\n",
    "\n",
    "---\n",
    "\n",
    "### Check 1: Verify Kubernetes Access\n",
    "\n",
    "Make sure you can access your Kubernetes cluster and see your GPU nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 2: Verify GPU Availability\n",
    "\n",
    "Check that you have at least 16 GPUs available across your nodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Kubernetes Cluster Info ===\n",
      "Kubernetes control plane is running at https://pu.mk8scluster-e00wp16ma54aq8nxch.mk8s.eu-north1.nebius.cloud:443\n",
      "CoreDNS is running at https://pu.mk8scluster-e00wp16ma54aq8nxch.mk8s.eu-north1.nebius.cloud:443/api/v1/namespaces/kube-system/services/coredns:udp-53/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "\n",
      "=== GPU Nodes ===\n",
      "NAME                                 GPUs\n",
      "computeinstance-e00a2atjrzyqw4exyb   8\n",
      "computeinstance-e00agtdhe3kkf3n9bm   8\n",
      "computeinstance-e00b2fejeckmrtrapb   8\n",
      "computeinstance-e00bqd1zygs98b1a4v   <none>\n",
      "computeinstance-e00bt38gd8kqc4cv87   8\n",
      "computeinstance-e00etcykgsd2pc3jeg   8\n",
      "computeinstance-e00fnwgf81feck707q   8\n",
      "computeinstance-e00g0nhq7x94mnke6q   8\n",
      "computeinstance-e00gnc7w1wke8558zb   8\n",
      "computeinstance-e00j0zkrwwx8g4fwwm   8\n",
      "computeinstance-e00k0jh433ns6pjwpv   8\n",
      "computeinstance-e00m1pty82yq5cdbz4   8\n",
      "computeinstance-e00mvtw5qrc0ckppvb   <none>\n",
      "computeinstance-e00nq20bp314y9yqx1   8\n",
      "computeinstance-e00qbecckv61jy6hsx   8\n",
      "computeinstance-e00qwksftphymhj3ar   8\n",
      "computeinstance-e00reahwn02d93vbfe   8\n",
      "computeinstance-e00x17zs8a0kk3hjpq   8\n",
      "\n",
      "âœ… If you see your nodes with GPUs listed above, you're good to go!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check Kubernetes access and nodes\n",
    "echo \"=== Kubernetes Cluster Info ===\"\n",
    "kubectl cluster-info\n",
    "\n",
    "echo -e \"\\n=== GPU Nodes ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.allocatable.'nvidia\\.com/gpu'\n",
    "\n",
    "echo -e \"\\nâœ… If you see your nodes with GPUs listed above, you're good to go!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 3: Verify Dynamo Operator is Installed\n",
    "\n",
    "Since you completed Lab 1 or Lab 2, the Dynamo Operator should already be installed. Let's verify:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking Dynamo CRDs ===\n",
      "dynamocomponentdeployments.nvidia.com               2025-10-21T14:49:52Z\n",
      "dynamocomponents.nvidia.com                         2025-10-25T05:16:10Z\n",
      "dynamographdeploymentrequests.nvidia.com            2025-10-13T17:52:51Z\n",
      "dynamographdeployments.nvidia.com                   2025-09-04T20:56:40Z\n",
      "\n",
      "=== Checking Dynamo Operator Pod ===\n",
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "dynamo-platform-dynamo-operator-controller-manager-79d445cnfj94   2/2     Running   0          33h\n",
      "\n",
      "=== Checking Platform Components ===\n",
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "dynamo-platform-dynamo-operator-controller-manager-79d445cnfj94   2/2     Running   0          33h\n",
      "dynamo-platform-etcd-0                                            1/1     Running   0          33h\n",
      "dynamo-platform-nats-0                                            1/1     Running   0          33h\n",
      "\n",
      "âœ… If you see dynamo-operator, etcd, and nats pods Running above, you're ready!\n",
      "âš ï¸  If not, please complete Lab 1 or Lab 2 first to install the Dynamo Operator.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "echo \"=== Checking Dynamo CRDs ===\"\n",
    "kubectl get crd | grep dynamo || echo \"âš ï¸  No Dynamo CRDs found!\"\n",
    "\n",
    "echo -e \"\\n=== Checking Dynamo Operator Pod ===\"\n",
    "kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=dynamo-operator || echo \"âš ï¸  Dynamo operator not found!\"\n",
    "\n",
    "echo -e \"\\n=== Checking Platform Components ===\"\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "\n",
    "echo -e \"\\nâœ… If you see dynamo-operator, etcd, and nats pods Running above, you're ready!\"\n",
    "echo \"âš ï¸  If not, please complete Lab 1 or Lab 2 first to install the Dynamo Operator.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Understanding Your Deployment Options\n",
    "\n",
    "Great! Your environment is ready. Now let's understand what we're about to deploy.\n",
    "\n",
    "### What is Wide EP?\n",
    "\n",
    "**Wide Expert Parallelism (Wide EP)** distributes the 256 experts of DeepSeek-R1 across multiple GPUs:\n",
    "\n",
    "```\n",
    "Without EP (Won't fit!):          With Wide EP (âœ… Works!):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Single GPU    â”‚              â”‚  GPU 0  â”‚  GPU 1  â”‚  GPU 2  â”‚\n",
    "â”‚  256 experts    â”‚              â”‚ 32 exp  â”‚ 32 exp  â”‚ 32 exp  â”‚...\n",
    "â”‚  (Out of memory)â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              Tokens routed to correct expert\n",
    "```\n",
    "\n",
    "### Deployment Configurations Available\n",
    "\n",
    "We have **two pre-configured deployment options** for 16 GPUs:\n",
    "\n",
    "**Option 1: Single-Node Workers** â­ **RECOMMENDED**\n",
    "- **Hardware**: 2 nodes Ã— 8 GPUs each = 16 GPUs total\n",
    "- **Prefill Worker**: 1 pod on Node 1 (8 GPUs: TP=8, EP=8)\n",
    "- **Decode Worker**: 1 pod on Node 2 (8 GPUs: TP=8, DP=8, EP=8)\n",
    "- **Benefits**: Simpler, faster NVLink, easier debugging\n",
    "- **File**: `k8s/deepseek-r1-8gpu-singlenode.yaml`\n",
    "\n",
    "**Option 2: Multi-Node Workers** (Advanced)\n",
    "- **Hardware**: 4 nodes Ã— 4 GPUs each = 16 GPUs total\n",
    "- **Prefill Worker**: 2 pods across 2 nodes (TP=8 cross-node)\n",
    "- **Decode Worker**: 2 pods across 2 nodes (TP=8 cross-node)\n",
    "- **Benefits**: Shows multi-node coordination\n",
    "- **Requirements**: Different hardware (4 nodes instead of 2)\n",
    "- **File**: `k8s/deepseek-r1-16gpu-multinode.yaml`\n",
    "\n",
    "ğŸ’¡ **For this lab, we'll use Option 1** (Single-Node Workers) as it's simpler and provides better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Step-by-Step Deployment Guide\n",
    "\n",
    "Now let's deploy DeepSeek-R1 with Wide EP! Follow these steps in order.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Set Environment Variables\n",
    "\n",
    "First, set the namespace variable that will be used throughout the deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set namespace for all subsequent commands\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "echo \"ğŸ”§ Using namespace: ${NAMESPACE}\"\n",
    "echo \"âœ… This namespace should already have Dynamo Operator installed from Lab 1/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create HuggingFace Token Secret\n",
    "\n",
    "Create a Kubernetes secret with your HuggingFace token for downloading the DeepSeek-R1 model.\n",
    "\n",
    "**Get your token**: https://huggingface.co/settings/tokens (requires accepting DeepSeek-R1 license)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "# Create HuggingFace token secret\n",
    "# âš ï¸ IMPORTANT: Replace 'your_hf_token_here' with your actual HuggingFace token!\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "# Uncomment and edit the line below with your actual token:\n",
    "# kubectl create secret generic hf-token-secret \\\n",
    "#   --from-literal=HF_TOKEN='your_hf_token_here' \\\n",
    "#   -n ${NAMESPACE}\n",
    "\n",
    "# Check if secret already exists\n",
    "kubectl get secret hf-token-secret -n ${NAMESPACE} 2>/dev/null && \\\n",
    "  echo \"âœ… HuggingFace token secret already exists!\" || \\\n",
    "  echo \"âš ï¸  Please create the secret by uncommenting and editing the command above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build Custom Docker Image (Part 1 - Overview)\n",
    "\n",
    "The deployment requires a custom Docker image with **DeepEP** support for Wide EP functionality.\n",
    "\n",
    "**What is DeepEP?** SGLang's optimized backend for Expert Parallelism with:\n",
    "- âœ… Efficient all-to-all communication for expert routing\n",
    "- âœ… EPLB (Expert Parallel Load Balancer) for dynamic load balancing\n",
    "- âœ… Optimized GroupGEMM kernels for MoE layers\n",
    "\n",
    "**Two options:**\n",
    "1. **Build locally** (30-60 mins) - Use for single-node testing\n",
    "2. **Build and push to registry** - Required for multi-node deployments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Build the Docker image with DeepEP support\n",
    "# This will take 30-60 minutes\n",
    "\n",
    "echo \"ğŸ—ï¸  Building Docker image with DeepEP support...\"\n",
    "echo \"ğŸ“ This may take 30-60 minutes depending on your machine\"\n",
    "echo \"\"\n",
    "\n",
    "# Uncomment the following lines to build:\n",
    "# cd /mnt/raid/dynamo-workshop/dynamo\n",
    "# git checkout v0.6.0\n",
    "# docker build -f container/Dockerfile.sglang-wideep -t dynamo-wideep:0.6.0 .\n",
    "\n",
    "echo \"âš ï¸  Uncomment the build commands above to start the build\"\n",
    "echo \"ğŸ’¡ Or use a pre-built image if available in your registry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Part 2): Push to Registry (Optional - for Multi-Node only)\n",
    "\n",
    "**Only needed if deploying multi-node** where pods run on different physical nodes.\n",
    "\n",
    "For **single-node deployment** (recommended), you can skip this and use the local image.\n",
    "\n",
    "**For multi-node**, tag and push to your container registry:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# OPTIONAL: Only for multi-node deployments\n",
    "# Tag and push image to your container registry\n",
    "\n",
    "# Replace <your-registry> with your actual registry (e.g., docker.io/username, gcr.io/project)\n",
    "# Uncomment to push:\n",
    "\n",
    "# docker tag dynamo-wideep:0.6.0 <your-registry>/dynamo-wideep:0.6.0\n",
    "# docker push <your-registry>/dynamo-wideep:0.6.0\n",
    "\n",
    "# Then update manifests:\n",
    "# cd /mnt/raid/dynamo-workshop/lab3\n",
    "# sed -i 's|<your-registry>/dynamo-wideep:0.6.0|your-actual-registry/dynamo-wideep:0.6.0|g' k8s/deepseek-r1-8gpu-singlenode.yaml\n",
    "\n",
    "echo \"ğŸ’¡ For single-node deployment, you can use the local image: dynamo-wideep:0.6.0\"\n",
    "echo \"ğŸ’¡ For multi-node, uncomment and run the commands above\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Deploy DeepSeek-R1 with Wide EP! ğŸš€\n",
    "\n",
    "Time to deploy! This will create a **disaggregated deployment** with:\n",
    "- **Frontend**: Handles requests (CPU only)\n",
    "- **Prefill Worker**: Processes prompts (8 GPUs on Node 1)\n",
    "- **Decode Worker**: Generates tokens (8 GPUs on Node 2)\n",
    "- **Expert Parallelism**: 256 experts distributed across 8 GPUs per worker\n",
    "- **EPLB**: Dynamic load balancing of experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Deploying DeepSeek-R1 with Wide Expert Parallelism...\n",
      "\n",
      "dynamographdeployment.nvidia.com/sgl-dsr1-8gpu created\n",
      "\n",
      "âœ… Deployment created!\n",
      "\n",
      "ğŸ“Š Checking deployment status...\n",
      "NAME            AGE\n",
      "sgl-dsr1-8gpu   1s\n",
      "\n",
      "ğŸ’¡ The pods will now start. This may take 5-10 minutes as:\n",
      "   1. Model weights are downloaded from HuggingFace (~100GB)\n",
      "   2. Workers initialize and load the model into GPU memory\n",
      "   3. Expert Parallelism topology is established\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "echo \"ğŸš€ Deploying DeepSeek-R1 with Wide Expert Parallelism...\"\n",
    "echo \"\"\n",
    "\n",
    "# Deploy using Single-Node Workers configuration (recommended)\n",
    "kubectl apply -f k8s/deepseek-r1-8gpu-singlenode.yaml -n ${NAMESPACE}\n",
    "\n",
    "# For multi-node deployment (if you have 4 nodes Ã— 4 GPUs), use:\n",
    "# kubectl apply -f k8s/deepseek-r1-16gpu-multinode.yaml -n ${NAMESPACE}\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… Deployment created!\"\n",
    "echo \"\"\n",
    "echo \"ğŸ“Š Checking deployment status...\"\n",
    "kubectl get dynamographdeployment -n ${NAMESPACE}\n",
    "\n",
    "echo \"\"\n",
    "echo \"ğŸ’¡ The pods will now start. This may take 5-10 minutes as:\"\n",
    "echo \"   1. Model weights are downloaded from HuggingFace (~100GB)\"\n",
    "echo \"   2. Workers initialize and load the model into GPU memory\"\n",
    "echo \"   3. Expert Parallelism topology is established\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Monitor Deployment Progress\n",
    "\n",
    "Watch the pods as they start up. You should see:\n",
    "1. **Frontend pod** (starts quickly, CPU only)\n",
    "2. **Prefill worker pod(s)** (downloading model, loading to GPU)\n",
    "3. **Decode worker pod(s)** (downloading model, loading to GPU)\n",
    "\n",
    "**Expected pod states:**\n",
    "- `Pending` â†’ `ContainerCreating` â†’ `Running`\n",
    "- Model download takes 5-10 minutes\n",
    "- Look for \"Model loaded successfully\" in logs\n",
    "\n",
    "ğŸ’¡ **Press Ctrl+C to stop watching** (pods continue running in background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "# Watch pods being created and starting\n",
    "# Press Ctrl+C to stop watching\n",
    "kubectl get pods -n ${NAMESPACE} -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Check Pod Logs (Troubleshooting)\n",
    "\n",
    "If pods are stuck or you want to see progress, check the logs:\n",
    "\n",
    "**What to look for:**\n",
    "- âœ… \"Downloading model...\" â†’ \"Model loaded successfully\"\n",
    "- âœ… \"EP topology established\" â†’ Expert Parallelism is working\n",
    "- âœ… \"EPLB initialized\" â†’ Load balancing is active\n",
    "- âš ï¸ Errors about memory, CUDA, or missing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "# Check logs of prefill workers\n",
    "kubectl logs -n ${NAMESPACE} -l component=prefill --tail=50\n",
    "\n",
    "# Check logs of decode workers\n",
    "kubectl logs -n ${NAMESPACE} -l component=decode --tail=50\n",
    "\n",
    "# Check frontend logs\n",
    "kubectl logs -n ${NAMESPACE} -l component=frontend --tail=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 7: Test the Deployment! ğŸ‰\n",
    "\n",
    "Once all pods are `Running`, let's test the deployment!\n",
    "\n",
    "#### Step 7a: Port Forward to Access the Frontend\n",
    "\n",
    "Create a port forward to access the deployment from your local machine.\n",
    "\n",
    "ğŸ’¡ **Note**: Keep this running in the background or in a separate terminal while testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=\"dynamo-workshop\"\n",
    "\n",
    "echo \"ğŸ”Œ Creating port forward to access the frontend...\"\n",
    "echo \"ğŸ’¡ Press Ctrl+C to stop (or run in background with & at the end)\"\n",
    "echo \"\"\n",
    "\n",
    "# Port forward to access the frontend\n",
    "kubectl port-forward svc/deepseek-r1-wideep-frontend 8000:8000 -n ${NAMESPACE}\n",
    "\n",
    "# To run in background instead:\n",
    "# kubectl port-forward svc/deepseek-r1-wideep-frontend 8000:8000 -n ${NAMESPACE} &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7b: Send a Test Request\n",
    "\n",
    "Send a test request to verify Expert Parallelism is working!\n",
    "\n",
    "ğŸ’¡ **Make sure** the port-forward from the previous cell is still running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ğŸ§ª Testing Wide EP deployment with a sample request...\"\n",
    "echo \"\"\n",
    "\n",
    "# Test the deployment with a simple request\n",
    "curl -s http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain MoE models in one sentence\"}],\n",
    "    \"max_tokens\": 100\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… If you see a response above, your Wide EP deployment is working!\"\n",
    "echo \"ğŸ¯ Behind the scenes:\"\n",
    "echo \"   - Your request was processed by the frontend\"\n",
    "echo \"   - Routed to prefill worker (prompt encoding)\"\n",
    "echo \"   - Tokens were routed to correct experts (out of 256)\"\n",
    "echo \"   - Decode worker generated the response\"\n",
    "echo \"   - EPLB balanced the load across GPUs!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations! Your Wide EP Deployment is Running!\n",
    "\n",
    "If you received a response above, you've successfully deployed DeepSeek-R1 with Wide Expert Parallelism!\n",
    "\n",
    "### What You Just Accomplished\n",
    "\n",
    "âœ… **Deployed a 671B parameter MoE model** with 256 experts  \n",
    "âœ… **Configured Expert Parallelism** to distribute experts across GPUs  \n",
    "âœ… **Set up disaggregated serving** with separate prefill/decode workers  \n",
    "âœ… **Enabled EPLB** for dynamic load balancing  \n",
    "âœ… **Made your first inference request** through the Wide EP deployment\n",
    "\n",
    "### What's Happening Under the Hood?\n",
    "\n",
    "Every request you send triggers this flow:\n",
    "1. **Frontend** receives request â†’ routes to prefill worker\n",
    "2. **Prefill Worker** (8 GPUs):\n",
    "   - Processes prompt tokens in parallel\n",
    "   - **Router** selects top-8 experts per token (out of 256)\n",
    "   - Tokens routed to correct GPUs via all-to-all communication\n",
    "   - **EPLB** balances load if some experts are hot\n",
    "   - Generates KV cache\n",
    "3. **KV Transfer** via NIXL to decode worker\n",
    "4. **Decode Worker** (8 GPUs):\n",
    "   - Generates tokens autoregressively\n",
    "   - Same expert routing and EPLB\n",
    "5. **Response** streamed back to you\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Want to learn more?**\n",
    "- ğŸ“š **Section 3**: Deep dive into SGLang configuration and EPLB tuning\n",
    "- ğŸ“Š **Section 4**: Benchmark and optimize performance\n",
    "- ğŸ” **Explore**: Check out the configuration files in `k8s/` directory\n",
    "\n",
    "**Ready for production?**\n",
    "- Scale to more replicas for higher throughput\n",
    "- Tune EPLB parameters for your workload\n",
    "- Set up monitoring and alerting\n",
    "\n",
    "### Cleanup (when done)\n",
    "\n",
    "To remove the deployment and free up GPUs:\n",
    "\n",
    "```bash\n",
    "# For single-node deployment\n",
    "kubectl delete -f k8s/deepseek-r1-8gpu-singlenode.yaml -n dynamo-workshop\n",
    "\n",
    "# For multi-node deployment\n",
    "# kubectl delete -f k8s/deepseek-r1-16gpu-multinode.yaml -n dynamo-workshop\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Part 3: Configuration Deep Dive\n",
    "\n",
    "Now that you have a working deployment, let's understand the configuration details and how to tune it for your needs.\n",
    "\n",
    "### Understanding the Kubernetes Manifest\n",
    "\n",
    "The `k8s/deepseek-r1-8gpu-singlenode.yaml` file you deployed contains:\n",
    "\n",
    "**Key Configuration Parameters:**\n",
    "- **Expert Parallelism**: `--ep-size 8` (256 experts across 8 GPUs = 32 experts per GPU)\n",
    "- **Tensor Parallelism**: `--tp-size 8` (model weights split across 8 GPUs)\n",
    "- **Data Parallelism**: `--dp-size 8` (decode only, for batch processing)\n",
    "- **EPLB**: `--ep-num-redundant-experts 32` (hot expert replication)\n",
    "- **Disaggregation**: Separate prefill and decode workers with NIXL transfer\n",
    "\n",
    "**Why these numbers?**\n",
    "- DeepSeek-R1 has 256 experts â†’ with EP=8, each GPU handles 32 experts\n",
    "- TP=8 allows the 37B active parameters to fit across 8 GPUs\n",
    "- DP=8 on decode enables parallel processing of multiple requests\n",
    "- EPLB with 32 redundant experts prevents bottlenecks on hot experts\n",
    "- High-bandwidth interconnect (InfiniBand or NVLink preferred)\n",
    "\n",
    "**Check GPU availability**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGLang Deployment Configurations\n",
    "\n",
    "All SGLang configuration is done via command-line arguments in the Kubernetes manifests.\n",
    "\n",
    "**Available Manifests** (based on official Dynamo recipes):\n",
    "1. `k8s/deepseek-r1-8gpu-singlenode.yaml` - 8 GPUs (Example 1)\n",
    "2. `k8s/deepseek-r1-16gpu-multinode.yaml` - 16 GPUs (Example 2)\n",
    "\n",
    "Each manifest includes complete configuration for:\n",
    "- Expert Parallelism parameters (`--tp-size`, `--dp-size`, `--ep-size`)\n",
    "- EPLB settings (EP redundancy, load balancing)\n",
    "- Memory optimization (`--mem-fraction-static`)\n",
    "- Disaggregated prefill/decode workers with NIXL transfer\n",
    "\n",
    "**Reference**: [Official Dynamo Recipes](https://github.com/ai-dynamo/dynamo/tree/main/recipes/deepseek-r1/sglang-wideep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Lab 3 DeepSeek-R1 Deployment Manifests\n",
      "============================================================\n",
      "\n",
      "Available Manifests:\n",
      "------------------------------------------------------------\n",
      "  âœ“ deepseek-r1-8gpu-singlenode.yaml         ( 3,178 bytes)\n",
      "    Example 1: 8 GPUs (Single-Node)\n",
      "  âœ“ deepseek-r1-16gpu-multinode.yaml         ( 3,646 bytes)\n",
      "    Example 2: 16 GPUs (Multi-Node)\n",
      "  âœ“ README.md                                Kubernetes Deployment Guide\n",
      "\n",
      "ğŸ“ Based on official Dynamo recipes:\n",
      "   https://github.com/ai-dynamo/dynamo/tree/main/recipes/deepseek-r1/sglang-wideep\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# List available Kubernetes manifests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Lab 3 DeepSeek-R1 Deployment Manifests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "k8s_dir = Path(\"k8s\")\n",
    "if k8s_dir.exists():\n",
    "    manifests = [\n",
    "        (\"deepseek-r1-8gpu-singlenode.yaml\", \"Example 1: 8 GPUs (Single-Node)\"),\n",
    "        (\"deepseek-r1-16gpu-multinode.yaml\", \"Example 2: 16 GPUs (Multi-Node)\"),\n",
    "        (\"README.md\", \"Kubernetes Deployment Guide\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nAvailable Manifests:\")\n",
    "    print(\"-\" * 60)\n",
    "    for filename, description in manifests:\n",
    "        file = k8s_dir / filename\n",
    "        if file.exists():\n",
    "            if filename.endswith('.yaml'):\n",
    "                size = file.stat().st_size\n",
    "                print(f\"  âœ“ {filename:<40} ({size:>6,} bytes)\")\n",
    "                print(f\"    {description}\")\n",
    "            else:\n",
    "                print(f\"  âœ“ {filename:<40} {description}\")\n",
    "else:\n",
    "    print(\"  âš ï¸  k8s/ directory not found\")\n",
    "\n",
    "print(\"\\nğŸ“ Based on official Dynamo recipes:\")\n",
    "print(\"   https://github.com/ai-dynamo/dynamo/tree/main/recipes/deepseek-r1/sglang-wideep\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Single-Node Workers (Recommended for 2 Nodes Ã— 8 GPUs)\n",
    "\n",
    "**âœ… Use this configuration for your setup!** (2 nodes Ã— 8 GPUs each = 16 GPUs total)\n",
    "\n",
    "This deploys DeepSeek-R1 with disaggregated prefill/decode workers, each worker running on a single node.\n",
    "\n",
    "**Manifest**: `k8s/deepseek-r1-8gpu-singlenode.yaml`\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚  Frontend   â”‚\n",
    "                â”‚  (CPU only) â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚                       â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Prefill Workerâ”‚       â”‚ Decode Worker â”‚\n",
    "    â”‚   Node 1      â”‚       â”‚   Node 2      â”‚\n",
    "    â”‚   8 GPUs      â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   8 GPUs      â”‚\n",
    "    â”‚               â”‚ NIXL  â”‚               â”‚\n",
    "    â”‚  TP=8, EP=8   â”‚  KV   â”‚ TP=8, DP=8    â”‚\n",
    "    â”‚               â”‚       â”‚ EP=8, DP-Attn â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Configuration Details**:\n",
    "\n",
    "**Total Resources**: 16 GPUs across 2 nodes\n",
    "\n",
    "**Prefill Worker** (1 pod, 8 GPUs on Node 1):\n",
    "- TP=8: Model tensor parallelism across 8 GPUs (NVLink within node)\n",
    "- EP=8: 256 experts distributed across 8 GPUs\n",
    "- Processes prompt encoding\n",
    "- Transfers KV cache to decode via NIXL\n",
    "\n",
    "**Decode Worker** (1 pod, 8 GPUs on Node 2):\n",
    "- TP=8: Model tensor parallelism across 8 GPUs (NVLink within node)\n",
    "- DP=8: Data parallelism for batch processing\n",
    "- EP=8: Expert parallelism (256 experts distributed)\n",
    "- DP Attention: Parallel attention computation\n",
    "- Receives KV cache from prefill\n",
    "- Generates tokens autoregressively\n",
    "\n",
    "**Key Parameters**:\n",
    "```bash\n",
    "# Prefill Worker (Node 1)\n",
    "--model-path deepseek-ai/DeepSeek-R1\n",
    "--tp-size 8\n",
    "--ep-size 8\n",
    "--disaggregation-mode prefill\n",
    "--disaggregation-transfer-backend nixl\n",
    "\n",
    "# Decode Worker (Node 2)\n",
    "--model-path deepseek-ai/DeepSeek-R1\n",
    "--tp-size 8\n",
    "--dp-size 8\n",
    "--ep-size 8\n",
    "--enable-dp-attention\n",
    "--disaggregation-mode decode\n",
    "--disaggregation-transfer-backend nixl\n",
    "```\n",
    "\n",
    "**Why this configuration?**\n",
    "- âœ… **Perfect for 2 nodes Ã— 8 GPUs** (your hardware!)\n",
    "- âœ… Each worker stays on one node - fast NVLink communication\n",
    "- âœ… No cross-node TP overhead - better performance\n",
    "- âœ… Simpler to deploy and debug\n",
    "- âœ… Optimal for learning Wide EP concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multi-Node Workers (For Larger Clusters)\n",
    "\n",
    "**Note**: This configuration is for clusters with **4+ nodes** and demonstrates advanced multi-node deployment patterns.\n",
    "\n",
    "**Your Setup**: You have **2 nodes Ã— 8 GPUs = 16 GPUs**. Use **Example 1** (`deepseek-r1-8gpu-singlenode.yaml`).\n",
    "\n",
    "**Manifest**: `k8s/deepseek-r1-16gpu-multinode.yaml` (requires 4 nodes Ã— 4 GPUs)\n",
    "\n",
    "**Architecture** (for 4-node clusters):\n",
    "```\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚  Frontend   â”‚\n",
    "                â”‚  (CPU only) â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚                         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Prefill Worker    â”‚     â”‚ Decode Worker    â”‚\n",
    "    â”‚  (Multi-node)     â”‚     â”‚  (Multi-node)    â”‚\n",
    "    â”‚                   â”‚     â”‚                  â”‚\n",
    "    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "    â”‚ â”‚Node 1      â”‚    â”‚â”€â”€â”€â”€â–¶â”‚ â”‚Node 3      â”‚   â”‚\n",
    "    â”‚ â”‚4 GPUs      â”‚    â”‚NIXL â”‚ â”‚4 GPUs      â”‚   â”‚\n",
    "    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ KV  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "    â”‚ â”‚Node 2      â”‚    â”‚     â”‚ â”‚Node 4      â”‚   â”‚\n",
    "    â”‚ â”‚4 GPUs      â”‚    â”‚     â”‚ â”‚4 GPUs      â”‚   â”‚\n",
    "    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "    â”‚                   â”‚     â”‚                  â”‚\n",
    "    â”‚ Total: 8 GPUs     â”‚     â”‚ Total: 8 GPUs    â”‚\n",
    "    â”‚ TP=8, EP=8        â”‚     â”‚ TP=8, DP=8, EP=8 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Configuration Details**:\n",
    "\n",
    "**Total Resources**: 16 GPUs across **4 nodes** (4 GPUs per node)\n",
    "\n",
    "**Prefill Worker** (2 pods Ã— 4 GPUs = 8 GPUs):\n",
    "- Multi-node: 2 pods across 2 nodes (Node 1 + Node 2)\n",
    "- TP=8: Model sharded across 8 GPUs (cross-node via NCCL)\n",
    "- EP=8: 256 experts distributed across 8 GPUs\n",
    "- Requires InfiniBand/RDMA for efficient cross-node communication\n",
    "\n",
    "**Decode Worker** (2 pods Ã— 4 GPUs = 8 GPUs):\n",
    "- Multi-node: 2 pods across 2 nodes (Node 3 + Node 4)\n",
    "- TP=8: Model sharded across 8 GPUs (cross-node via NCCL)\n",
    "- DP=8: Data parallelism for batch processing\n",
    "- EP=8: Expert parallelism\n",
    "\n",
    "**When to use this**:\n",
    "- âš ï¸ **You need 4 nodes** with 4 GPUs each (not 2 nodes with 8 GPUs each)\n",
    "- Shows advanced multi-node coordination\n",
    "- Demonstrates cross-node TP/EP communication\n",
    "- Requires excellent inter-node networking (25+ Gbps InfiniBand)\n",
    "\n",
    "**For your 2-node Ã— 8 GPU setup**: Use Example 1 instead!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek-R1 Deployment Examples\n",
    "# Choose the right configuration for your hardware\n",
    "\n",
    "print(\"\"\"\n",
    "=================================================================\n",
    "DeepSeek-R1 Wide EP Deployment Commands\n",
    "=================================================================\n",
    "\n",
    "Example 1: Single-Node Workers (âœ… USE THIS for 2 nodes Ã— 8 GPUs)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Manifest: k8s/deepseek-r1-8gpu-singlenode.yaml\n",
    "Deploy:   kubectl apply -f k8s/deepseek-r1-8gpu-singlenode.yaml\n",
    "\n",
    "Hardware: 2 nodes Ã— 8 GPUs each = 16 GPUs total\n",
    "Architecture:\n",
    "  - Prefill: 1 pod Ã— 8 GPUs on Node 1 (TP=8, EP=8)\n",
    "  - Decode:  1 pod Ã— 8 GPUs on Node 2 (TP=8, DP=8, EP=8)\n",
    "  - Fast NVLink within each node, no cross-node TP overhead\n",
    "\n",
    "\n",
    "Example 2: Multi-Node Workers (âš ï¸  Requires 4 nodes Ã— 4 GPUs)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Manifest: k8s/deepseek-r1-16gpu-multinode.yaml\n",
    "Deploy:   kubectl apply -f k8s/deepseek-r1-16gpu-multinode.yaml\n",
    "\n",
    "Hardware: 4 nodes Ã— 4 GPUs each = 16 GPUs total (DIFFERENT from above!)\n",
    "Architecture:\n",
    "  - Prefill: 2 pods Ã— 4 GPUs each (TP=8 cross-node)\n",
    "  - Decode:  2 pods Ã— 4 GPUs each (TP=8 cross-node, DP=8, EP=8)\n",
    "  - Requires excellent inter-node networking (InfiniBand/RDMA)\n",
    "  - Only use if you have 4 nodes with 4 GPUs each\n",
    "\n",
    "\n",
    "ğŸ“ For your 2-node Ã— 8 GPU setup, use Example 1!\n",
    "\n",
    "=================================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Expert Parallelism and EPLB\n",
    "\n",
    "When running MoE models with EP and EPLB, monitoring is crucial to ensure optimal performance.\n",
    "\n",
    "#### Key Metrics to Monitor\n",
    "\n",
    "**1. Expert Usage Distribution**\n",
    "```python\n",
    "# SGLang automatically logs expert usage statistics\n",
    "# Look for logs like:\n",
    "# \"Expert usage: [0.05, 0.12, 0.03, 0.15, ...]\"\n",
    "# These show the fraction of tokens routed to each expert\n",
    "```\n",
    "\n",
    "**2. GPU Utilization per Expert**\n",
    "```bash\n",
    "# Use nvidia-smi to check GPU utilization\n",
    "watch -n 1 nvidia-smi\n",
    "\n",
    "# For detailed metrics, use DCGM:\n",
    "dcgmi dmon -e 155,156,203,204 -d 1\n",
    "# 155 = GPU Utilization\n",
    "# 156 = Memory Utilization\n",
    "# 203 = Tensor Core Utilization\n",
    "# 204 = FP16 Activity\n",
    "```\n",
    "\n",
    "**3. EPLB Rebalancing Events**\n",
    "```python\n",
    "# Enable verbose logging to see EPLB rebalancing\n",
    "# Set environment variable: DYNAMO_LOG=debug\n",
    "\n",
    "# Look for logs like:\n",
    "# \"EPLB: Rebalancing experts after 100 iterations\"\n",
    "# \"EPLB: Expert 5 replicated to GPU 2 (high usage: 0.25)\"\n",
    "# \"EPLB: Expert 17 removed from GPU 3 (low usage: 0.01)\"\n",
    "```\n",
    "\n",
    "**4. Network Bandwidth (for Multi-Node)**\n",
    "```bash\n",
    "# Monitor InfiniBand bandwidth\n",
    "ibstat\n",
    "\n",
    "# Monitor network throughput\n",
    "iftop -i ib0  # Replace ib0 with your IB interface\n",
    "```\n",
    "\n",
    "#### Troubleshooting Common Issues\n",
    "\n",
    "**Issue 1: Uneven GPU Utilization**\n",
    "```\n",
    "Symptoms:\n",
    "- Some GPUs at 100%, others at <50%\n",
    "- Throughput lower than expected\n",
    "- Long token generation times\n",
    "\n",
    "Solution:\n",
    "- Enable EPLB: --enable-eplb\n",
    "- Increase redundant experts: --ep-num-redundant-experts 32\n",
    "- Adjust rebalancing frequency: --eplb-rebalance-num-iterations 50\n",
    "```\n",
    "\n",
    "**Issue 2: High Memory Usage**\n",
    "```\n",
    "Symptoms:\n",
    "- OOM errors\n",
    "- Cannot create redundant experts\n",
    "\n",
    "Solution:\n",
    "- Reduce memory fraction: --mem-fraction-static 0.80 (from 0.85)\n",
    "- Reduce redundant experts: --ep-num-redundant-experts 16\n",
    "- Disable features: --disable-radix-cache\n",
    "```\n",
    "\n",
    "**Issue 3: Slow Expert All-to-All Communication**\n",
    "```\n",
    "Symptoms:\n",
    "- High latency during expert routing\n",
    "- Low GPU utilization despite balanced load\n",
    "\n",
    "Solution:\n",
    "- Use DeepEP backend: --moe-a2a-backend deepep\n",
    "- Enable two-batch overlap: --enable-two-batch-overlap\n",
    "- Check network: Ensure InfiniBand is active and configured\n",
    "```\n",
    "\n",
    "**Issue 4: EPLB Not Rebalancing**\n",
    "```\n",
    "Symptoms:\n",
    "- No rebalancing logs\n",
    "- Expert usage remains imbalanced over time\n",
    "\n",
    "Solution:\n",
    "- Enable explicit EPLB: --enable-eplb\n",
    "- Use appropriate recorder mode: --expert-distribution-recorder-mode stat\n",
    "- Lower rebalance threshold: --eplb-rebalance-num-iterations 50\n",
    "```\n",
    "\n",
    "#### Performance Tuning Tips\n",
    "\n",
    "**1. Optimize Memory Allocation**\n",
    "```bash\n",
    "# Start with conservative memory fraction\n",
    "--mem-fraction-static 0.80\n",
    "\n",
    "# Gradually increase if no OOM\n",
    "--mem-fraction-static 0.85\n",
    "\n",
    "# Monitor with nvidia-smi\n",
    "```\n",
    "\n",
    "**2. Tune Redundant Expert Count**\n",
    "```bash\n",
    "# Formula: redundant_experts â‰ˆ num_GPUs / 2 to num_GPUs\n",
    "# For 32 GPUs: try 16-32 redundant experts\n",
    "\n",
    "# Start low\n",
    "--ep-num-redundant-experts 16\n",
    "\n",
    "# Increase if imbalance persists\n",
    "--ep-num-redundant-experts 32\n",
    "```\n",
    "\n",
    "**3. DeepEP Mode Selection**\n",
    "```bash\n",
    "# For prefill (focus on throughput)\n",
    "--deepep-mode normal\n",
    "\n",
    "# For decode (focus on latency)\n",
    "--deepep-mode low_latency\n",
    "```\n",
    "\n",
    "**4. Batch Size Tuning**\n",
    "```bash\n",
    "# For decode, tune CUDA graph batch size\n",
    "# Larger = better throughput, more memory\n",
    "--cuda-graph-bs 128\n",
    "\n",
    "# If OOM, reduce\n",
    "--cuda-graph-bs 64\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 4: Performance Benchmarking\n",
    "\n",
    "Great! You have a working Wide EP deployment. Now let's measure its performance and understand how EPLB improves throughput.\n",
    "\n",
    "**In this section, you'll:**\n",
    "- Learn key metrics for MoE deployments\n",
    "- Benchmark your Wide EP deployment\n",
    "- Understand expert load balancing effectiveness\n",
    "- Compare throughput with and without EPLB\n",
    "\n",
    "### Objectives\n",
    "- Benchmark Expert Parallelism and EPLB performance\n",
    "- Compare single-node vs multi-node deployments\n",
    "- Measure expert load balancing effectiveness\n",
    "- Analyze throughput and latency characteristics\n",
    "\n",
    "### Key Metrics for MoE Models\n",
    "\n",
    "#### 1. **Throughput Metrics**\n",
    "```python\n",
    "# Requests per second across all replicas\n",
    "# Tokens per second (both input and output)\n",
    "# Expert activations per second\n",
    "```\n",
    "\n",
    "#### 2. **Latency Metrics**\n",
    "```python\n",
    "# Time to First Token (TTFT)\n",
    "# Time per Output Token (TPOT)  \n",
    "# Expert routing latency\n",
    "# All-to-all communication time\n",
    "```\n",
    "\n",
    "#### 3. **Load Balancing Metrics**\n",
    "```python\n",
    "# GPU utilization variance (should be low with EPLB)\n",
    "# Expert usage distribution (should be balanced)\n",
    "# EPLB rebalancing frequency\n",
    "# Redundant expert utilization\n",
    "```\n",
    "\n",
    "#### 4. **Resource Utilization**\n",
    "```python\n",
    "# GPU memory usage per worker\n",
    "# Network bandwidth (especially for multi-node)\n",
    "# CPU usage for pre/post-processing\n",
    "```\n",
    "\n",
    "### Benchmarking Exercise 1: Expert Load Distribution\n",
    "\n",
    "**Goal**: Measure how EPLB improves expert load balancing\n",
    "\n",
    "**Setup**:\n",
    "1. Deploy a MoE model WITHOUT EPLB\n",
    "2. Run workload and measure GPU utilization variance\n",
    "3. Enable EPLB and re-run same workload\n",
    "4. Compare results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import statistics\n",
    "\n",
    "def benchmark_deployment(endpoint, num_requests=10):\n",
    "    \"\"\"Benchmark an EP deployment\"\"\"\n",
    "    print(f\"Benchmarking {endpoint}...\")\n",
    "    print(f\"Sending {num_requests} requests...\\n\")\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{endpoint}/v1/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                    \"max_tokens\": 50\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            latency = time.time() - start\n",
    "            latencies.append(latency)\n",
    "            print(f\"Request {i+1}: {latency:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1}: Failed - {e}\")\n",
    "    \n",
    "    if latencies:\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Mean latency: {statistics.mean(latencies):.2f}s\")\n",
    "        print(f\"  Median latency: {statistics.median(latencies):.2f}s\")\n",
    "        print(f\"  Throughput: {num_requests / sum(latencies):.2f} req/s\")\n",
    "\n",
    "# Example usage (uncomment when deployment is running):\n",
    "# benchmark_deployment(\"http://localhost:8000\", num_requests=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- âœ… Wide EP deployments across multiple nodes with SGLang\n",
    "- âœ… Expert Parallelism configuration and EPLB tuning\n",
    "- âœ… Advanced performance measurement and optimization\n",
    "- âœ… Production deployment best practices with Kubernetes\n",
    "- âœ… Building custom Docker images with DeepEP support\n",
    "\n",
    "### Key Takeaways\n",
    "- Wide EP enables datacenter-scale MoE deployments\n",
    "- EPLB significantly improves load balancing and throughput\n",
    "- Multi-node deployments require careful network and resource planning\n",
    "- Custom Docker images are required for DeepEP backend support\n",
    "- SGLang provides flexible deployment and easier experimentation\n",
    "\n",
    "### Performance Improvements with Wide EP\n",
    "Key benefits you can expect:\n",
    "- **SGLang with DeepEP**: Optimized all-to-all communication for expert routing\n",
    "- **EPLB**: Balanced GPU utilization, preventing hotspots\n",
    "- **Multi-Node**: Horizontal scaling with proper network configuration\n",
    "- **Wide EP**: Better resource utilization across large GPU clusters\n",
    "- **Disaggregated Serving**: Separate prefill and decode workers for efficiency\n",
    "\n",
    "### Next Steps\n",
    "- Apply these techniques to your production deployments\n",
    "- Experiment with different configurations for your specific workloads\n",
    "- Contribute optimizations back to the Dynamo community\n",
    "- Explore the latest features in the [Dynamo repository](https://github.com/ai-dynamo/dynamo)\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Dynamo Workshop. You now have the knowledge to:\n",
    "- Deploy Dynamo from local to datacenter scale\n",
    "- Choose the right topology for your use case\n",
    "- Optimize performance with Wide EP and EPLB\n",
    "- Operate production-grade LLM inference infrastructure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
