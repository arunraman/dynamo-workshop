{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Wide EP Production Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will deploy Expert Parallelism concepts from Lab 3.1 into production environments.\n",
    "\n",
    "**Prerequisites**: Complete Lab 3.1 (Expert Parallelism Foundations)\n",
    "\n",
    "**Duration**: 60-75 minutes\n",
    "\n",
    "### What You'll Learn\n",
    "- Deploy Dynamo with Kubernetes and the Dynamo Operator\n",
    "- Configure multi-node SGLang deployments with Wide EP and EPLB\n",
    "- Set up TensorRT-LLM for optimized MoE inference\n",
    "- Monitor, troubleshoot, and tune production deployments\n",
    "- Benchmark and optimize performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "**Getting Started**\n",
    "- [Quick Recap: Lab 3.1 Concepts](#Quick-Recap:-Lab-3.1-Concepts)\n",
    "\n",
    "**Section 1: Concepts**\n",
    "- [Wide EP Deep Dive & MoE Deployment](#Section-1:-Wide-EP-Deep-Dive-&-MoE-Deployment)\n",
    "\n",
    "**Section 2: Infrastructure**\n",
    "- [Kubernetes Deployment with Dynamo Operator](#Section-2:-Kubernetes-Deployment-with-Dynamo-Operator)\n",
    "\n",
    "**Section 3: SGLang Backend**\n",
    "- [Deploying MoE Models with SGLang](#Section-3:-Deploying-MoE-Models-with-SGLang-and-Expert-Parallelism)\n",
    "  - [Configuration Files Overview](#Configuration-Files-Overview)\n",
    "  - [Multi-Node WideEP Deployment](#Example-2:-Multi-Node-WideEP-Deployment-with-DeepSeek-R1)\n",
    "  - [Monitoring Expert Parallelism and EPLB](#Monitoring-Expert-Parallelism-and-EPLB)\n",
    "\n",
    "**Section 4: TensorRT-LLM Backend**\n",
    "- [TensorRT-LLM Wide EP Implementation](#Section-4:-TensorRT-LLM-Wide-EP-Implementation)\n",
    "  - [Architecture Comparison](#Architecture:-TensorRT-LLM-vs-SGLang)\n",
    "  - [Configuration Comparison](#Configuration-Comparison)\n",
    "  - [Hands-On Deployment](#Hands-On:-Deploying-DeepSeek-R1-with-TensorRT-LLM-Wide-EP)\n",
    "  - [Performance Comparison](#Performance-Comparison:-TensorRT-LLM-vs-SGLang)\n",
    "  - [When to Use Each Backend](#When-to-Use-Each)\n",
    "\n",
    "**Section 5: Performance**\n",
    "- [Performance Benchmarking](#Section-5:-Performance-Benchmarking-for-EP-Deployments)\n",
    "\n",
    "**Wrap-Up**\n",
    "- [Summary](#Summary)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Recap: Lab 3.1 Concepts\n",
    "\n",
    "In Lab 3.1, you learned the foundations of Expert Parallelism:\n",
    "\n",
    "**Key Concepts**:\n",
    "- **MoE Models**: Activate only a subset of experts per token (e.g., top-2 out of 256)\n",
    "- **Expert Parallelism (EP)**: Distribute experts across GPUs to scale capacity\n",
    "- **Wide EP**: Horizontal scaling with experts distributed across many nodes\n",
    "- **EPLB**: Dynamic load balancing to prevent expert hotspots\n",
    "- **NVL72**: High-bandwidth interconnect enabling 1.8× throughput gains\n",
    "\n",
    "**EP Variants**:\n",
    "- **Standard EP**: Static expert placement\n",
    "- **Wide EP**: Distributed across clusters for throughput\n",
    "- **Deep EP**: Hierarchical experts for specialization\n",
    "- **Dynamic EP (EPLB)**: Runtime load balancing\n",
    "\n",
    "Now let's deploy these concepts in production!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Wide EP Deep Dive & MoE Deployment\n",
    "\n",
    "### What is Wide EP (Elastic Parallelism)?\n",
    "\n",
    "Wide EP enables **horizontal scaling** of LLM inference by deploying multiple complete model replicas across many nodes. Unlike tensor or pipeline parallelism that split a single model, Wide EP creates independent replicas that can process requests in parallel.\n",
    "\n",
    "### Important: \"Wide EP\" is MoE-Specific Terminology\n",
    "\n",
    "⚠️ **Clarification**: The term \"**Wide EP**\" specifically refers to **Expert Parallelism** for **MoE models**:\n",
    "- **EP** = **Expert Parallelism** (distributing experts across GPUs)\n",
    "- **Wide EP** = Multiple replicas, each using EP internally\n",
    "\n",
    "For **non-MoE models** (like Llama, GPT), there are **no experts**, so:\n",
    "- ✅ Use: \"**Multi-Replica Deployment**\" or \"**Wide DP**\" (Wide Data Parallelism)\n",
    "- ❌ Don't use: \"Wide EP\" (there are no experts to parallelize!)\n",
    "\n",
    "### Multi-Replica Deployment Patterns\n",
    "\n",
    "#### 1. **Multi-Replica Deployment** (Non-MoE Models)\n",
    "For standard dense models without experts:\n",
    "\n",
    "```\n",
    "Load Balancer → Replica 1 (1-8 GPUs)  → Complete Llama-2-7B model\n",
    "              → Replica 2 (1-8 GPUs)  → Complete Llama-2-7B model\n",
    "              → Replica N (1-8 GPUs)  → Complete Llama-2-7B model\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- No experts (dense model)\n",
    "- May use **TP** (Tensor Parallelism) if model is large\n",
    "- May use **PP** (Pipeline Parallelism) for very deep models\n",
    "- Horizontal scaling via replication\n",
    "- **Load Balancer** distributes requests across replicas (infrastructure level)\n",
    "- Examples: Llama-2-7B, Mistral-7B, GPT-3\n",
    "\n",
    "#### 2. **Wide EP Deployment** (MoE Models) ⭐\n",
    "For Mixture-of-Experts models with expert parallelism:\n",
    "\n",
    "```\n",
    "Load Balancer → Replica 1 (8 GPUs)  → DeepSeek-R1 with 256 experts via EP\n",
    "              → Replica 2 (8 GPUs)  → DeepSeek-R1 with 256 experts via EP\n",
    "              → Replica N (8 GPUs)  → DeepSeek-R1 with 256 experts via EP\n",
    "                                      ↑\n",
    "                                      Each replica has a Router (model level)\n",
    "                                      that selects top-K experts per token\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Has experts (MoE architecture)\n",
    "- Uses **EP** (Expert Parallelism) to distribute experts across GPUs\n",
    "- Uses **EPLB** for load balancing experts\n",
    "- Combines horizontal (replicas) + vertical (EP) scaling\n",
    "- **Load Balancer** distributes requests (infrastructure)\n",
    "- **Router** selects experts within each replica (model architecture)\n",
    "- Examples: DeepSeek-R1, Mixtral-8x7B, DeepSeek-V3\n",
    "\n",
    "### Key Advantages for Production\n",
    "\n",
    "1. **Horizontal Throughput Scaling**: Add more replicas = more throughput\n",
    "2. **Fault Tolerance**: Loss of one replica doesn't affect others\n",
    "3. **Load Balancing**: Load balancer distributes requests across healthy replicas\n",
    "4. **Independent Operation**: No inter-replica synchronization needed\n",
    "5. **Geographic Distribution**: Can place replicas in different datacenter zones\n",
    "\n",
    "Let's deploy Wide EP with Dynamo and SGLang!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Kubernetes Deployment with Dynamo Operator\n",
    "\n",
    "### Using Pre-Created Kubernetes Manifests\n",
    "\n",
    "For production deployments, we use the **Dynamo Kubernetes Operator** with `DynamoGraphDeployment` CRDs. \n",
    "\n",
    "All necessary Kubernetes manifests are **pre-created** in the `k8s/` directory:\n",
    "\n",
    "**Available Manifests**:\n",
    "- `k8s/deepseek-r1-wideep.yaml` - SGLang backend with Wide EP\n",
    "- `k8s/deepseek-r1-trtllm.yaml` - TensorRT-LLM backend with Wide EP  \n",
    "- `k8s/README.md` - Detailed deployment guide\n",
    "\n",
    "These manifests use the Dynamo Operator to manage:\n",
    "- Multi-node coordination\n",
    "- Service discovery via etcd\n",
    "- Automatic health checks\n",
    "- Resource allocation\n",
    "- Pod scheduling with anti-affinity\n",
    "\n",
    "### Quick Deployment Steps\n",
    "\n",
    "**Step 1: Install Dynamo Platform** (if not already installed):\n",
    "```bash\n",
    "export NAMESPACE=dynamo-system\n",
    "export RELEASE_VERSION=0.3.2  # Use latest version\n",
    "helm install dynamo-platform nvidia/dynamo-platform \\\n",
    "  --namespace ${NAMESPACE} --create-namespace\n",
    "```\n",
    "\n",
    "**Step 2: Create Secrets**:\n",
    "```bash\n",
    "# Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "  --from-literal=HF_TOKEN='your_hf_token_here' -n dynamo\n",
    "```\n",
    "\n",
    "**Step 3: Deploy DeepSeek-R1 with SGLang**:\n",
    "```bash\n",
    "# Deploy using the pre-created manifest\n",
    "kubectl apply -f k8s/deepseek-r1-wideep.yaml -n dynamo\n",
    "\n",
    "# Monitor deployment\n",
    "kubectl get dynamographdeployment -n dynamo\n",
    "kubectl get pods -n dynamo -w\n",
    "```\n",
    "\n",
    "**Step 4: Test the Deployment**:\n",
    "```bash\n",
    "# Port forward to access the frontend\n",
    "kubectl port-forward svc/deepseek-r1-wideep-frontend 8000:8000 -n dynamo\n",
    "\n",
    "# Test with curl\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain MoE models\"}],\n",
    "    \"max_tokens\": 100\n",
    "  }'\n",
    "```\n",
    "\n",
    "For detailed configuration options and customization, see `k8s/README.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Deploying MoE Models with SGLang and Expert Parallelism\n",
    "\n",
    "Now that you understand how to deploy with Kubernetes, let's dive deeper into hands-on deployment of MoE models with Expert Parallelism using Dynamo's **SGLang backend**.\n",
    "\n",
    "**In this section, you'll learn:**\n",
    "- How to configure SGLang for Expert Parallelism\n",
    "- Single-node vs multi-node deployment strategies\n",
    "- EPLB configuration and tuning\n",
    "- Monitoring and troubleshooting EP deployments\n",
    "\n",
    "### Prerequisites for MoE Deployment\n",
    "\n",
    "**What you need**:\n",
    "- Multiple GPUs (minimum 4 GPUs for this example)\n",
    "- NATS and etcd running (infrastructure from Lab 2)\n",
    "- Model that fits with EP distribution\n",
    "- High-bandwidth interconnect (InfiniBand or NVLink preferred)\n",
    "\n",
    "**Check GPU availability**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Files Overview\n",
    "\n",
    "All configuration files for this lab are pre-created in the `configs/trtllm/` directory:\n",
    "\n",
    "**TensorRT-LLM Configurations**:\n",
    "- `eplb.yaml` - EPLB settings (algorithm, redundant experts, rebalancing)\n",
    "- `wide_ep_prefill.yaml` - Prefill worker configuration\n",
    "- `wide_ep_decode.yaml` - Decode worker configuration  \n",
    "- `wide_ep_agg.yaml` - Aggregated mode (optional)\n",
    "\n",
    "These files are ready to use and include detailed comments explaining each parameter.\n",
    "\n",
    "**SGLang Deployments**: \n",
    "Command examples are provided directly in the cells below for easy copy-paste execution.\n",
    "\n",
    "### Example 1: Single-Node MoE with DP Attention (4 GPUs)\n",
    "\n",
    "This example deploys a MoE model with Expert Parallelism and DP (Data Parallel) attention on a single node with 4 GPUs.\n",
    "\n",
    "**Configuration**:\n",
    "- Model: DeepSeek-R1-Distill-Llama-8B (smaller MoE for learning)\n",
    "- Topology: Disaggregated prefill/decode\n",
    "- Parallelism: TP=1, DP=4, DP attention enabled\n",
    "- Mode: Prefill worker\n",
    "\n",
    "**Why this configuration**:\n",
    "- DP attention allows parallel processing of attention across multiple requests\n",
    "- Expert parallelism distributes experts across the 4 GPUs\n",
    "- Good for learning concepts before scaling to multi-node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available configuration files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Lab 3 Configuration Files\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs_dir = Path(\"configs\")\n",
    "\n",
    "# TensorRT-LLM configs\n",
    "print(\"\\nTensorRT-LLM Configurations (configs/trtllm/):\")\n",
    "print(\"-\" * 60)\n",
    "trtllm_dir = configs_dir / \"trtllm\"\n",
    "if trtllm_dir.exists():\n",
    "    for file in sorted(trtllm_dir.glob(\"*.yaml\")):\n",
    "        size = file.stat().st_size\n",
    "        print(f\"  ✓ {file.name:<30} ({size:>6,} bytes)\")\n",
    "else:\n",
    "    print(\"  ⚠️  Directory not found\")\n",
    "\n",
    "# Kubernetes manifests\n",
    "print(\"\\nKubernetes Manifests (k8s/):\")\n",
    "print(\"-\" * 60)\n",
    "k8s_dir = Path(\"k8s\")\n",
    "if k8s_dir.exists():\n",
    "    for file in sorted(k8s_dir.glob(\"*.yaml\")):\n",
    "        size = file.stat().st_size\n",
    "        print(f\"  ✓ {file.name:<30} ({size:>6,} bytes)\")\n",
    "    if (k8s_dir / \"README.md\").exists():\n",
    "        print(f\"  ✓ README.md (deployment guide)\")\n",
    "else:\n",
    "    print(\"  ⚠️  Directory not found\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multi-Node WideEP Deployment with DeepSeek-R1\n",
    "\n",
    "This is a production-scale deployment of DeepSeek-R1 with WideEP across multiple nodes.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    Load Balancer                     │\n",
    "└──────────────────────┬───────────────────────────────────────┘\n",
    "                       │\n",
    "        ┌──────────────┼──────────────┐\n",
    "        │                             │\n",
    "┌───────▼─────────────┐    ┌──────────▼────────────┐\n",
    "│  Prefill Cluster    │    │  Decode Cluster       │\n",
    "│  (4 nodes)          │    │  (4 nodes)            │\n",
    "│                     │    │                       │\n",
    "│  32 GPUs total      │    │  32 GPUs total        │\n",
    "│  TP=32, DP=32       │───▶│  TP=32, DP=32         │\n",
    "│  + DP Attention     │    │  + DP Attention       │\n",
    "│  + EPLB             │    │  + EPLB               │\n",
    "│  + DeepEP backend   │    │  + DeepEP backend     │\n",
    "└─────────────────────┘    └───────────────────────┘\n",
    "```\n",
    "\n",
    "**Configuration Details**:\n",
    "\n",
    "**Prefill Workers** (4 nodes × 8 GPUs = 32 GPUs):\n",
    "- Model: DeepSeek-R1 (671B parameters, 256 experts)\n",
    "- TP Size: 32 (tensor parallelism across 32 GPUs)\n",
    "- DP Size: 32 (data parallelism)\n",
    "- DP Attention: Enabled for efficient attention computation\n",
    "- Expert Parallelism: Experts distributed across GPUs\n",
    "- EPLB: 32 redundant experts with dynamic load balancing\n",
    "- MoE Backend: DeepEP (high-performance all-to-all)\n",
    "- KV Transfer: NIXL (RDMA-based transfer to decode)\n",
    "\n",
    "**Decode Workers** (4 nodes × 8 GPUs = 32 GPUs):\n",
    "- Same model and parallelism configuration\n",
    "- Optimized for decode phase with CUDA graphs\n",
    "- Receives KV cache from prefill via NIXL\n",
    "\n",
    "**Key Parameters**:\n",
    "```bash\n",
    "# Common parameters\n",
    "--model-path deepseek-ai/DeepSeek-R1\n",
    "--tp-size 32\n",
    "--dp-size 32\n",
    "--enable-dp-attention\n",
    "--trust-remote-code\n",
    "\n",
    "# Expert parallelism parameters\n",
    "--ep-num-redundant-experts 32       # Create 32 additional expert copies\n",
    "--eplb-algorithm deepseek            # Use DeepSeek's EPLB algorithm\n",
    "--moe-a2a-backend deepep             # Use DeepEP for expert communication\n",
    "--moe-dense-tp-size 1                # TP size for dense layers\n",
    "--enable-dp-lm-head                  # Enable DP for LM head\n",
    "\n",
    "# Memory and performance\n",
    "--mem-fraction-static 0.85           # Reserve 85% GPU memory\n",
    "--page-size 1                        # KV cache page size\n",
    "--disable-radix-cache                # Disable radix cache for disagg\n",
    "--enable-two-batch-overlap           # Overlap computation\n",
    "--watchdog-timeout 1000000           # Long timeout for large model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Node DeepSeek-R1 WideEP Deployment with SGLang\n",
    "# This example shows the full deployment command for reference\n",
    "# In production, use the Kubernetes manifests in k8s/\n",
    "\n",
    "print(\"\"\"\n",
    "=================================================================\n",
    "Multi-Node DeepSeek-R1 Deployment Example\n",
    "=================================================================\n",
    "\n",
    "This deployment uses:\n",
    "- 4 prefill nodes × 8 GPUs = 32 GPUs\n",
    "- 4 decode nodes × 8 GPUs = 32 GPUs  \n",
    "- TP=32, DP=32, DP Attention\n",
    "- DeepEP backend with EPLB\n",
    "- NIXL for KV transfer\n",
    "\n",
    "For production, use: kubectl apply -f k8s/deepseek-r1-wideep.yaml\n",
    "\n",
    "Command reference (for understanding the configuration):\n",
    "\"\"\")\n",
    "\n",
    "# Prefill worker command (runs on 4 nodes)\n",
    "prefill_cmd = \"\"\"python -m dynamo.sglang \\\n",
    "  --model-path /path/to/DeepSeek-R1 \\\n",
    "  --served-model-name deepseek-ai/DeepSeek-R1 \\\n",
    "  --skip-tokenizer-init \\\n",
    "  --disaggregation-mode prefill \\\n",
    "  --disaggregation-transfer-backend nixl \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --disaggregation-bootstrap-port 30001 \\\n",
    "  --tp-size 32 \\\n",
    "  --dp-size 32 \\\n",
    "  --enable-dp-attention \\\n",
    "  --moe-a2a-backend deepep \\\n",
    "  --ep-num-redundant-experts 32 \\\n",
    "  --eplb-algorithm deepseek \\\n",
    "  --trust-remote-code\"\"\"\n",
    "\n",
    "print(\"\\nPrefill Worker Command:\")\n",
    "print(prefill_cmd)\n",
    "\n",
    "print(\"\\n✅ See k8s/deepseek-r1-wideep.yaml for full Kubernetes deployment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Expert Parallelism and EPLB\n",
    "\n",
    "When running MoE models with EP and EPLB, monitoring is crucial to ensure optimal performance.\n",
    "\n",
    "#### Key Metrics to Monitor\n",
    "\n",
    "**1. Expert Usage Distribution**\n",
    "```python\n",
    "# SGLang automatically logs expert usage statistics\n",
    "# Look for logs like:\n",
    "# \"Expert usage: [0.05, 0.12, 0.03, 0.15, ...]\"\n",
    "# These show the fraction of tokens routed to each expert\n",
    "```\n",
    "\n",
    "**2. GPU Utilization per Expert**\n",
    "```bash\n",
    "# Use nvidia-smi to check GPU utilization\n",
    "watch -n 1 nvidia-smi\n",
    "\n",
    "# For detailed metrics, use DCGM:\n",
    "dcgmi dmon -e 155,156,203,204 -d 1\n",
    "# 155 = GPU Utilization\n",
    "# 156 = Memory Utilization\n",
    "# 203 = Tensor Core Utilization\n",
    "# 204 = FP16 Activity\n",
    "```\n",
    "\n",
    "**3. EPLB Rebalancing Events**\n",
    "```python\n",
    "# Enable verbose logging to see EPLB rebalancing\n",
    "# Set environment variable: DYNAMO_LOG=debug\n",
    "\n",
    "# Look for logs like:\n",
    "# \"EPLB: Rebalancing experts after 100 iterations\"\n",
    "# \"EPLB: Expert 5 replicated to GPU 2 (high usage: 0.25)\"\n",
    "# \"EPLB: Expert 17 removed from GPU 3 (low usage: 0.01)\"\n",
    "```\n",
    "\n",
    "**4. Network Bandwidth (for Multi-Node)**\n",
    "```bash\n",
    "# Monitor InfiniBand bandwidth\n",
    "ibstat\n",
    "\n",
    "# Monitor network throughput\n",
    "iftop -i ib0  # Replace ib0 with your IB interface\n",
    "```\n",
    "\n",
    "#### Troubleshooting Common Issues\n",
    "\n",
    "**Issue 1: Uneven GPU Utilization**\n",
    "```\n",
    "Symptoms:\n",
    "- Some GPUs at 100%, others at <50%\n",
    "- Throughput lower than expected\n",
    "- Long token generation times\n",
    "\n",
    "Solution:\n",
    "- Enable EPLB: --enable-eplb\n",
    "- Increase redundant experts: --ep-num-redundant-experts 32\n",
    "- Adjust rebalancing frequency: --eplb-rebalance-num-iterations 50\n",
    "```\n",
    "\n",
    "**Issue 2: High Memory Usage**\n",
    "```\n",
    "Symptoms:\n",
    "- OOM errors\n",
    "- Cannot create redundant experts\n",
    "\n",
    "Solution:\n",
    "- Reduce memory fraction: --mem-fraction-static 0.80 (from 0.85)\n",
    "- Reduce redundant experts: --ep-num-redundant-experts 16\n",
    "- Disable features: --disable-radix-cache\n",
    "```\n",
    "\n",
    "**Issue 3: Slow Expert All-to-All Communication**\n",
    "```\n",
    "Symptoms:\n",
    "- High latency during expert routing\n",
    "- Low GPU utilization despite balanced load\n",
    "\n",
    "Solution:\n",
    "- Use DeepEP backend: --moe-a2a-backend deepep\n",
    "- Enable two-batch overlap: --enable-two-batch-overlap\n",
    "- Check network: Ensure InfiniBand is active and configured\n",
    "```\n",
    "\n",
    "**Issue 4: EPLB Not Rebalancing**\n",
    "```\n",
    "Symptoms:\n",
    "- No rebalancing logs\n",
    "- Expert usage remains imbalanced over time\n",
    "\n",
    "Solution:\n",
    "- Enable explicit EPLB: --enable-eplb\n",
    "- Use appropriate recorder mode: --expert-distribution-recorder-mode stat\n",
    "- Lower rebalance threshold: --eplb-rebalance-num-iterations 50\n",
    "```\n",
    "\n",
    "#### Performance Tuning Tips\n",
    "\n",
    "**1. Optimize Memory Allocation**\n",
    "```bash\n",
    "# Start with conservative memory fraction\n",
    "--mem-fraction-static 0.80\n",
    "\n",
    "# Gradually increase if no OOM\n",
    "--mem-fraction-static 0.85\n",
    "\n",
    "# Monitor with nvidia-smi\n",
    "```\n",
    "\n",
    "**2. Tune Redundant Expert Count**\n",
    "```bash\n",
    "# Formula: redundant_experts ≈ num_GPUs / 2 to num_GPUs\n",
    "# For 32 GPUs: try 16-32 redundant experts\n",
    "\n",
    "# Start low\n",
    "--ep-num-redundant-experts 16\n",
    "\n",
    "# Increase if imbalance persists\n",
    "--ep-num-redundant-experts 32\n",
    "```\n",
    "\n",
    "**3. DeepEP Mode Selection**\n",
    "```bash\n",
    "# For prefill (focus on throughput)\n",
    "--deepep-mode normal\n",
    "\n",
    "# For decode (focus on latency)\n",
    "--deepep-mode low_latency\n",
    "```\n",
    "\n",
    "**4. Batch Size Tuning**\n",
    "```bash\n",
    "# For decode, tune CUDA graph batch size\n",
    "# Larger = better throughput, more memory\n",
    "--cuda-graph-bs 128\n",
    "\n",
    "# If OOM, reduce\n",
    "--cuda-graph-bs 64\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: TensorRT-LLM Wide EP Implementation\n",
    "\n",
    "You've learned how to deploy Wide EP with SGLang. Now let's explore **TensorRT-LLM**, NVIDIA's highly optimized inference engine.\n",
    "\n",
    "NVIDIA's [TensorRT-LLM also supports Wide EP](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/wide_ep) with its own optimizations, offering maximum performance for production deployments.\n",
    "\n",
    "**In this section, you'll learn:**\n",
    "- How TensorRT-LLM differs from SGLang\n",
    "- YAML-based configuration approach\n",
    "- FP8 quantization and CUDA graph optimizations\n",
    "- When to choose TensorRT-LLM vs SGLang\n",
    "\n",
    "### TensorRT-LLM Wide EP Overview\n",
    "\n",
    "TensorRT-LLM (TRT-LLM) provides a highly optimized inference engine with native Wide EP support for MoE models. It's particularly well-suited for production deployments requiring maximum performance.\n",
    "\n",
    "**Key Features**:\n",
    "- Built-in WIDEEP backend for expert parallelism\n",
    "- Integrated EPLB (Expert Parallelism Load Balancer)\n",
    "- Highly optimized CUDA kernels\n",
    "- FP8 quantization support for MoE layers\n",
    "- CUDA graphs for decode optimization\n",
    "\n",
    "### Architecture: TensorRT-LLM vs SGLang\n",
    "\n",
    "**TensorRT-LLM Approach**:\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│          TensorRT-LLM Wide EP Architecture           │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "        ┌─────────────────┼─────────────────┐\n",
    "        │                                   │\n",
    "   ┌────▼─────┐                       ┌────▼─────┐\n",
    "   │ Prefill  │                       │  Decode  │\n",
    "   │ Workers  │ ─────KV Transfer────▶ │ Workers  │\n",
    "   │          │                       │          │\n",
    "   │ - TP=16  │                       │ - TP=16  │\n",
    "   │ - EP=16  │                       │ - EP=16  │\n",
    "   │ - WIDEEP │                       │ - WIDEEP │\n",
    "   │ - EPLB   │                       │ - EPLB   │\n",
    "   │ - FP8 KV │                       │ - FP8 KV │\n",
    "   └──────────┘                       └──────────┘\n",
    "```\n",
    "\n",
    "**SGLang Approach**:\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│            SGLang Wide EP Architecture               │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "        ┌─────────────────┼─────────────────┐\n",
    "        │                                   │\n",
    "   ┌────▼─────┐                       ┌────▼─────┐\n",
    "   │ Prefill  │                       │  Decode  │\n",
    "   │ Workers  │ ──────NIXL/────────▶  │ Workers  │\n",
    "   │          │    Mooncake           │          │\n",
    "   │ - TP=32  │                       │ - TP=32  │\n",
    "   │ - DP=32  │                       │ - DP=32  │\n",
    "   │ - DeepEP │                       │ - DeepEP │\n",
    "   │ - EPLB   │                       │ - EPLB   │\n",
    "   └──────────┘                       └──────────┘\n",
    "```\n",
    "\n",
    "### Configuration Comparison\n",
    "\n",
    "#### TensorRT-LLM Configuration (YAML-based)\n",
    "\n",
    "**Prefill Worker** (`wide_ep_prefill.yaml`):\n",
    "```yaml\n",
    "backend: pytorch\n",
    "\n",
    "# WideEP settings\n",
    "moe_config:\n",
    "  backend: WIDEEP\n",
    "  load_balancer: /path/to/eplb.yaml\n",
    "\n",
    "# Parallelism\n",
    "tensor_parallel_size: 16\n",
    "moe_expert_parallel_size: 16\n",
    "pipeline_parallel_size: 1\n",
    "enable_attention_dp: true\n",
    "\n",
    "# Batch and sequence settings\n",
    "max_batch_size: 256\n",
    "max_num_tokens: 256\n",
    "max_seq_len: 8448\n",
    "\n",
    "# KV cache with FP8\n",
    "kv_cache_config:\n",
    "  free_gpu_memory_fraction: 0.30\n",
    "  dtype: fp8\n",
    "\n",
    "# CUDA graphs\n",
    "cuda_graph_config:\n",
    "  enable_padding: true\n",
    "  batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "```\n",
    "\n",
    "**Decode Worker** (`wide_ep_decode.yaml`):\n",
    "```yaml\n",
    "backend: pytorch\n",
    "\n",
    "moe_config:\n",
    "  backend: WIDEEP\n",
    "  load_balancer: /path/to/eplb.yaml\n",
    "\n",
    "tensor_parallel_size: 16\n",
    "moe_expert_parallel_size: 16\n",
    "enable_attention_dp: true\n",
    "\n",
    "max_batch_size: 256\n",
    "max_seq_len: 8448\n",
    "\n",
    "kv_cache_config:\n",
    "  free_gpu_memory_fraction: 0.30\n",
    "  dtype: fp8\n",
    "\n",
    "cuda_graph_config:\n",
    "  enable_padding: true\n",
    "  batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "```\n",
    "\n",
    "#### SGLang Configuration (Command-line based)\n",
    "\n",
    "**Prefill Worker**:\n",
    "```bash\n",
    "python -m dynamo.sglang \\\n",
    "  --model-path deepseek-ai/DeepSeek-R1 \\\n",
    "  --tp-size 32 \\\n",
    "  --dp-size 32 \\\n",
    "  --enable-dp-attention \\\n",
    "  --moe-a2a-backend deepep \\\n",
    "  --ep-num-redundant-experts 32 \\\n",
    "  --eplb-algorithm deepseek \\\n",
    "  --disaggregation-mode prefill\n",
    "```\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | TensorRT-LLM | SGLang |\n",
    "|--------|--------------|--------|\n",
    "| **Configuration** | YAML files | Command-line arguments |\n",
    "| **MoE Backend** | WIDEEP (built-in) | DeepEP (external library) |\n",
    "| **KV Cache** | FP8 quantization native | FP16/BF16 default |\n",
    "| **CUDA Graphs** | Explicit batch size list | Auto-generated |\n",
    "| **Optimization** | Highly tuned kernels | Flexible, easier to customize |\n",
    "| **Deployment** | More structured | More flexible |\n",
    "| **Memory Management** | `free_gpu_memory_fraction` | `mem_fraction_static` |\n",
    "\n",
    "### EPLB Configuration (TensorRT-LLM)\n",
    "\n",
    "TensorRT-LLM uses a separate YAML file for EPLB configuration:\n",
    "\n",
    "**`eplb.yaml`**:\n",
    "```yaml\n",
    "# Expert Parallelism Load Balancer\n",
    "algorithm: deepseek  # or hierarchical, global\n",
    "redundant_experts: 32\n",
    "rebalance_frequency: 100\n",
    "usage_recorder_mode: stat\n",
    "buffer_size: 10\n",
    "```\n",
    "\n",
    "This modular approach allows easy tuning without restarting workers.\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "**TensorRT-LLM Advantages**:\n",
    "- ✅ FP8 KV cache reduces memory by ~50%\n",
    "- ✅ Highly optimized CUDA kernels\n",
    "- ✅ Better CUDA graph integration\n",
    "- ✅ Lower latency for decode phase\n",
    "- ✅ Built-in profiling and optimization tools\n",
    "\n",
    "**SGLang Advantages**:\n",
    "- ✅ More flexible configuration\n",
    "- ✅ Easier to experiment and customize\n",
    "- ✅ Better support for diverse models\n",
    "- ✅ Active development and community\n",
    "- ✅ Simpler deployment workflow\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use TensorRT-LLM Wide EP when**:\n",
    "- Maximum performance is critical\n",
    "- You have NVIDIA GPUs (H100, H200, GB200)\n",
    "- Production deployment with stable configuration\n",
    "- Need FP8 quantization\n",
    "- Willing to invest in optimization\n",
    "\n",
    "**Use SGLang Wide EP when**:\n",
    "- Rapid experimentation needed\n",
    "- Diverse model support required\n",
    "- Flexible deployment patterns\n",
    "- Active development workflow\n",
    "- Community support important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On: Deploying DeepSeek-R1 with TensorRT-LLM Wide EP\n",
    "\n",
    "Here's a complete example of deploying DeepSeek-R1 using TensorRT-LLM's Wide EP backend with Dynamo.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. TensorRT-LLM installed (version with Wide EP support)\n",
    "2. DeepSeek-R1 model converted to TensorRT-LLM engine format\n",
    "3. Multi-node cluster with H100/H200 GPUs\n",
    "4. Dynamo with TensorRT-LLM backend support\n",
    "\n",
    "**Configuration Files Provided**\n",
    "\n",
    "All necessary configuration files are pre-created in `configs/trtllm/`:\n",
    "\n",
    "- **`eplb.yaml`** - EPLB configuration (algorithm, redundant experts, rebalancing)\n",
    "- **`wide_ep_prefill.yaml`** - Prefill worker config (TP=16, EP=16, FP8 KV cache)\n",
    "- **`wide_ep_decode.yaml`** - Decode worker config (CUDA graphs, FP8 KV cache)\n",
    "- **`wide_ep_agg.yaml`** - Aggregated mode config (optional, for non-disaggregated deployment)\n",
    "\n",
    "You can view and customize these files in the `configs/trtllm/` directory.\n",
    "\n",
    "**Key Configuration Highlights**:\n",
    "\n",
    "From `eplb.yaml`:\n",
    "- Algorithm: `deepseek` (or hierarchical, global)\n",
    "- Redundant experts: `32` (adjust based on GPU count)\n",
    "- Rebalancing frequency: `100` iterations\n",
    "\n",
    "From `wide_ep_prefill.yaml` and `wide_ep_decode.yaml`:\n",
    "- Tensor Parallelism: `16` GPUs\n",
    "- Expert Parallelism: `16` GPUs\n",
    "- FP8 KV cache for 50% memory savings\n",
    "- DP attention enabled\n",
    "- Max batch size: `256`\n",
    "\n",
    "**Step 1: Start Infrastructure**\n",
    "```bash\n",
    "# Start NATS and etcd\n",
    "docker compose -f deploy/docker-compose.yml up -d\n",
    "\n",
    "# Start Dynamo frontend\n",
    "python -m dynamo.frontend --http-port=8000 &\n",
    "```\n",
    "\n",
    "**Step 2: Launch Prefill Workers** (2 nodes × 8 GPUs = 16 GPUs)\n",
    "```bash\n",
    "# On Prefill Node 0:\n",
    "python -m dynamo.trtllm \\\n",
    "  --engine-dir /path/to/deepseek-r1-engine \\\n",
    "  --config-path ./configs/trtllm/wide_ep_prefill.yaml \\\n",
    "  --disaggregation-mode prefill \\\n",
    "  --dist-init-addr ${PREFILL_HEAD_IP}:29500 \\\n",
    "  --nnodes 2 \\\n",
    "  --node-rank 0\n",
    "\n",
    "# On Prefill Node 1:\n",
    "# Same command with --node-rank 1\n",
    "```\n",
    "\n",
    "**Step 3: Launch Decode Workers** (2 nodes × 8 GPUs = 16 GPUs)\n",
    "```bash\n",
    "# On Decode Node 0:\n",
    "python -m dynamo.trtllm \\\n",
    "  --engine-dir /path/to/deepseek-r1-engine \\\n",
    "  --config-path ./configs/trtllm/wide_ep_decode.yaml \\\n",
    "  --disaggregation-mode decode \\\n",
    "  --dist-init-addr ${DECODE_HEAD_IP}:29500 \\\n",
    "  --nnodes 2 \\\n",
    "  --node-rank 0\n",
    "\n",
    "# On Decode Node 1:\n",
    "# Same command with --node-rank 1\n",
    "```\n",
    "\n",
    "**Step 4: Test the Deployment**\n",
    "```bash\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"Explain quantum entanglement\"}\n",
    "    ],\n",
    "    \"max_tokens\": 200\n",
    "  }'\n",
    "```\n",
    "\n",
    "### TensorRT-LLM Specific Optimizations\n",
    "\n",
    "**1. FP8 Quantization for KV Cache**\n",
    "\n",
    "TensorRT-LLM's FP8 KV cache provides ~50% memory savings:\n",
    "\n",
    "```yaml\n",
    "kv_cache_config:\n",
    "  dtype: fp8  # Instead of fp16/bf16\n",
    "  free_gpu_memory_fraction: 0.30\n",
    "```\n",
    "\n",
    "Memory comparison for DeepSeek-R1:\n",
    "- FP16 KV cache: ~280GB across 16 GPUs\n",
    "- FP8 KV cache: ~140GB across 16 GPUs\n",
    "- **Savings**: 50% memory, enabling higher batch sizes\n",
    "\n",
    "**2. CUDA Graph Optimization**\n",
    "\n",
    "Explicitly specify batch sizes for CUDA graph compilation:\n",
    "\n",
    "```yaml\n",
    "cuda_graph_config:\n",
    "  enable_padding: true\n",
    "  batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- Pre-compiled kernels for common batch sizes\n",
    "- Lower decode latency (5-10ms improvement)\n",
    "- Better GPU utilization\n",
    "\n",
    "**3. DP Attention Tuning**\n",
    "\n",
    "Balance memory usage with attention DP:\n",
    "\n",
    "```yaml\n",
    "enable_attention_dp: true\n",
    "kv_cache_config:\n",
    "  # Lower fraction when using DP attention\n",
    "  free_gpu_memory_fraction: 0.30  # vs 0.85 without DP\n",
    "```\n",
    "\n",
    "**4. Expert-Specific Configurations**\n",
    "\n",
    "```yaml\n",
    "moe_config:\n",
    "  backend: WIDEEP\n",
    "  # Maximum tokens processed by MoE layers\n",
    "  moe_max_num_tokens: 4096  # = max_batch_size * EP size\n",
    "```\n",
    "\n",
    "### Performance Comparison: TensorRT-LLM vs SGLang\n",
    "\n",
    "**DeepSeek-R1 on 32 H100 GPUs (16 prefill + 16 decode)**:\n",
    "\n",
    "| Metric | TensorRT-LLM | SGLang | Improvement |\n",
    "|--------|--------------|--------|-------------|\n",
    "| TTFT (median) | 280ms | 450ms | **38% faster** |\n",
    "| TPOT (median) | 15ms | 22ms | **32% faster** |\n",
    "| Throughput | 12,000 tok/s | 10,000 tok/s | **20% higher** |\n",
    "| GPU Memory | 140GB (FP8) | 280GB (FP16) | **50% less** |\n",
    "| Max Batch Size | 256 | 128 | **2x larger** |\n",
    "\n",
    "*Note: Results vary based on workload, hardware, and configuration*\n",
    "\n",
    "### Troubleshooting TensorRT-LLM Wide EP\n",
    "\n",
    "**Issue 1: FP8 Accuracy Degradation**\n",
    "```\n",
    "Symptom: Lower quality outputs with FP8 KV cache\n",
    "Solution: \n",
    "- Use FP16 for critical applications\n",
    "- Enable FP8 only for decode phase\n",
    "- Tune quantization scales\n",
    "```\n",
    "\n",
    "**Issue 2: CUDA Graph OOM**\n",
    "```\n",
    "Symptom: Out of memory during CUDA graph capture\n",
    "Solution:\n",
    "- Reduce cuda_graph_config.batch_sizes list\n",
    "- Increase free_gpu_memory_fraction slightly\n",
    "- Disable padding: enable_padding: false\n",
    "```\n",
    "\n",
    "**Issue 3: WIDEEP Backend Not Found**\n",
    "```\n",
    "Symptom: \"WIDEEP backend not available\" error\n",
    "Solution:\n",
    "- Ensure TensorRT-LLM built with Wide EP support\n",
    "- Check library paths for DeepEP/DeepGEMM\n",
    "- Verify GPU architecture support (Hopper recommended)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Performance Benchmarking for EP Deployments\n",
    "\n",
    "Now that you've deployed Wide EP with both SGLang and TensorRT-LLM, let's learn how to **measure and optimize performance**.\n",
    "\n",
    "**In this section, you'll learn:**\n",
    "- Key metrics for MoE model deployments\n",
    "- How to benchmark Expert Parallelism and EPLB\n",
    "- Comparing single-node vs multi-node performance\n",
    "- Measuring expert load balancing effectiveness\n",
    "\n",
    "### Objectives\n",
    "- Benchmark Expert Parallelism and EPLB performance\n",
    "- Compare single-node vs multi-node deployments\n",
    "- Measure expert load balancing effectiveness\n",
    "- Analyze throughput and latency characteristics\n",
    "\n",
    "### Key Metrics for MoE Models\n",
    "\n",
    "#### 1. **Throughput Metrics**\n",
    "```python\n",
    "# Requests per second across all replicas\n",
    "# Tokens per second (both input and output)\n",
    "# Expert activations per second\n",
    "```\n",
    "\n",
    "#### 2. **Latency Metrics**\n",
    "```python\n",
    "# Time to First Token (TTFT)\n",
    "# Time per Output Token (TPOT)  \n",
    "# Expert routing latency\n",
    "# All-to-all communication time\n",
    "```\n",
    "\n",
    "#### 3. **Load Balancing Metrics**\n",
    "```python\n",
    "# GPU utilization variance (should be low with EPLB)\n",
    "# Expert usage distribution (should be balanced)\n",
    "# EPLB rebalancing frequency\n",
    "# Redundant expert utilization\n",
    "```\n",
    "\n",
    "#### 4. **Resource Utilization**\n",
    "```python\n",
    "# GPU memory usage per worker\n",
    "# Network bandwidth (especially for multi-node)\n",
    "# CPU usage for pre/post-processing\n",
    "```\n",
    "\n",
    "### Benchmarking Exercise 1: Expert Load Distribution\n",
    "\n",
    "**Goal**: Measure how EPLB improves expert load balancing\n",
    "\n",
    "**Setup**:\n",
    "1. Deploy a MoE model WITHOUT EPLB\n",
    "2. Run workload and measure GPU utilization variance\n",
    "3. Enable EPLB and re-run same workload\n",
    "4. Compare results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import statistics\n",
    "\n",
    "def benchmark_deployment(endpoint, num_requests=10):\n",
    "    \"\"\"Benchmark an EP deployment\"\"\"\n",
    "    print(f\"Benchmarking {endpoint}...\")\n",
    "    print(f\"Sending {num_requests} requests...\\n\")\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{endpoint}/v1/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                    \"max_tokens\": 50\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            latency = time.time() - start\n",
    "            latencies.append(latency)\n",
    "            print(f\"Request {i+1}: {latency:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1}: Failed - {e}\")\n",
    "    \n",
    "    if latencies:\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Mean latency: {statistics.mean(latencies):.2f}s\")\n",
    "        print(f\"  Median latency: {statistics.median(latencies):.2f}s\")\n",
    "        print(f\"  Throughput: {num_requests / sum(latencies):.2f} req/s\")\n",
    "\n",
    "# Example usage (uncomment when deployment is running):\n",
    "# benchmark_deployment(\"http://localhost:8000\", num_requests=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- ✅ Wide EP deployments across multiple nodes\n",
    "- ✅ KVBM architecture and configuration\n",
    "- ✅ Advanced performance measurement and optimization\n",
    "- ✅ Production deployment best practices\n",
    "- ✅ Cache management and bandwidth optimization\n",
    "\n",
    "### Key Takeaways\n",
    "- Wide EP enables datacenter-scale deployments\n",
    "- EPLB significantly improves load balancing and throughput\n",
    "- Multi-node deployments require careful network and resource planning\n",
    "- Production deployments need comprehensive monitoring and HA configuration\n",
    "- Different cache policies suit different workload patterns\n",
    "\n",
    "### Performance Improvements with Wide EP\n",
    "Typical improvements observed:\n",
    "- 30-50% reduction in GPU memory usage\n",
    "- 20-40% increase in throughput for cache-friendly workloads\n",
    "- Reduced TTFT for cached prefixes\n",
    "- Better resource utilization across the cluster\n",
    "\n",
    "### Next Steps\n",
    "- Apply these techniques to your production deployments\n",
    "- Experiment with different configurations for your specific workloads\n",
    "- Contribute optimizations back to the Dynamo community\n",
    "- Explore the latest features in the [Dynamo repository](https://github.com/ai-dynamo/dynamo)\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Dynamo Workshop. You now have the knowledge to:\n",
    "- Deploy Dynamo from local to datacenter scale\n",
    "- Choose the right topology for your use case\n",
    "- Optimize performance with Wide EP and EPLB\n",
    "- Operate production-grade LLM inference infrastructure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
