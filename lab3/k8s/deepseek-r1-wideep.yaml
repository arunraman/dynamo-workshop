# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# DeepSeek-R1 WideEP Deployment with Expert Parallelism
# This manifest deploys DeepSeek-R1 with disaggregated serving and EPLB

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: ds-r1-ep
  namespace: dynamo-workshop
spec:
  envs:
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: hf-token-secret
        key: HF_TOKEN
  - name: GLOO_SOCKET_IFNAME
    value: "eth0"
  
  backendFramework: sglang
  
  services:
    Frontend:
      dynamoNamespace: ds-r1-ep
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          
    PrefillWorker:
      multinode:
        nodeCount: 4  # 4 nodes × 8 GPUs = 32 GPUs for prefill
      envFromSecret: hf-token-secret
      dynamoNamespace: ds-r1-ep
      componentType: worker
      subComponentType: prefill
      replicas: 1
      resources:
        limits:
          gpu: "8"  # 8 GPUs per node
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          workingDir: /workspace
          command:
          - python3
          - -m
          - dynamo.sglang
          args:
            # Model configuration
            - --model-path
            - deepseek-ai/DeepSeek-R1
            - --served-model-name
            - deepseek-ai/DeepSeek-R1
            - --trust-remote-code
            - --skip-tokenizer-init
            
            # Disaggregation configuration
            - --disaggregation-mode
            - prefill
            - --disaggregation-transfer-backend
            - nixl
            - --disaggregation-bootstrap-port
            - "30001"
            - --host
            - "0.0.0.0"
            
            # Parallelism configuration
            - --tp-size
            - "32"  # Tensor parallelism across 32 GPUs
            - --dp-size
            - "32"  # Data parallelism
            - --enable-dp-attention
            
            # Expert Parallelism and EPLB
            - --moe-a2a-backend
            - deepep
            - --ep-num-redundant-experts
            - "32"
            - --eplb-algorithm
            - deepseek
            - --moe-dense-tp-size
            - "1"
            - --enable-dp-lm-head
            
            # Performance tuning
            - --load-balance-method
            - round_robin
            - --page-size
            - "1"
            - --disable-radix-cache
            - --watchdog-timeout
            - "1000000"
            - --enable-two-batch-overlap
            - --deepep-mode
            - normal
            - --mem-fraction-static
            - "0.85"
            - --decode-log-interval
            - "1000"
    
    DecodeWorker:
      multinode:
        nodeCount: 4  # 4 nodes × 8 GPUs = 32 GPUs for decode
      envFromSecret: hf-token-secret
      dynamoNamespace: ds-r1-ep
      componentType: worker
      subComponentType: decode
      replicas: 1
      resources:
        limits:
          gpu: "8"  # 8 GPUs per node
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          workingDir: /workspace
          command:
          - python3
          - -m
          - dynamo.sglang
          args:
            # Model configuration
            - --model-path
            - deepseek-ai/DeepSeek-R1
            - --served-model-name
            - deepseek-ai/DeepSeek-R1
            - --trust-remote-code
            - --skip-tokenizer-init
            
            # Disaggregation configuration
            - --disaggregation-mode
            - decode
            - --disaggregation-transfer-backend
            - nixl
            - --disaggregation-bootstrap-port
            - "30001"
            - --host
            - "0.0.0.0"
            
            # Parallelism configuration
            - --tp-size
            - "32"
            - --dp-size
            - "32"
            - --enable-dp-attention
            
            # Expert Parallelism and EPLB
            - --moe-a2a-backend
            - deepep
            - --ep-num-redundant-experts
            - "32"
            - --moe-dense-tp-size
            - "1"
            - --enable-dp-lm-head
            
            # Performance tuning (decode-optimized)
            - --prefill-round-robin-balance
            - --page-size
            - "1"
            - --disable-radix-cache
            - --watchdog-timeout
            - "1000000"
            - --enable-two-batch-overlap
            - --deepep-mode
            - low_latency
            - --mem-fraction-static
            - "0.835"
            - --cuda-graph-bs
            - "128"
            - --decode-log-interval
            - "1000"

