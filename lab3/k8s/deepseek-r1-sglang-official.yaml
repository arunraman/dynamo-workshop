# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# DeepSeek-R1 Wide EP Deployment - Based on Official Recipe
# Adapted from: dynamo-repo/recipes/deepseek-r1/sglang/disagg-16gpu/deploy.yaml
# Scaled to 32 GPUs (4 nodes × 8 GPUs each)

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: ds-r1-ep
  namespace: dynamo-workshop
spec:
  # Note: This requires building the sglang-wideep container first
  # See: dynamo-repo/container/Dockerfile.sglang-wideep
  
  services:
    Frontend:
      dynamoNamespace: ds-r1-ep
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 10
            timeoutSeconds: 1800
            failureThreshold: 60
          # TODO: Replace with your custom built image
          # image: your-registry/sglang-wideep-runtime:0.6.0
          image: nvcr.io/nvidia/ai-dynamo/dynamo-sglang:0.6.0
          
    decode:
      dynamoNamespace: ds-r1-ep
      componentType: worker
      replicas: 1
      multinode:
        nodeCount: 4  # 4 nodes × 8 GPUs = 32 GPUs
      resources:
        limits:
          gpu: "8"
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 600
          # TODO: Replace with your custom built image
          # image: your-registry/sglang-wideep-runtime:0.6.0
          image: nvcr.io/nvidia/ai-dynamo/dynamo-sglang:0.6.0
          workingDir: /workspace/components/backends/sglang
          command:
            - python3
            - -m
            - dynamo.sglang
          args:
            # Model configuration
            - --model-path
            - deepseek-ai/DeepSeek-R1
            - --served-model-name
            - deepseek-ai/DeepSeek-R1
            - --trust-remote-code
            - --skip-tokenizer-init
            
            # Disaggregation configuration
            - --disaggregation-mode
            - decode
            - --disaggregation-transfer-backend
            - nixl
            - --disaggregation-bootstrap-port
            - "30001"
            - --host
            - "0.0.0.0"
            
            # Parallelism configuration (official parameter names)
            - --tp
            - "32"
            - --dp
            - "32"
            - --enable-dp-attention
            - --ep-size
            - "32"
            
            # Performance tuning
            - --mem-fraction-static
            - "0.75"
            - --decode-log-interval
            - "1000"
            
    prefill:
      dynamoNamespace: ds-r1-ep
      componentType: worker
      replicas: 1
      multinode:
        nodeCount: 4  # 4 nodes × 8 GPUs = 32 GPUs
      resources:
        limits:
          gpu: "8"
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 600
          # TODO: Replace with your custom built image
          # image: your-registry/sglang-wideep-runtime:0.6.0
          image: nvcr.io/nvidia/ai-dynamo/dynamo-sglang:0.6.0
          workingDir: /workspace/components/backends/sglang
          command:
            - python3
            - -m
            - dynamo.sglang
          args:
            # Model configuration
            - --model-path
            - deepseek-ai/DeepSeek-R1
            - --served-model-name
            - deepseek-ai/DeepSeek-R1
            - --trust-remote-code
            - --skip-tokenizer-init
            
            # Disaggregation configuration
            - --disaggregation-mode
            - prefill
            - --disaggregation-transfer-backend
            - nixl
            - --disaggregation-bootstrap-port
            - "30001"
            - --host
            - "0.0.0.0"
            
            # Parallelism configuration (official parameter names)
            - --tp
            - "32"
            - --ep-size
            - "32"
            
            # Performance tuning
            - --mem-fraction-static
            - "0.75"

