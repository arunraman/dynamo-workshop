# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# DeepSeek-R1 TensorRT-LLM WideEP Deployment
# This manifest deploys DeepSeek-R1 using TensorRT-LLM backend with FP8 and EPLB

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: deepseek-r1-trtllm
  namespace: dynamo
spec:
  envs:
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: hf-token-secret
        key: HF_TOKEN
  
  backendFramework: trtllm
  
  services:
    Frontend:
      dynamoNamespace: deepseek-r1-trtllm
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          
    PrefillWorker:
      multinode:
        nodeCount: 2  # 2 nodes × 8 GPUs = 16 GPUs for prefill
      envFromSecret: hf-token-secret
      dynamoNamespace: deepseek-r1-trtllm
      componentType: worker
      subComponentType: prefill
      replicas: 1
      resources:
        limits:
          gpu: "8"  # 8 GPUs per node
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          workingDir: /workspace
          command:
          - python3
          - -m
          - dynamo.trtllm
          args:
            # Engine configuration
            - --engine-dir
            - /models/deepseek-r1-engine
            - --config-path
            - /workspace/lab3/configs/trtllm/wide_ep_prefill.yaml
            
            # Disaggregation
            - --disaggregation-mode
            - prefill
          
          volumeMounts:
          - name: model-storage
            mountPath: /models
          - name: config-volume
            mountPath: /workspace/lab3/configs/trtllm
        
        volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-pvc
        - name: config-volume
          configMap:
            name: trtllm-configs
    
    DecodeWorker:
      multinode:
        nodeCount: 2  # 2 nodes × 8 GPUs = 16 GPUs for decode
      envFromSecret: hf-token-secret
      dynamoNamespace: deepseek-r1-trtllm
      componentType: worker
      subComponentType: decode
      replicas: 1
      resources:
        limits:
          gpu: "8"  # 8 GPUs per node
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/dynamo:latest
          workingDir: /workspace
          command:
          - python3
          - -m
          - dynamo.trtllm
          args:
            # Engine configuration
            - --engine-dir
            - /models/deepseek-r1-engine
            - --config-path
            - /workspace/lab3/configs/trtllm/wide_ep_decode.yaml
            
            # Disaggregation
            - --disaggregation-mode
            - decode
          
          volumeMounts:
          - name: model-storage
            mountPath: /models
          - name: config-volume
            mountPath: /workspace/lab3/configs/trtllm
        
        volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-pvc
        - name: config-volume
          configMap:
            name: trtllm-configs

---
# ConfigMap for TensorRT-LLM configuration files
apiVersion: v1
kind: ConfigMap
metadata:
  name: trtllm-configs
  namespace: dynamo
data:
  # Mount the YAML files from configs/trtllm/ directory
  # These should be created from the actual files using:
  # kubectl create configmap trtllm-configs --from-file=configs/trtllm/ -n dynamo

